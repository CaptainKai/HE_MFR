2021-07-29 15:30:48,929: [('name', 'SResnet_split'), ('backbone_model_name', 'SimpleResnet_split_abla_36'), ('classify_model_name', 'MarginCosineProduct'), ('resume_net_model', '/home/ubuntu/data2/lk/recognition/pytorch_new/snapshot/SResnet_36_split_noDrop_noLA_b3_init_casia/backbone_20_checkpoint.pth'), ('resume_net_classifier', '/home/ubuntu/data2/lk/recognition/pytorch_new/snapshot/SResnet_36_split_noDrop_noLA_b3_init_casia/classifier_status_20_checkpoint.pth'), ('no_cuda', False), ('gpu_num', 1), ('log_interval', 100), ('log_path', './logs/SResnet_36_split_noDrop_noLA_b3_init_tri_casia_20_mixdataParra.log'), ('log_pic_path', './logs/pic/SResnet_36_split_noDrop_noLA_b3_init_tri_casia_20_mixdataParra/'), ('save_path', 'snapshot/SResnet_36_split_noDrop_noLA_b3_init_tri_casia_20_mixdataParra/'), ('lmdb_path', '/home/ubuntu/data4/lk/data/lmdb_mask_augu_full_casia'), ('batch_size', 512), ('datanum', 493733), ('num_class', 10575), ('lmdb_workers', 4), ('num_workers', 4), ('start_epoch', 1), ('max_epoch', 65), ('lr', 0.1), ('base', 'epoch'), ('step_size', [10, 20, 30, 40, 50, 60]), ('momentum', 0.9), ('gama', 0.1), ('weight_decay', 0.0005), ('rank', 0), ('dist_url', 'env://'), ('world_size', 2), ('gpu', 0), ('dist_backend', 'nccl'), ('distributed', True), ('master_port', 32345), ('multiprocessing_distributed', False), ('SEED', 1337), ('local_rank', 0)]
2021-07-29 15:30:48,929: SimpleResidual_split_abla(
  (conv1): ConvPrelu(
    (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (prelu): PReLU(num_parameters=64)
  )
  (layer1): Sequential(
    (0): SimpleResidualUnit(
      (conv1): ConvPrelu(
        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=64)
      )
      (conv2): ConvPrelu(
        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=64)
      )
    )
    (1): SimpleResidualUnit(
      (conv1): ConvPrelu(
        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=64)
      )
      (conv2): ConvPrelu(
        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=64)
      )
    )
  )
  (conv2): ConvPrelu(
    (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (prelu): PReLU(num_parameters=128)
  )
  (layer2): Sequential(
    (0): SimpleResidualUnit(
      (conv1): ConvPrelu(
        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=128)
      )
      (conv2): ConvPrelu(
        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=128)
      )
    )
    (1): SimpleResidualUnit(
      (conv1): ConvPrelu(
        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=128)
      )
      (conv2): ConvPrelu(
        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=128)
      )
    )
    (2): SimpleResidualUnit(
      (conv1): ConvPrelu(
        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=128)
      )
      (conv2): ConvPrelu(
        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=128)
      )
    )
    (3): SimpleResidualUnit(
      (conv1): ConvPrelu(
        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=128)
      )
      (conv2): ConvPrelu(
        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=128)
      )
    )
  )
  (conv3): ConvPrelu(
    (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (prelu): PReLU(num_parameters=256)
  )
  (layer3): Sequential(
    (0): SimpleResidualUnit(
      (conv1): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
      (conv2): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
    )
    (1): SimpleResidualUnit(
      (conv1): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
      (conv2): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
    )
    (2): SimpleResidualUnit(
      (conv1): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
      (conv2): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
    )
    (3): SimpleResidualUnit(
      (conv1): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
      (conv2): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
    )
    (4): SimpleResidualUnit(
      (conv1): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
      (conv2): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
    )
    (5): SimpleResidualUnit(
      (conv1): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
      (conv2): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
    )
    (6): SimpleResidualUnit(
      (conv1): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
      (conv2): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
    )
    (7): SimpleResidualUnit(
      (conv1): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
      (conv2): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
    )
  )
  (backbone): Sequential(
    (0): ConvPrelu(
      (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (prelu): PReLU(num_parameters=64)
    )
    (1): Sequential(
      (0): SimpleResidualUnit(
        (conv1): ConvPrelu(
          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (prelu): PReLU(num_parameters=64)
        )
        (conv2): ConvPrelu(
          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (prelu): PReLU(num_parameters=64)
        )
      )
      (1): SimpleResidualUnit(
        (conv1): ConvPrelu(
          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (prelu): PReLU(num_parameters=64)
        )
        (conv2): ConvPrelu(
          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (prelu): PReLU(num_parameters=64)
        )
      )
    )
    (2): ConvPrelu(
      (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (prelu): PReLU(num_parameters=128)
    )
    (3): Sequential(
      (0): SimpleResidualUnit(
        (conv1): ConvPrelu(
          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (prelu): PReLU(num_parameters=128)
        )
        (conv2): ConvPrelu(
          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (prelu): PReLU(num_parameters=128)
        )
      )
      (1): SimpleResidualUnit(
        (conv1): ConvPrelu(
          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (prelu): PReLU(num_parameters=128)
        )
        (conv2): ConvPrelu(
          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (prelu): PReLU(num_parameters=128)
        )
      )
      (2): SimpleResidualUnit(
        (conv1): ConvPrelu(
          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (prelu): PReLU(num_parameters=128)
        )
        (conv2): ConvPrelu(
          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (prelu): PReLU(num_parameters=128)
        )
      )
      (3): SimpleResidualUnit(
        (conv1): ConvPrelu(
          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (prelu): PReLU(num_parameters=128)
        )
        (conv2): ConvPrelu(
          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (prelu): PReLU(num_parameters=128)
        )
      )
    )
    (4): ConvPrelu(
      (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (prelu): PReLU(num_parameters=256)
    )
    (5): Sequential(
      (0): SimpleResidualUnit(
        (conv1): ConvPrelu(
          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (prelu): PReLU(num_parameters=256)
        )
        (conv2): ConvPrelu(
          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (prelu): PReLU(num_parameters=256)
        )
      )
      (1): SimpleResidualUnit(
        (conv1): ConvPrelu(
          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (prelu): PReLU(num_parameters=256)
        )
        (conv2): ConvPrelu(
          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (prelu): PReLU(num_parameters=256)
        )
      )
      (2): SimpleResidualUnit(
        (conv1): ConvPrelu(
          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (prelu): PReLU(num_parameters=256)
        )
        (conv2): ConvPrelu(
          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (prelu): PReLU(num_parameters=256)
        )
      )
      (3): SimpleResidualUnit(
        (conv1): ConvPrelu(
          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (prelu): PReLU(num_parameters=256)
        )
        (conv2): ConvPrelu(
          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (prelu): PReLU(num_parameters=256)
        )
      )
      (4): SimpleResidualUnit(
        (conv1): ConvPrelu(
          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (prelu): PReLU(num_parameters=256)
        )
        (conv2): ConvPrelu(
          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (prelu): PReLU(num_parameters=256)
        )
      )
      (5): SimpleResidualUnit(
        (conv1): ConvPrelu(
          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (prelu): PReLU(num_parameters=256)
        )
        (conv2): ConvPrelu(
          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (prelu): PReLU(num_parameters=256)
        )
      )
      (6): SimpleResidualUnit(
        (conv1): ConvPrelu(
          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (prelu): PReLU(num_parameters=256)
        )
        (conv2): ConvPrelu(
          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (prelu): PReLU(num_parameters=256)
        )
      )
      (7): SimpleResidualUnit(
        (conv1): ConvPrelu(
          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (prelu): PReLU(num_parameters=256)
        )
        (conv2): ConvPrelu(
          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (prelu): PReLU(num_parameters=256)
        )
      )
    )
  )
  (SSS_up): SpaceSliceStrategy_subject(
    (layer4): Sequential(
      (0): SimpleResidualUnit(
        (conv1): ConvPrelu(
          (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (prelu): PReLU(num_parameters=512)
        )
        (conv2): ConvPrelu(
          (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (prelu): PReLU(num_parameters=512)
        )
      )
      (1): SimpleResidualUnit(
        (conv1): ConvPrelu(
          (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (prelu): PReLU(num_parameters=512)
        )
        (conv2): ConvPrelu(
          (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (prelu): PReLU(num_parameters=512)
        )
      )
    )
    (conv4): ConvPrelu(
      (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (prelu): PReLU(num_parameters=512)
    )
    (subject): ModuleList(
      (0): Sequential(
        (0): SimpleResidualUnit(
          (conv1): ConvPrelu(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (prelu): PReLU(num_parameters=512)
          )
          (conv2): ConvPrelu(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (prelu): PReLU(num_parameters=512)
          )
        )
        (1): SimpleResidualUnit(
          (conv1): ConvPrelu(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (prelu): PReLU(num_parameters=512)
          )
          (conv2): ConvPrelu(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (prelu): PReLU(num_parameters=512)
          )
        )
      )
      (1): ConvPrelu(
        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=512)
      )
    )
  )
  (SSS_down): SpaceSliceStrategy_subject(
    (layer4): Sequential(
      (0): SimpleResidualUnit(
        (conv1): ConvPrelu(
          (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (prelu): PReLU(num_parameters=512)
        )
        (conv2): ConvPrelu(
          (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (prelu): PReLU(num_parameters=512)
        )
      )
      (1): SimpleResidualUnit(
        (conv1): ConvPrelu(
          (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (prelu): PReLU(num_parameters=512)
        )
        (conv2): ConvPrelu(
          (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (prelu): PReLU(num_parameters=512)
        )
      )
    )
    (conv4): ConvPrelu(
      (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (prelu): PReLU(num_parameters=512)
    )
    (subject): ModuleList(
      (0): Sequential(
        (0): SimpleResidualUnit(
          (conv1): ConvPrelu(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (prelu): PReLU(num_parameters=512)
          )
          (conv2): ConvPrelu(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (prelu): PReLU(num_parameters=512)
          )
        )
        (1): SimpleResidualUnit(
          (conv1): ConvPrelu(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (prelu): PReLU(num_parameters=512)
          )
          (conv2): ConvPrelu(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (prelu): PReLU(num_parameters=512)
          )
        )
      )
      (1): ConvPrelu(
        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=512)
      )
    )
  )
  (fc_up): Linear(in_features=10752, out_features=256, bias=True)
  (fc_down): Linear(in_features=14336, out_features=256, bias=True)
)
2021-07-29 15:30:49,002: Loading resume (model) network...
2021-07-29 15:30:50,568: resume net (model) loaded
2021-07-29 15:30:50,568: Loading resume (classifier) network...
2021-07-29 15:30:52,408: start epoch: 20
2021-07-29 15:32:11,763: [('name', 'SResnet_split'), ('backbone_model_name', 'SimpleResnet_split_abla_36'), ('classify_model_name', 'MarginCosineProduct'), ('resume_net_model', '/home/ubuntu/data2/lk/recognition/pytorch_new/snapshot/SResnet_36_split_noDrop_noLA_b3_init_casia/backbone_20_checkpoint.pth'), ('resume_net_classifier', '/home/ubuntu/data2/lk/recognition/pytorch_new/snapshot/SResnet_36_split_noDrop_noLA_b3_init_casia/classifier_status_20_checkpoint.pth'), ('no_cuda', False), ('gpu_num', 1), ('log_interval', 100), ('log_path', './logs/SResnet_36_split_noDrop_noLA_b3_init_tri_casia_20_mixdataParra.log'), ('log_pic_path', './logs/pic/SResnet_36_split_noDrop_noLA_b3_init_tri_casia_20_mixdataParra/'), ('save_path', 'snapshot/SResnet_36_split_noDrop_noLA_b3_init_tri_casia_20_mixdataParra/'), ('lmdb_path', '/home/ubuntu/data4/lk/data/lmdb_mask_augu_full_casia'), ('batch_size', 512), ('datanum', 493733), ('num_class', 10575), ('lmdb_workers', 4), ('num_workers', 4), ('start_epoch', 1), ('max_epoch', 65), ('lr', 0.1), ('base', 'epoch'), ('step_size', [10, 20, 30, 40, 50, 60]), ('momentum', 0.9), ('gama', 0.1), ('weight_decay', 0.0005), ('rank', 0), ('dist_url', 'env://'), ('world_size', 2), ('gpu', 0), ('dist_backend', 'nccl'), ('distributed', True), ('master_port', 32345), ('multiprocessing_distributed', False), ('SEED', 1337), ('local_rank', 0)]
2021-07-29 15:32:11,763: SimpleResidual_split_abla(
  (conv1): ConvPrelu(
    (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (prelu): PReLU(num_parameters=64)
  )
  (layer1): Sequential(
    (0): SimpleResidualUnit(
      (conv1): ConvPrelu(
        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=64)
      )
      (conv2): ConvPrelu(
        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=64)
      )
    )
    (1): SimpleResidualUnit(
      (conv1): ConvPrelu(
        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=64)
      )
      (conv2): ConvPrelu(
        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=64)
      )
    )
  )
  (conv2): ConvPrelu(
    (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (prelu): PReLU(num_parameters=128)
  )
  (layer2): Sequential(
    (0): SimpleResidualUnit(
      (conv1): ConvPrelu(
        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=128)
      )
      (conv2): ConvPrelu(
        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=128)
      )
    )
    (1): SimpleResidualUnit(
      (conv1): ConvPrelu(
        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=128)
      )
      (conv2): ConvPrelu(
        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=128)
      )
    )
    (2): SimpleResidualUnit(
      (conv1): ConvPrelu(
        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=128)
      )
      (conv2): ConvPrelu(
        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=128)
      )
    )
    (3): SimpleResidualUnit(
      (conv1): ConvPrelu(
        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=128)
      )
      (conv2): ConvPrelu(
        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=128)
      )
    )
  )
  (conv3): ConvPrelu(
    (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (prelu): PReLU(num_parameters=256)
  )
  (layer3): Sequential(
    (0): SimpleResidualUnit(
      (conv1): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
      (conv2): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
    )
    (1): SimpleResidualUnit(
      (conv1): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
      (conv2): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
    )
    (2): SimpleResidualUnit(
      (conv1): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
      (conv2): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
    )
    (3): SimpleResidualUnit(
      (conv1): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
      (conv2): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
    )
    (4): SimpleResidualUnit(
      (conv1): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
      (conv2): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
    )
    (5): SimpleResidualUnit(
      (conv1): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
      (conv2): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
    )
    (6): SimpleResidualUnit(
      (conv1): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
      (conv2): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
    )
    (7): SimpleResidualUnit(
      (conv1): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
      (conv2): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
    )
  )
  (backbone): Sequential(
    (0): ConvPrelu(
      (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (prelu): PReLU(num_parameters=64)
    )
    (1): Sequential(
      (0): SimpleResidualUnit(
        (conv1): ConvPrelu(
          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (prelu): PReLU(num_parameters=64)
        )
        (conv2): ConvPrelu(
          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (prelu): PReLU(num_parameters=64)
        )
      )
      (1): SimpleResidualUnit(
        (conv1): ConvPrelu(
          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (prelu): PReLU(num_parameters=64)
        )
        (conv2): ConvPrelu(
          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (prelu): PReLU(num_parameters=64)
        )
      )
    )
    (2): ConvPrelu(
      (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (prelu): PReLU(num_parameters=128)
    )
    (3): Sequential(
      (0): SimpleResidualUnit(
        (conv1): ConvPrelu(
          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (prelu): PReLU(num_parameters=128)
        )
        (conv2): ConvPrelu(
          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (prelu): PReLU(num_parameters=128)
        )
      )
      (1): SimpleResidualUnit(
        (conv1): ConvPrelu(
          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (prelu): PReLU(num_parameters=128)
        )
        (conv2): ConvPrelu(
          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (prelu): PReLU(num_parameters=128)
        )
      )
      (2): SimpleResidualUnit(
        (conv1): ConvPrelu(
          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (prelu): PReLU(num_parameters=128)
        )
        (conv2): ConvPrelu(
          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (prelu): PReLU(num_parameters=128)
        )
      )
      (3): SimpleResidualUnit(
        (conv1): ConvPrelu(
          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (prelu): PReLU(num_parameters=128)
        )
        (conv2): ConvPrelu(
          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (prelu): PReLU(num_parameters=128)
        )
      )
    )
    (4): ConvPrelu(
      (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (prelu): PReLU(num_parameters=256)
    )
    (5): Sequential(
      (0): SimpleResidualUnit(
        (conv1): ConvPrelu(
          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (prelu): PReLU(num_parameters=256)
        )
        (conv2): ConvPrelu(
          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (prelu): PReLU(num_parameters=256)
        )
      )
      (1): SimpleResidualUnit(
        (conv1): ConvPrelu(
          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (prelu): PReLU(num_parameters=256)
        )
        (conv2): ConvPrelu(
          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (prelu): PReLU(num_parameters=256)
        )
      )
      (2): SimpleResidualUnit(
        (conv1): ConvPrelu(
          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (prelu): PReLU(num_parameters=256)
        )
        (conv2): ConvPrelu(
          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (prelu): PReLU(num_parameters=256)
        )
      )
      (3): SimpleResidualUnit(
        (conv1): ConvPrelu(
          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (prelu): PReLU(num_parameters=256)
        )
        (conv2): ConvPrelu(
          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (prelu): PReLU(num_parameters=256)
        )
      )
      (4): SimpleResidualUnit(
        (conv1): ConvPrelu(
          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (prelu): PReLU(num_parameters=256)
        )
        (conv2): ConvPrelu(
          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (prelu): PReLU(num_parameters=256)
        )
      )
      (5): SimpleResidualUnit(
        (conv1): ConvPrelu(
          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (prelu): PReLU(num_parameters=256)
        )
        (conv2): ConvPrelu(
          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (prelu): PReLU(num_parameters=256)
        )
      )
      (6): SimpleResidualUnit(
        (conv1): ConvPrelu(
          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (prelu): PReLU(num_parameters=256)
        )
        (conv2): ConvPrelu(
          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (prelu): PReLU(num_parameters=256)
        )
      )
      (7): SimpleResidualUnit(
        (conv1): ConvPrelu(
          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (prelu): PReLU(num_parameters=256)
        )
        (conv2): ConvPrelu(
          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (prelu): PReLU(num_parameters=256)
        )
      )
    )
  )
  (SSS_up): SpaceSliceStrategy_subject(
    (layer4): Sequential(
      (0): SimpleResidualUnit(
        (conv1): ConvPrelu(
          (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (prelu): PReLU(num_parameters=512)
        )
        (conv2): ConvPrelu(
          (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (prelu): PReLU(num_parameters=512)
        )
      )
      (1): SimpleResidualUnit(
        (conv1): ConvPrelu(
          (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (prelu): PReLU(num_parameters=512)
        )
        (conv2): ConvPrelu(
          (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (prelu): PReLU(num_parameters=512)
        )
      )
    )
    (conv4): ConvPrelu(
      (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (prelu): PReLU(num_parameters=512)
    )
    (subject): ModuleList(
      (0): Sequential(
        (0): SimpleResidualUnit(
          (conv1): ConvPrelu(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (prelu): PReLU(num_parameters=512)
          )
          (conv2): ConvPrelu(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (prelu): PReLU(num_parameters=512)
          )
        )
        (1): SimpleResidualUnit(
          (conv1): ConvPrelu(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (prelu): PReLU(num_parameters=512)
          )
          (conv2): ConvPrelu(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (prelu): PReLU(num_parameters=512)
          )
        )
      )
      (1): ConvPrelu(
        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=512)
      )
    )
  )
  (SSS_down): SpaceSliceStrategy_subject(
    (layer4): Sequential(
      (0): SimpleResidualUnit(
        (conv1): ConvPrelu(
          (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (prelu): PReLU(num_parameters=512)
        )
        (conv2): ConvPrelu(
          (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (prelu): PReLU(num_parameters=512)
        )
      )
      (1): SimpleResidualUnit(
        (conv1): ConvPrelu(
          (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (prelu): PReLU(num_parameters=512)
        )
        (conv2): ConvPrelu(
          (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (prelu): PReLU(num_parameters=512)
        )
      )
    )
    (conv4): ConvPrelu(
      (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (prelu): PReLU(num_parameters=512)
    )
    (subject): ModuleList(
      (0): Sequential(
        (0): SimpleResidualUnit(
          (conv1): ConvPrelu(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (prelu): PReLU(num_parameters=512)
          )
          (conv2): ConvPrelu(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (prelu): PReLU(num_parameters=512)
          )
        )
        (1): SimpleResidualUnit(
          (conv1): ConvPrelu(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (prelu): PReLU(num_parameters=512)
          )
          (conv2): ConvPrelu(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (prelu): PReLU(num_parameters=512)
          )
        )
      )
      (1): ConvPrelu(
        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=512)
      )
    )
  )
  (fc_up): Linear(in_features=10752, out_features=256, bias=True)
  (fc_down): Linear(in_features=14336, out_features=256, bias=True)
)
2021-07-29 15:32:11,831: Loading resume (model) network...
2021-07-29 15:32:12,110: resume net (model) loaded
2021-07-29 15:32:12,110: Loading resume (classifier) network...
2021-07-29 15:32:12,245: start epoch: 20
2021-07-29 15:32:12,283: resume net (classifier) loaded
2021-07-29 15:32:18,366: data balance
2021-07-29 15:34:38,487: time cost, forward:0.0350494384765625, backward:0.1960317125224104, data cost:1.0473807869535503 
2021-07-29 15:34:38,487: ============================================================
2021-07-29 15:34:38,487: Epoch 20/65 Batch 100/964 eta: 17:12:55.045236	Training Loss2 2.8279 (2.6944)	Training Loss3 4.1538 (3.9999)	Training Loss4 3.5359 (3.3555)	Training Total_Loss 7.3357 (7.0249)	Training Prec@1_up 97.461 (97.520)	Training Prec@1_down 93.750 (94.072)	
2021-07-29 15:34:38,487: ============================================================
2021-07-29 15:36:57,110: time cost, forward:0.029474324317433725, backward:0.1858074772897078, data cost:1.0508100818749049 
2021-07-29 15:36:57,128: ============================================================
2021-07-29 15:36:57,128: Epoch 20/65 Batch 200/964 eta: 17:00:03.137035	Training Loss2 2.7359 (2.6854)	Training Loss3 3.8717 (3.9888)	Training Loss4 3.3848 (3.3483)	Training Total_Loss 6.9321 (7.0057)	Training Prec@1_up 98.047 (97.552)	Training Prec@1_down 95.117 (94.131)	
2021-07-29 15:36:57,128: ============================================================
2021-07-29 15:39:15,316: time cost, forward:0.027229022022872466, backward:0.1828780341706547, data cost:1.0481452583070583 
2021-07-29 15:39:15,317: ============================================================
2021-07-29 15:39:15,317: Epoch 20/65 Batch 300/964 eta: 16:54:25.105891	Training Loss2 2.9544 (2.6887)	Training Loss3 4.2158 (3.9867)	Training Loss4 3.6211 (3.3532)	Training Total_Loss 7.5036 (7.0076)	Training Prec@1_up 96.484 (97.518)	Training Prec@1_down 93.164 (94.117)	
2021-07-29 15:39:15,317: ============================================================
2021-07-29 15:41:32,490: time cost, forward:0.02664274679389514, backward:0.18190347580682664, data cost:1.0446872191321581 
2021-07-29 15:41:32,491: ============================================================
2021-07-29 15:41:32,491: Epoch 20/65 Batch 400/964 eta: 16:44:41.169422	Training Loss2 2.5670 (2.6925)	Training Loss3 3.7699 (3.9892)	Training Loss4 3.2699 (3.3563)	Training Total_Loss 6.6883 (7.0136)	Training Prec@1_up 97.852 (97.486)	Training Prec@1_down 94.531 (94.096)	
2021-07-29 15:41:32,491: ============================================================
2021-07-29 15:43:49,939: time cost, forward:0.026583643857845084, backward:0.18133655291998793, data cost:1.043635580964939 
2021-07-29 15:43:49,939: ============================================================
2021-07-29 15:43:49,940: Epoch 20/65 Batch 500/964 eta: 16:44:24.399010	Training Loss2 2.6099 (2.6961)	Training Loss3 3.7340 (3.9912)	Training Loss4 3.2394 (3.3594)	Training Total_Loss 6.6587 (7.0189)	Training Prec@1_up 98.242 (97.453)	Training Prec@1_down 94.336 (94.074)	
2021-07-29 15:43:49,940: ============================================================
2021-07-29 15:46:06,930: time cost, forward:0.026234579404725854, backward:0.18087533439738127, data cost:1.0423723437352252 
2021-07-29 15:46:06,931: ============================================================
2021-07-29 15:46:06,931: Epoch 20/65 Batch 600/964 eta: 16:38:46.927877	Training Loss2 2.7396 (2.6999)	Training Loss3 4.0830 (3.9940)	Training Loss4 3.3742 (3.3637)	Training Total_Loss 7.1399 (7.0257)	Training Prec@1_up 96.289 (97.434)	Training Prec@1_down 92.383 (94.054)	
2021-07-29 15:46:06,932: ============================================================
2021-07-29 15:48:23,160: time cost, forward:0.026157258747302754, backward:0.18041427316242703, data cost:1.0411525911868726 
2021-07-29 15:48:23,161: ============================================================
2021-07-29 15:48:23,161: Epoch 20/65 Batch 700/964 eta: 16:30:57.419409	Training Loss2 2.5091 (2.7001)	Training Loss3 3.6075 (3.9924)	Training Loss4 3.1592 (3.3649)	Training Total_Loss 6.4416 (7.0248)	Training Prec@1_up 98.242 (97.440)	Training Prec@1_down 95.508 (94.062)	
2021-07-29 15:48:23,161: ============================================================
2021-07-29 15:50:39,651: time cost, forward:0.026115964440738455, backward:0.18017644906073846, data cost:1.0389033074074603 
2021-07-29 15:50:39,672: ============================================================
2021-07-29 15:50:39,673: Epoch 20/65 Batch 800/964 eta: 16:30:44.049213	Training Loss2 2.7055 (2.7024)	Training Loss3 3.9716 (3.9928)	Training Loss4 3.3607 (3.3687)	Training Total_Loss 7.0047 (7.0284)	Training Prec@1_up 97.070 (97.447)	Training Prec@1_down 94.336 (94.057)	
2021-07-29 15:50:39,673: ============================================================
2021-07-29 15:52:57,756: time cost, forward:0.026011634854241395, backward:0.18007712422541702, data cost:1.0402866478624015 
2021-07-29 15:52:57,757: ============================================================
2021-07-29 15:52:57,757: Epoch 20/65 Batch 900/964 eta: 16:39:50.813750	Training Loss2 2.5836 (2.7074)	Training Loss3 3.8461 (3.9962)	Training Loss4 3.2028 (3.3744)	Training Total_Loss 6.7393 (7.0371)	Training Prec@1_up 97.852 (97.422)	Training Prec@1_down 94.531 (94.038)	
2021-07-29 15:52:57,757: ============================================================
2021-07-29 15:54:26,273: Epoch: 20/65 eta: 16:38:21.058821	Training Loss2 2.5313 (2.7083)	Training Loss3 3.6090 (3.9958)	Training Loss4 3.2760 (3.3756)	Training Total_Loss 6.5126 (7.0377)	Training Prec@1_up 98.242 (97.412)	Training Prec@1_down 95.312 (94.034)	
2021-07-29 15:54:26,273: ============================================================
2021-07-29 15:54:26,385: Save Checkpoint...
2021-07-29 15:54:26,385: ============================================================
2021-07-29 15:54:27,255: Save done!
2021-07-29 15:54:27,255: ============================================================
2021-07-29 15:56:44,866: time cost, forward:0.024711276545669094, backward:0.17773370309309525, data cost:1.0542132589552138 
2021-07-29 15:56:44,888: ============================================================
2021-07-29 15:56:44,888: Epoch 21/65 Batch 100/964 eta: 16:32:36.797976	Training Loss2 2.5612 (2.5767)	Training Loss3 3.6176 (3.8513)	Training Loss4 3.2664 (3.2381)	Training Total_Loss 6.5314 (6.7587)	Training Prec@1_up 97.656 (97.684)	Training Prec@1_down 94.922 (94.444)	
2021-07-29 15:56:44,888: ============================================================
2021-07-29 15:59:00,706: time cost, forward:0.024400685899820758, backward:0.1785636906647802, data cost:1.0390643642176336 
2021-07-29 15:59:00,707: ============================================================
2021-07-29 15:59:00,707: Epoch 21/65 Batch 200/964 eta: 16:17:27.953696	Training Loss2 2.4516 (2.5662)	Training Loss3 3.5910 (3.8388)	Training Loss4 3.1586 (3.2273)	Training Total_Loss 6.3961 (6.7355)	Training Prec@1_up 98.242 (97.668)	Training Prec@1_down 95.508 (94.474)	
2021-07-29 15:59:00,707: ============================================================
2021-07-29 16:01:15,165: time cost, forward:0.024321953987198133, backward:0.17894435806019252, data cost:1.0292574059604402 
2021-07-29 16:01:15,183: ============================================================
2021-07-29 16:01:15,183: Epoch 21/65 Batch 300/964 eta: 16:05:33.613294	Training Loss2 2.1949 (2.5804)	Training Loss3 3.3391 (3.8549)	Training Loss4 2.9085 (3.2419)	Training Total_Loss 5.8909 (6.7660)	Training Prec@1_up 98.633 (97.644)	Training Prec@1_down 96.680 (94.416)	
2021-07-29 16:01:15,183: ============================================================
2021-07-29 16:03:28,673: time cost, forward:0.024218454098044184, backward:0.17902030980676636, data cost:1.0220185509301667 
2021-07-29 16:03:28,674: ============================================================
2021-07-29 16:03:28,674: Epoch 21/65 Batch 400/964 eta: 15:56:15.816486	Training Loss2 2.3955 (2.5892)	Training Loss3 3.5007 (3.8635)	Training Loss4 3.0767 (3.2519)	Training Total_Loss 6.2368 (6.7840)	Training Prec@1_up 98.828 (97.627)	Training Prec@1_down 96.094 (94.389)	
2021-07-29 16:03:28,674: ============================================================
2021-07-29 16:05:43,728: time cost, forward:0.024055466145456197, backward:0.1790998937610634, data cost:1.0207419022768438 
2021-07-29 16:05:43,729: ============================================================
2021-07-29 16:05:43,729: Epoch 21/65 Batch 500/964 eta: 16:05:12.821301	Training Loss2 2.6301 (2.5967)	Training Loss3 3.8807 (3.8639)	Training Loss4 3.2990 (3.2611)	Training Total_Loss 6.8453 (6.7927)	Training Prec@1_up 97.852 (97.631)	Training Prec@1_down 95.508 (94.413)	
2021-07-29 16:05:43,729: ============================================================
2021-07-29 16:07:58,357: time cost, forward:0.024060094097818875, backward:0.17908246568924996, data cost:1.019261209315967 
2021-07-29 16:07:58,357: ============================================================
2021-07-29 16:07:58,358: Epoch 21/65 Batch 600/964 eta: 15:59:55.526696	Training Loss2 2.4500 (2.5992)	Training Loss3 3.6927 (3.8687)	Training Loss4 3.1338 (3.2635)	Training Total_Loss 6.4846 (6.8000)	Training Prec@1_up 98.438 (97.636)	Training Prec@1_down 94.727 (94.413)	
2021-07-29 16:07:58,358: ============================================================
2021-07-29 16:10:13,891: time cost, forward:0.024001443505457715, backward:0.17904028565075264, data cost:1.019528383519687 
2021-07-29 16:10:13,892: ============================================================
2021-07-29 16:10:13,892: Epoch 21/65 Batch 700/964 eta: 16:04:07.374574	Training Loss2 2.6663 (2.6038)	Training Loss3 3.7625 (3.8721)	Training Loss4 3.3571 (3.2681)	Training Total_Loss 6.7741 (6.8081)	Training Prec@1_up 97.656 (97.627)	Training Prec@1_down 94.922 (94.400)	
2021-07-29 16:10:13,892: ============================================================
2021-07-29 16:12:29,531: time cost, forward:0.024067581818906475, backward:0.17901081644996386, data cost:1.0198828490714407 
2021-07-29 16:12:29,531: ============================================================
2021-07-29 16:12:29,532: Epoch 21/65 Batch 800/964 eta: 16:02:36.790375	Training Loss2 2.4017 (2.6068)	Training Loss3 3.5846 (3.8724)	Training Loss4 3.0434 (3.2707)	Training Total_Loss 6.3072 (6.8111)	Training Prec@1_up 97.656 (97.629)	Training Prec@1_down 95.508 (94.396)	
2021-07-29 16:12:29,532: ============================================================
2021-07-29 16:14:44,170: time cost, forward:0.024295987488828327, backward:0.1790664596472752, data cost:1.0189129099564769 
2021-07-29 16:14:44,170: ============================================================
2021-07-29 16:14:44,170: Epoch 21/65 Batch 900/964 eta: 15:53:15.866049	Training Loss2 2.7099 (2.6090)	Training Loss3 4.0841 (3.8708)	Training Loss4 3.3704 (3.2729)	Training Total_Loss 7.1242 (6.8117)	Training Prec@1_up 97.266 (97.621)	Training Prec@1_down 94.336 (94.405)	
2021-07-29 16:14:44,170: ============================================================
2021-07-29 16:16:14,840: Epoch: 21/65 eta: 15:51:48.350894	Training Loss2 3.0231 (2.6100)	Training Loss3 4.3826 (3.8690)	Training Loss4 3.6305 (3.2739)	Training Total_Loss 7.7093 (6.8110)	Training Prec@1_up 97.266 (97.624)	Training Prec@1_down 93.359 (94.406)	
2021-07-29 16:16:14,896: ============================================================
2021-07-29 16:18:32,853: time cost, forward:0.02466962795064907, backward:0.18019705107717804, data cost:1.0547497176160716 
2021-07-29 16:18:32,853: ============================================================
2021-07-29 16:18:32,853: Epoch 22/65 Batch 100/964 eta: 16:12:24.111251	Training Loss2 2.7828 (2.4858)	Training Loss3 4.0043 (3.7366)	Training Loss4 3.4474 (3.1318)	Training Total_Loss 7.1194 (6.5454)	Training Prec@1_up 97.266 (97.832)	Training Prec@1_down 94.531 (94.845)	
2021-07-29 16:18:32,854: ============================================================
2021-07-29 16:20:48,961: time cost, forward:0.0246375337917002, backward:0.18035806842784785, data cost:1.0397244625954172 
2021-07-29 16:20:49,057: ============================================================
2021-07-29 16:20:49,057: Epoch 22/65 Batch 200/964 eta: 15:58:21.297534	Training Loss2 2.5525 (2.4803)	Training Loss3 3.7999 (3.7222)	Training Loss4 3.2423 (3.1336)	Training Total_Loss 6.6973 (6.5292)	Training Prec@1_up 97.070 (97.895)	Training Prec@1_down 93.555 (94.906)	
2021-07-29 16:20:49,058: ============================================================
2021-07-29 16:23:03,818: time cost, forward:0.02493452946079216, backward:0.18069653208040473, data cost:1.0302104583153358 
2021-07-29 16:23:03,819: ============================================================
2021-07-29 16:23:03,819: Epoch 22/65 Batch 300/964 eta: 15:45:57.439652	Training Loss2 2.5054 (2.4913)	Training Loss3 3.7576 (3.7328)	Training Loss4 3.1772 (3.1476)	Training Total_Loss 6.5989 (6.5522)	Training Prec@1_up 97.656 (97.915)	Training Prec@1_down 94.141 (94.860)	
2021-07-29 16:23:03,819: ============================================================
2021-07-29 16:25:24,667: time cost, forward:0.02511336092363324, backward:0.18071094192657852, data cost:1.0405780874696888 
2021-07-29 16:25:24,667: ============================================================
2021-07-29 16:25:24,667: Epoch 22/65 Batch 400/964 eta: 16:26:20.197946	Training Loss2 2.1707 (2.4956)	Training Loss3 3.3196 (3.7387)	Training Loss4 2.7786 (3.1542)	Training Total_Loss 5.7943 (6.5636)	Training Prec@1_up 98.633 (97.895)	Training Prec@1_down 96.289 (94.830)	
2021-07-29 16:25:24,667: ============================================================
2021-07-29 16:28:12,661: time cost, forward:0.02534705077956817, backward:0.1990321953454333, data cost:1.0676926271709986 
2021-07-29 16:28:12,701: ============================================================
2021-07-29 16:28:12,702: Epoch 22/65 Batch 500/964 eta: 19:33:55.051497	Training Loss2 2.5906 (2.5037)	Training Loss3 4.0364 (3.7488)	Training Loss4 3.2077 (3.1619)	Training Total_Loss 6.9356 (6.5816)	Training Prec@1_up 98.633 (97.870)	Training Prec@1_down 92.969 (94.780)	
2021-07-29 16:28:12,702: ============================================================
2021-07-29 16:31:26,409: time cost, forward:0.02567027287809598, backward:0.21045569585440355, data cost:1.1307070227417604 
2021-07-29 16:31:26,428: ============================================================
2021-07-29 16:31:26,428: Epoch 22/65 Batch 600/964 eta: 22:30:10.719790	Training Loss2 2.6937 (2.5133)	Training Loss3 4.0347 (3.7580)	Training Loss4 3.4234 (3.1729)	Training Total_Loss 7.0933 (6.6011)	Training Prec@1_up 97.461 (97.854)	Training Prec@1_down 94.531 (94.751)	
2021-07-29 16:31:26,428: ============================================================
2021-07-29 16:34:12,172: time cost, forward:0.025552842750058154, backward:0.22142582287604204, data cost:1.1300866508347454 
2021-07-29 16:34:12,172: ============================================================
2021-07-29 16:34:12,172: Epoch 22/65 Batch 700/964 eta: 19:12:23.463949	Training Loss2 2.7822 (2.5189)	Training Loss3 3.8864 (3.7601)	Training Loss4 3.5537 (3.1794)	Training Total_Loss 7.0543 (6.6093)	Training Prec@1_up 97.852 (97.845)	Training Prec@1_down 95.312 (94.748)	
2021-07-29 16:34:12,173: ============================================================
2021-07-29 16:36:39,539: time cost, forward:0.02543335354820509, backward:0.22052747615437035, data cost:1.1233702651848632 
2021-07-29 16:36:39,539: ============================================================
2021-07-29 16:36:39,540: Epoch 22/65 Batch 800/964 eta: 17:02:09.762137	Training Loss2 2.4394 (2.5249)	Training Loss3 3.7014 (3.7626)	Training Loss4 3.0906 (3.1854)	Training Total_Loss 6.4664 (6.6178)	Training Prec@1_up 98.633 (97.813)	Training Prec@1_down 94.531 (94.722)	
2021-07-29 16:36:39,540: ============================================================
2021-07-29 16:39:00,360: time cost, forward:0.025258268212052154, backward:0.21590130427257637, data cost:1.1178464380334296 
2021-07-29 16:39:00,361: ============================================================
2021-07-29 16:39:00,361: Epoch 22/65 Batch 900/964 eta: 16:14:24.779826	Training Loss2 2.4677 (2.5306)	Training Loss3 3.7646 (3.7661)	Training Loss4 3.1463 (3.1918)	Training Total_Loss 6.5716 (6.6273)	Training Prec@1_up 99.414 (97.801)	Training Prec@1_down 95.508 (94.715)	
2021-07-29 16:39:00,361: ============================================================
2021-07-29 16:40:32,160: Epoch: 22/65 eta: 16:12:53.245980	Training Loss2 2.7537 (2.5323)	Training Loss3 3.9484 (3.7665)	Training Loss4 3.3693 (3.1935)	Training Total_Loss 7.0099 (6.6294)	Training Prec@1_up 97.266 (97.802)	Training Prec@1_down 93.555 (94.710)	
2021-07-29 16:40:32,160: ============================================================
2021-07-29 16:42:55,788: time cost, forward:0.022566571380152847, backward:0.17843576151915272, data cost:1.095178726947669 
2021-07-29 16:42:55,788: ============================================================
2021-07-29 16:42:55,788: Epoch 23/65 Batch 100/964 eta: 16:28:20.388578	Training Loss2 2.5451 (2.4124)	Training Loss3 3.7678 (3.6389)	Training Loss4 3.2192 (3.0714)	Training Total_Loss 6.6499 (6.3808)	Training Prec@1_up 97.070 (98.063)	Training Prec@1_down 93.555 (95.125)	
2021-07-29 16:42:55,789: ============================================================
2021-07-29 16:45:14,003: time cost, forward:0.023225646522176926, backward:0.17836006322697778, data cost:1.078566312789917 
2021-07-29 16:45:14,003: ============================================================
2021-07-29 16:45:14,003: Epoch 23/65 Batch 200/964 eta: 15:50:17.676744	Training Loss2 2.3908 (2.4277)	Training Loss3 3.5262 (3.6500)	Training Loss4 3.0994 (3.0848)	Training Total_Loss 6.2713 (6.4062)	Training Prec@1_up 98.633 (97.990)	Training Prec@1_down 95.508 (95.086)	
2021-07-29 16:45:14,003: ============================================================
2021-07-29 16:47:31,920: time cost, forward:0.023547148624790154, backward:0.17883338417895264, data cost:1.067421681107486 
2021-07-29 16:47:31,920: ============================================================
2021-07-29 16:47:31,920: Epoch 23/65 Batch 300/964 eta: 15:45:57.040627	Training Loss2 2.4140 (2.4344)	Training Loss3 3.5100 (3.6556)	Training Loss4 2.9978 (3.0906)	Training Total_Loss 6.2159 (6.4180)	Training Prec@1_up 97.852 (97.944)	Training Prec@1_down 95.117 (95.013)	
2021-07-29 16:47:31,920: ============================================================
2021-07-29 16:49:51,365: time cost, forward:0.023952848032901163, backward:0.17905728440535695, data cost:1.0653058867107956 
2021-07-29 16:49:51,365: ============================================================
2021-07-29 16:49:51,365: Epoch 23/65 Batch 400/964 eta: 15:54:06.456441	Training Loss2 2.6375 (2.4413)	Training Loss3 3.8214 (3.6596)	Training Loss4 3.2878 (3.0981)	Training Total_Loss 6.7841 (6.4293)	Training Prec@1_up 98.438 (97.939)	Training Prec@1_down 95.508 (95.010)	
2021-07-29 16:49:51,366: ============================================================
2021-07-29 16:52:10,639: time cost, forward:0.024236797091956128, backward:0.179219019436884, data cost:1.0636932415092637 
2021-07-29 16:52:10,640: ============================================================
2021-07-29 16:52:10,640: Epoch 23/65 Batch 500/964 eta: 15:50:37.047276	Training Loss2 2.4430 (2.4451)	Training Loss3 3.9079 (3.6648)	Training Loss4 3.1169 (3.1022)	Training Total_Loss 6.6879 (6.4385)	Training Prec@1_up 97.852 (97.945)	Training Prec@1_down 94.531 (95.009)	
2021-07-29 16:52:10,640: ============================================================
2021-07-29 16:54:30,789: time cost, forward:0.0242007411581049, backward:0.17924039113103646, data cost:1.0642091296551024 
2021-07-29 16:54:30,790: ============================================================
2021-07-29 16:54:30,790: Epoch 23/65 Batch 600/964 eta: 15:54:15.402495	Training Loss2 2.6235 (2.4448)	Training Loss3 3.9193 (3.6584)	Training Loss4 3.2796 (3.1017)	Training Total_Loss 6.8709 (6.4316)	Training Prec@1_up 97.852 (97.955)	Training Prec@1_down 94.141 (95.042)	
2021-07-29 16:54:30,790: ============================================================
2021-07-29 16:56:48,695: time cost, forward:0.024238173712647183, backward:0.1792884688179551, data cost:1.0613510598440539 
2021-07-29 16:56:48,696: ============================================================
2021-07-29 16:56:48,696: Epoch 23/65 Batch 700/964 eta: 15:36:40.860510	Training Loss2 2.7116 (2.4535)	Training Loss3 4.0099 (3.6677)	Training Loss4 3.3568 (3.1113)	Training Total_Loss 7.0441 (6.4502)	Training Prec@1_up 96.680 (97.941)	Training Prec@1_down 93.555 (95.016)	
2021-07-29 16:56:48,696: ============================================================
2021-07-29 16:59:07,840: time cost, forward:0.024268713701651602, backward:0.17929263705753712, data cost:1.0607609751824294 
2021-07-29 16:59:07,840: ============================================================
2021-07-29 16:59:07,840: Epoch 23/65 Batch 800/964 eta: 15:42:46.346714	Training Loss2 2.4000 (2.4571)	Training Loss3 3.7422 (3.6707)	Training Loss4 2.9797 (3.1156)	Training Total_Loss 6.4321 (6.4571)	Training Prec@1_up 98.633 (97.936)	Training Prec@1_down 95.312 (95.001)	
2021-07-29 16:59:07,840: ============================================================
2021-07-29 17:01:26,378: time cost, forward:0.024286840861047865, backward:0.17930008969927524, data cost:1.0596569963503997 
2021-07-29 17:01:26,379: ============================================================
2021-07-29 17:01:26,379: Epoch 23/65 Batch 900/964 eta: 15:36:21.588611	Training Loss2 2.5229 (2.4615)	Training Loss3 3.8096 (3.6748)	Training Loss4 3.1171 (3.1204)	Training Total_Loss 6.6296 (6.4658)	Training Prec@1_up 97.852 (97.934)	Training Prec@1_down 94.922 (94.995)	
2021-07-29 17:01:26,379: ============================================================
2021-07-29 17:02:57,126: Epoch: 23/65 eta: 15:34:51.538473	Training Loss2 2.6804 (2.4644)	Training Loss3 4.0160 (3.6757)	Training Loss4 3.4153 (3.1236)	Training Total_Loss 7.0638 (6.4697)	Training Prec@1_up 97.266 (97.930)	Training Prec@1_down 92.969 (94.994)	
2021-07-29 17:02:57,127: ============================================================
2021-07-29 17:05:14,700: time cost, forward:0.024913192999483358, backward:0.1795795758565267, data cost:1.0524412188867125 
2021-07-29 17:05:14,700: ============================================================
2021-07-29 17:05:14,700: Epoch 24/65 Batch 100/964 eta: 15:26:01.992312	Training Loss2 2.0496 (2.3427)	Training Loss3 3.1215 (3.5629)	Training Loss4 2.7176 (2.9992)	Training Total_Loss 5.5051 (6.2339)	Training Prec@1_up 98.828 (98.177)	Training Prec@1_down 97.266 (95.336)	
2021-07-29 17:05:14,700: ============================================================
2021-07-29 17:07:31,403: time cost, forward:0.02465962165564149, backward:0.1797322040826232, data cost:1.0418554298841773 
2021-07-29 17:07:31,403: ============================================================
2021-07-29 17:07:31,403: Epoch 24/65 Batch 200/964 eta: 15:17:56.196966	Training Loss2 2.5055 (2.3597)	Training Loss3 3.6047 (3.5681)	Training Loss4 3.1222 (3.0118)	Training Total_Loss 6.4186 (6.2539)	Training Prec@1_up 97.461 (98.096)	Training Prec@1_down 94.727 (95.303)	
2021-07-29 17:07:31,403: ============================================================
2021-07-29 17:09:49,543: time cost, forward:0.024548080852597853, backward:0.1797851471597933, data cost:1.0431772338905463 
2021-07-29 17:09:49,543: ============================================================
2021-07-29 17:09:49,543: Epoch 24/65 Batch 300/964 eta: 15:25:17.223866	Training Loss2 2.3785 (2.3647)	Training Loss3 3.5159 (3.5661)	Training Loss4 3.0657 (3.0176)	Training Total_Loss 6.2380 (6.2573)	Training Prec@1_up 97.656 (98.126)	Training Prec@1_down 95.508 (95.312)	
2021-07-29 17:09:49,543: ============================================================
2021-07-29 17:12:06,150: time cost, forward:0.024212839609399475, backward:0.17959749071221603, data cost:1.0402376508354245 
2021-07-29 17:12:06,151: ============================================================
2021-07-29 17:12:06,151: Epoch 24/65 Batch 400/964 eta: 15:12:44.593035	Training Loss2 2.5252 (2.3726)	Training Loss3 3.7373 (3.5712)	Training Loss4 3.1894 (3.0253)	Training Total_Loss 6.5946 (6.2701)	Training Prec@1_up 97.656 (98.125)	Training Prec@1_down 95.117 (95.326)	
2021-07-29 17:12:06,151: ============================================================
2021-07-29 17:14:24,282: time cost, forward:0.023932221418392203, backward:0.17950474523112386, data cost:1.0415486716077418 
2021-07-29 17:14:24,283: ============================================================
2021-07-29 17:14:24,283: Epoch 24/65 Batch 500/964 eta: 15:20:37.614239	Training Loss2 2.4673 (2.3781)	Training Loss3 3.6862 (3.5750)	Training Loss4 3.1090 (3.0326)	Training Total_Loss 6.4744 (6.2803)	Training Prec@1_up 98.047 (98.113)	Training Prec@1_down 94.727 (95.313)	
2021-07-29 17:14:24,283: ============================================================
2021-07-29 17:16:41,696: time cost, forward:0.02370934255533107, backward:0.1793655967075558, data cost:1.0413279704538132 
2021-07-29 17:16:41,696: ============================================================
2021-07-29 17:16:41,696: Epoch 24/65 Batch 600/964 eta: 15:13:32.827771	Training Loss2 2.4490 (2.3846)	Training Loss3 3.5969 (3.5801)	Training Loss4 3.1657 (3.0406)	Training Total_Loss 6.4042 (6.2928)	Training Prec@1_up 98.047 (98.097)	Training Prec@1_down 95.312 (95.299)	
2021-07-29 17:16:41,696: ============================================================
2021-07-29 17:18:59,665: time cost, forward:0.02367358419175482, backward:0.17926584224673642, data cost:1.042007495064251 
2021-07-29 17:18:59,666: ============================================================
2021-07-29 17:18:59,666: Epoch 24/65 Batch 700/964 eta: 15:14:56.690832	Training Loss2 2.5031 (2.3881)	Training Loss3 3.7662 (3.5806)	Training Loss4 3.1251 (3.0450)	Training Total_Loss 6.5803 (6.2972)	Training Prec@1_up 96.680 (98.088)	Training Prec@1_down 93.164 (95.282)	
2021-07-29 17:18:59,666: ============================================================
2021-07-29 17:21:23,047: time cost, forward:0.023870306707293877, backward:0.17935773726548063, data cost:1.048922475795722 
2021-07-29 17:21:23,048: ============================================================
2021-07-29 17:21:23,048: Epoch 24/65 Batch 800/964 eta: 15:48:26.865787	Training Loss2 2.4966 (2.3936)	Training Loss3 3.6900 (3.5840)	Training Loss4 3.1362 (3.0516)	Training Total_Loss 6.5064 (6.3066)	Training Prec@1_up 97.266 (98.077)	Training Prec@1_down 94.141 (95.273)	
2021-07-29 17:21:23,048: ============================================================
2021-07-29 17:23:47,590: time cost, forward:0.023746030614426455, backward:0.17924100591555586, data cost:1.0555897806589807 
2021-07-29 17:23:47,590: ============================================================
2021-07-29 17:23:47,590: Epoch 24/65 Batch 900/964 eta: 15:53:42.910417	Training Loss2 2.6033 (2.4002)	Training Loss3 3.7186 (3.5899)	Training Loss4 3.3444 (3.0589)	Training Total_Loss 6.6925 (6.3194)	Training Prec@1_up 97.656 (98.053)	Training Prec@1_down 93.945 (95.237)	
2021-07-29 17:23:47,590: ============================================================
2021-07-29 17:25:22,088: Epoch: 24/65 eta: 15:52:08.957824	Training Loss2 2.3935 (2.4023)	Training Loss3 3.5932 (3.5913)	Training Loss4 3.0146 (3.0606)	Training Total_Loss 6.2972 (6.3227)	Training Prec@1_up 98.047 (98.052)	Training Prec@1_down 94.727 (95.241)	
2021-07-29 17:25:22,124: ============================================================
2021-07-29 17:28:36,987: time cost, forward:0.02762348483307193, backward:0.3097126580247975, data cost:1.3926336741206622 
2021-07-29 17:28:37,000: ============================================================
2021-07-29 17:28:37,000: Epoch 25/65 Batch 100/964 eta: 21:20:27.754629	Training Loss2 2.4481 (2.2547)	Training Loss3 3.5869 (3.4424)	Training Loss4 3.0717 (2.9058)	Training Total_Loss 6.3468 (6.0227)	Training Prec@1_up 97.852 (98.357)	Training Prec@1_down 95.703 (95.695)	
2021-07-29 17:28:37,000: ============================================================
2021-07-29 17:31:44,889: time cost, forward:0.02764761627618991, backward:0.28605594826703096, data cost:1.3908242448490469 
2021-07-29 17:31:45,267: ============================================================
2021-07-29 17:31:45,268: Epoch 25/65 Batch 200/964 eta: 20:33:56.250881	Training Loss2 2.2353 (2.2760)	Training Loss3 3.3584 (3.4583)	Training Loss4 2.8333 (2.9249)	Training Total_Loss 5.8927 (6.0588)	Training Prec@1_up 98.242 (98.290)	Training Prec@1_down 95.117 (95.682)	
2021-07-29 17:31:45,268: ============================================================
2021-07-29 17:36:00,570: time cost, forward:0.029635624742029503, backward:0.27415040504174887, data cost:1.6230502925987627 
2021-07-29 17:36:00,646: ============================================================
2021-07-29 17:36:00,669: Epoch 25/65 Batch 300/964 eta: 1 day, 3:49:32.223165	Training Loss2 2.3931 (2.2896)	Training Loss3 3.4690 (3.4687)	Training Loss4 3.0328 (2.9403)	Training Total_Loss 6.1819 (6.0836)	Training Prec@1_up 98.438 (98.240)	Training Prec@1_down 96.680 (95.626)	
2021-07-29 17:36:00,669: ============================================================
2021-07-29 17:39:04,446: time cost, forward:0.02922020161659795, backward:0.2679375281608792, data cost:1.5626877794289649 
2021-07-29 17:39:04,446: ============================================================
2021-07-29 17:39:04,446: Epoch 25/65 Batch 400/964 eta: 19:58:31.746437	Training Loss2 2.4685 (2.3049)	Training Loss3 3.5589 (3.4813)	Training Loss4 3.1734 (2.9570)	Training Total_Loss 6.3798 (6.1123)	Training Prec@1_up 97.852 (98.220)	Training Prec@1_down 96.289 (95.593)	
2021-07-29 17:39:04,446: ============================================================
2021-07-29 17:41:30,759: time cost, forward:0.028028984585840383, backward:0.25066988358277836, data cost:1.4743019368701087 
2021-07-29 17:41:30,759: ============================================================
2021-07-29 17:41:30,760: Epoch 25/65 Batch 500/964 eta: 15:51:38.842882	Training Loss2 2.3601 (2.3147)	Training Loss3 3.4921 (3.4890)	Training Loss4 3.0034 (2.9681)	Training Total_Loss 6.1739 (6.1304)	Training Prec@1_up 98.242 (98.215)	Training Prec@1_down 95.117 (95.568)	
2021-07-29 17:41:30,760: ============================================================
2021-07-29 17:43:58,238: time cost, forward:0.02737141849600612, backward:0.23836281223965805, data cost:1.4157747752678413 
2021-07-29 17:43:58,239: ============================================================
2021-07-29 17:43:58,239: Epoch 25/65 Batch 600/964 eta: 15:56:46.388529	Training Loss2 2.2952 (2.3205)	Training Loss3 3.3510 (3.4917)	Training Loss4 2.9381 (2.9734)	Training Total_Loss 5.9677 (6.1386)	Training Prec@1_up 98.242 (98.191)	Training Prec@1_down 96.484 (95.546)	
2021-07-29 17:43:58,239: ============================================================
2021-07-29 17:46:24,739: time cost, forward:0.026887036529562845, backward:0.22967719110808146, data cost:1.374125889613052 
2021-07-29 17:46:24,740: ============================================================
2021-07-29 17:46:24,740: Epoch 25/65 Batch 700/964 eta: 15:47:59.009519	Training Loss2 2.4135 (2.3275)	Training Loss3 3.5744 (3.4976)	Training Loss4 3.1299 (2.9806)	Training Total_Loss 6.3461 (6.1517)	Training Prec@1_up 98.633 (98.182)	Training Prec@1_down 95.312 (95.522)	
2021-07-29 17:46:24,740: ============================================================
2021-07-29 17:48:48,647: time cost, forward:0.026442703824765393, backward:0.2233049985911879, data cost:1.3404075407116522 
2021-07-29 17:48:48,648: ============================================================
2021-07-29 17:48:48,648: Epoch 25/65 Batch 800/964 eta: 15:28:48.405496	Training Loss2 2.3515 (2.3357)	Training Loss3 3.4550 (3.5039)	Training Loss4 2.9833 (2.9899)	Training Total_Loss 6.1224 (6.1667)	Training Prec@1_up 98.438 (98.167)	Training Prec@1_down 96.484 (95.498)	
2021-07-29 17:48:48,648: ============================================================
2021-07-29 17:51:39,929: time cost, forward:0.02637024424365153, backward:0.22066456802164489, data cost:1.3389822757814298 
2021-07-29 17:51:40,121: ============================================================
2021-07-29 17:51:40,121: Epoch 25/65 Batch 900/964 eta: 18:23:51.310990	Training Loss2 2.4491 (2.3420)	Training Loss3 3.6760 (3.5094)	Training Loss4 3.0018 (2.9976)	Training Total_Loss 6.4014 (6.1792)	Training Prec@1_up 97.070 (98.157)	Training Prec@1_down 94.141 (95.477)	
2021-07-29 17:51:40,121: ============================================================
2021-07-29 17:53:49,502: Epoch: 25/65 eta: 18:21:59.853768	Training Loss2 2.1446 (2.3440)	Training Loss3 3.2809 (3.5111)	Training Loss4 2.7598 (2.9995)	Training Total_Loss 5.7331 (6.1828)	Training Prec@1_up 99.023 (98.159)	Training Prec@1_down 96.680 (95.478)	
2021-07-29 17:53:49,502: ============================================================
2021-07-29 17:53:49,562: Save Checkpoint...
2021-07-29 17:53:49,627: ============================================================
2021-07-29 17:53:50,816: Save done!
2021-07-29 17:53:50,816: ============================================================
2021-07-29 17:56:43,953: time cost, forward:0.02639029724429352, backward:0.25006557233405835, data cost:1.2825885902751575 
2021-07-29 17:56:43,954: ============================================================
2021-07-29 17:56:43,954: Epoch 26/65 Batch 100/964 eta: 18:29:32.936589	Training Loss2 2.1183 (2.2261)	Training Loss3 3.2199 (3.3839)	Training Loss4 2.7518 (2.8753)	Training Total_Loss 5.6549 (5.9346)	Training Prec@1_up 97.852 (98.359)	Training Prec@1_down 95.117 (95.877)	
2021-07-29 17:56:43,954: ============================================================
2021-07-29 17:59:06,672: time cost, forward:0.025410120211653973, backward:0.21393697106059473, data cost:1.1875982919530055 
2021-07-29 17:59:06,673: ============================================================
2021-07-29 17:59:06,673: Epoch 26/65 Batch 200/964 eta: 15:12:28.408740	Training Loss2 2.4392 (2.2450)	Training Loss3 3.5747 (3.4059)	Training Loss4 3.0592 (2.8942)	Training Total_Loss 6.3239 (5.9755)	Training Prec@1_up 97.656 (98.312)	Training Prec@1_down 95.117 (95.767)	
2021-07-29 17:59:06,673: ============================================================
2021-07-29 18:01:29,612: time cost, forward:0.025220186814017918, backward:0.2018005784139984, data cost:1.155686937446977 
2021-07-29 18:01:29,612: ============================================================
2021-07-29 18:01:29,612: Epoch 26/65 Batch 300/964 eta: 15:11:30.076110	Training Loss2 2.4173 (2.2508)	Training Loss3 3.5231 (3.4058)	Training Loss4 2.9944 (2.9009)	Training Total_Loss 6.2290 (5.9817)	Training Prec@1_up 97.656 (98.333)	Training Prec@1_down 95.898 (95.785)	
2021-07-29 18:01:29,613: ============================================================
2021-07-29 18:03:53,402: time cost, forward:0.0246939007799727, backward:0.19574648993355886, data cost:1.138743215336238 
2021-07-29 18:03:53,402: ============================================================
2021-07-29 18:03:53,402: Epoch 26/65 Batch 400/964 eta: 15:14:31.706071	Training Loss2 2.0612 (2.2585)	Training Loss3 3.2543 (3.4135)	Training Loss4 2.7342 (2.9102)	Training Total_Loss 5.6519 (5.9979)	Training Prec@1_up 98.242 (98.315)	Training Prec@1_down 96.484 (95.746)	
2021-07-29 18:03:53,403: ============================================================
2021-07-29 18:06:17,221: time cost, forward:0.02427299180346166, backward:0.192175266499032, data cost:1.13071310066269 
2021-07-29 18:06:17,221: ============================================================
2021-07-29 18:06:17,222: Epoch 26/65 Batch 500/964 eta: 15:12:18.995418	Training Loss2 2.2853 (2.2634)	Training Loss3 3.3125 (3.4159)	Training Loss4 2.9583 (2.9153)	Training Total_Loss 5.9343 (6.0052)	Training Prec@1_up 98.047 (98.299)	Training Prec@1_down 96.680 (95.765)	
2021-07-29 18:06:17,222: ============================================================
2021-07-29 18:08:40,590: time cost, forward:0.024099769894786193, backward:0.18984522763795167, data cost:1.1250709098249128 
2021-07-29 18:08:40,591: ============================================================
2021-07-29 18:08:40,591: Epoch 26/65 Batch 600/964 eta: 15:07:04.542657	Training Loss2 2.0447 (2.2705)	Training Loss3 3.0626 (3.4216)	Training Loss4 2.6513 (2.9229)	Training Total_Loss 5.4106 (6.0183)	Training Prec@1_up 98.828 (98.282)	Training Prec@1_down 97.852 (95.743)	
2021-07-29 18:08:40,591: ============================================================
2021-07-29 18:11:02,033: time cost, forward:0.024004517365593427, backward:0.18819545028206275, data cost:1.118837467420084 
2021-07-29 18:11:02,034: ============================================================
2021-07-29 18:11:02,034: Epoch 26/65 Batch 700/964 eta: 14:52:31.585318	Training Loss2 2.1172 (2.2752)	Training Loss3 3.2383 (3.4252)	Training Loss4 2.8117 (2.9284)	Training Total_Loss 5.7027 (6.0270)	Training Prec@1_up 98.438 (98.276)	Training Prec@1_down 96.289 (95.741)	
2021-07-29 18:11:02,034: ============================================================
2021-07-29 18:13:25,834: time cost, forward:0.023882869188120128, backward:0.18691452960944147, data cost:1.1166102083514122 
2021-07-29 18:13:25,834: ============================================================
2021-07-29 18:13:25,834: Epoch 26/65 Batch 800/964 eta: 15:05:00.398558	Training Loss2 2.6129 (2.2794)	Training Loss3 3.7263 (3.4271)	Training Loss4 3.2568 (2.9327)	Training Total_Loss 6.6612 (6.0332)	Training Prec@1_up 97.656 (98.270)	Training Prec@1_down 95.117 (95.738)	
2021-07-29 18:13:25,834: ============================================================
2021-07-29 18:15:48,412: time cost, forward:0.023935049606509946, backward:0.18593515437490019, data cost:1.113774824460701 
2021-07-29 18:15:48,412: ============================================================
2021-07-29 18:15:48,412: Epoch 26/65 Batch 900/964 eta: 14:54:56.422721	Training Loss2 2.3424 (2.2872)	Training Loss3 3.5064 (3.4346)	Training Loss4 2.9515 (2.9407)	Training Total_Loss 6.1534 (6.0485)	Training Prec@1_up 97.656 (98.252)	Training Prec@1_down 95.898 (95.703)	
2021-07-29 18:15:48,413: ============================================================
2021-07-29 18:17:22,117: Epoch: 26/65 eta: 14:53:23.746811	Training Loss2 2.1580 (2.2891)	Training Loss3 3.2374 (3.4341)	Training Loss4 2.7838 (2.9425)	Training Total_Loss 5.7083 (6.0499)	Training Prec@1_up 98.633 (98.250)	Training Prec@1_down 95.703 (95.700)	
2021-07-29 18:17:22,118: ============================================================
2021-07-29 18:17:22,120: Save Checkpoint...
2021-07-29 18:17:22,121: ============================================================
2021-07-29 18:17:22,829: Save done!
2021-07-29 18:17:22,830: ============================================================
2021-07-29 18:19:44,175: time cost, forward:0.023475808326644126, backward:0.17963660124576453, data cost:1.0907378678370003 
2021-07-29 18:19:44,175: ============================================================
2021-07-29 18:19:44,175: Epoch 27/65 Batch 100/964 eta: 14:43:19.756768	Training Loss2 2.3754 (2.1856)	Training Loss3 3.5870 (3.3335)	Training Loss4 3.0603 (2.8309)	Training Total_Loss 6.3049 (5.8417)	Training Prec@1_up 98.633 (98.422)	Training Prec@1_down 95.508 (95.975)	
2021-07-29 18:19:44,175: ============================================================
2021-07-29 18:22:02,385: time cost, forward:0.023543843072862482, backward:0.17855172660482588, data cost:1.061370607596546 
2021-07-29 18:22:02,385: ============================================================
2021-07-29 18:22:02,386: Epoch 27/65 Batch 200/964 eta: 14:21:26.501510	Training Loss2 2.0876 (2.2007)	Training Loss3 3.3547 (3.3376)	Training Loss4 2.8533 (2.8492)	Training Total_Loss 5.8251 (5.8626)	Training Prec@1_up 99.609 (98.403)	Training Prec@1_down 96.680 (95.942)	
2021-07-29 18:22:02,386: ============================================================
2021-07-29 18:24:21,845: time cost, forward:0.02307221562567363, backward:0.1782550963271023, data cost:1.0604303401449453 
2021-07-29 18:24:21,845: ============================================================
2021-07-29 18:24:21,845: Epoch 27/65 Batch 300/964 eta: 14:26:54.269054	Training Loss2 2.2500 (2.2049)	Training Loss3 3.3583 (3.3364)	Training Loss4 2.8957 (2.8537)	Training Total_Loss 5.9312 (5.8658)	Training Prec@1_up 98.047 (98.405)	Training Prec@1_down 96.289 (95.966)	
2021-07-29 18:24:21,845: ============================================================
2021-07-29 18:26:41,243: time cost, forward:0.022778415440915523, backward:0.1780022188535609, data cost:1.0603774693376737 
2021-07-29 18:26:41,243: ============================================================
2021-07-29 18:26:41,243: Epoch 27/65 Batch 400/964 eta: 14:24:11.921241	Training Loss2 2.0656 (2.2071)	Training Loss3 3.3622 (3.3393)	Training Loss4 2.6700 (2.8553)	Training Total_Loss 5.7300 (5.8704)	Training Prec@1_up 98.633 (98.391)	Training Prec@1_down 95.312 (95.931)	
2021-07-29 18:26:41,244: ============================================================
2021-07-29 18:28:59,906: time cost, forward:0.022588991211029235, backward:0.1778910833752466, data cost:1.058721123811955 
2021-07-29 18:28:59,906: ============================================================
2021-07-29 18:28:59,906: Epoch 27/65 Batch 500/964 eta: 14:17:19.797141	Training Loss2 2.4041 (2.2108)	Training Loss3 3.5479 (3.3417)	Training Loss4 3.1267 (2.8600)	Training Total_Loss 6.3133 (5.8771)	Training Prec@1_up 97.461 (98.387)	Training Prec@1_down 95.117 (95.938)	
2021-07-29 18:28:59,906: ============================================================
2021-07-29 18:31:19,779: time cost, forward:0.022508147562883534, backward:0.1779204501532553, data cost:1.059368621526855 
2021-07-29 18:31:19,780: ============================================================
2021-07-29 18:31:19,780: Epoch 27/65 Batch 600/964 eta: 14:22:29.064726	Training Loss2 2.2618 (2.2158)	Training Loss3 3.4931 (3.3469)	Training Loss4 2.8574 (2.8667)	Training Total_Loss 6.0527 (5.8881)	Training Prec@1_up 97.656 (98.384)	Training Prec@1_down 96.484 (95.918)	
2021-07-29 18:31:19,780: ============================================================
2021-07-29 18:33:38,080: time cost, forward:0.022815675694543406, backward:0.178011580086573, data cost:1.0576618727355216 
2021-07-29 18:33:38,080: ============================================================
2021-07-29 18:33:38,080: Epoch 27/65 Batch 700/964 eta: 14:10:28.615200	Training Loss2 2.2859 (2.2205)	Training Loss3 3.4142 (3.3486)	Training Loss4 2.9324 (2.8716)	Training Total_Loss 6.0234 (5.8947)	Training Prec@1_up 98.242 (98.384)	Training Prec@1_down 95.508 (95.920)	
2021-07-29 18:33:38,080: ============================================================
2021-07-29 18:35:56,252: time cost, forward:0.022894880201700184, backward:0.17806881211129238, data cost:1.0564894416603785 
2021-07-29 18:35:56,253: ============================================================
2021-07-29 18:35:56,253: Epoch 27/65 Batch 800/964 eta: 14:07:23.411639	Training Loss2 2.2919 (2.2238)	Training Loss3 3.4634 (3.3486)	Training Loss4 2.8886 (2.8748)	Training Total_Loss 6.0536 (5.8979)	Training Prec@1_up 97.852 (98.376)	Training Prec@1_down 95.312 (95.926)	
2021-07-29 18:35:56,253: ============================================================
2021-07-29 18:38:15,371: time cost, forward:0.02277409645818895, backward:0.17798909886395176, data cost:1.0563221810524404 
2021-07-29 18:38:15,371: ============================================================
2021-07-29 18:38:15,371: Epoch 27/65 Batch 900/964 eta: 14:10:52.335979	Training Loss2 2.2359 (2.2313)	Training Loss3 3.3160 (3.3560)	Training Loss4 2.8128 (2.8826)	Training Total_Loss 5.8403 (5.9130)	Training Prec@1_up 98.633 (98.355)	Training Prec@1_down 95.898 (95.905)	
2021-07-29 18:38:15,372: ============================================================
2021-07-29 18:40:21,283: Epoch: 27/65 eta: 14:09:21.908918	Training Loss2 2.2419 (2.2361)	Training Loss3 3.4201 (3.3593)	Training Loss4 2.9168 (2.8878)	Training Total_Loss 5.9994 (5.9213)	Training Prec@1_up 97.266 (98.337)	Training Prec@1_down 95.508 (95.895)	
2021-07-29 18:40:21,284: ============================================================
2021-07-29 18:40:21,285: Save Checkpoint...
2021-07-29 18:40:21,286: ============================================================
2021-07-29 18:40:21,980: Save done!
2021-07-29 18:40:21,980: ============================================================
2021-07-29 18:42:44,479: time cost, forward:0.022524566361398407, backward:0.17676465198247118, data cost:1.0991304522812968 
2021-07-29 18:42:44,479: ============================================================
2021-07-29 18:42:44,479: Epoch 28/65 Batch 100/964 eta: 14:27:38.699888	Training Loss2 2.1378 (2.1218)	Training Loss3 3.2214 (3.2420)	Training Loss4 2.7349 (2.7655)	Training Total_Loss 5.6577 (5.6856)	Training Prec@1_up 98.633 (98.536)	Training Prec@1_down 96.289 (96.232)	
2021-07-29 18:42:44,479: ============================================================
2021-07-29 18:45:03,936: time cost, forward:0.022376537322998047, backward:0.17636675810694097, data cost:1.0808976511260373 
2021-07-29 18:45:03,937: ============================================================
2021-07-29 18:45:03,937: Epoch 28/65 Batch 200/964 eta: 14:06:48.590491	Training Loss2 2.1795 (2.1315)	Training Loss3 3.2149 (3.2445)	Training Loss4 2.7551 (2.7755)	Training Total_Loss 5.6822 (5.6980)	Training Prec@1_up 98.047 (98.527)	Training Prec@1_down 96.484 (96.171)	
2021-07-29 18:45:03,937: ============================================================
2021-07-29 18:47:38,186: time cost, forward:0.02458692633587381, backward:0.1811460412066916, data cost:1.1182112877185528 
2021-07-29 18:47:38,186: ============================================================
2021-07-29 18:47:38,186: Epoch 28/65 Batch 300/964 eta: 15:34:03.398517	Training Loss2 2.4374 (2.1357)	Training Loss3 3.6394 (3.2453)	Training Loss4 3.0501 (2.7797)	Training Total_Loss 6.3832 (5.7030)	Training Prec@1_up 97.656 (98.523)	Training Prec@1_down 94.531 (96.185)	
2021-07-29 18:47:38,186: ============================================================
2021-07-29 18:50:03,132: time cost, forward:0.025052978878929502, backward:0.1806250915192721, data cost:1.1173207180243088 
2021-07-29 18:50:03,133: ============================================================
2021-07-29 18:50:03,133: Epoch 28/65 Batch 400/964 eta: 14:35:18.689713	Training Loss2 2.2235 (2.1416)	Training Loss3 3.2714 (3.2477)	Training Loss4 2.8773 (2.7856)	Training Total_Loss 5.8219 (5.7114)	Training Prec@1_up 97.461 (98.517)	Training Prec@1_down 95.508 (96.219)	
2021-07-29 18:50:03,133: ============================================================
2021-07-29 18:52:40,605: time cost, forward:0.024705370824657128, backward:0.1801504429452166, data cost:1.1388140417530923 
2021-07-29 18:52:40,605: ============================================================
2021-07-29 18:52:40,605: Epoch 28/65 Batch 500/964 eta: 15:48:19.372622	Training Loss2 2.3156 (2.1459)	Training Loss3 3.4818 (3.2497)	Training Loss4 2.9360 (2.7911)	Training Total_Loss 6.1076 (5.7182)	Training Prec@1_up 97.461 (98.501)	Training Prec@1_down 95.508 (96.214)	
2021-07-29 18:52:40,605: ============================================================
2021-07-29 18:55:04,256: time cost, forward:0.02483330346109075, backward:0.1798744130015174, data cost:1.1324269528779045 
2021-07-29 18:55:04,305: ============================================================
2021-07-29 18:55:04,305: Epoch 28/65 Batch 600/964 eta: 14:22:59.367582	Training Loss2 2.1710 (2.1559)	Training Loss3 3.3212 (3.2585)	Training Loss4 2.8526 (2.8028)	Training Total_Loss 5.8330 (5.7379)	Training Prec@1_up 99.023 (98.478)	Training Prec@1_down 96.094 (96.202)	
2021-07-29 18:55:04,305: ============================================================
2021-07-29 18:57:26,107: time cost, forward:0.02490364908319345, backward:0.17969107048023752, data cost:1.1250526465059862 
2021-07-29 18:57:26,123: ============================================================
2021-07-29 18:57:26,123: Epoch 28/65 Batch 700/964 eta: 14:09:19.528534	Training Loss2 2.2264 (2.1688)	Training Loss3 3.3377 (3.2710)	Training Loss4 2.9545 (2.8165)	Training Total_Loss 5.9282 (5.7637)	Training Prec@1_up 98.828 (98.441)	Training Prec@1_down 96.680 (96.145)	
2021-07-29 18:57:26,124: ============================================================
2021-07-29 18:59:46,240: time cost, forward:0.024851829447644823, backward:0.17947098698574251, data cost:1.119093226252569 
2021-07-29 18:59:46,240: ============================================================
2021-07-29 18:59:46,241: Epoch 28/65 Batch 800/964 eta: 13:56:48.215633	Training Loss2 2.3013 (2.1778)	Training Loss3 3.4850 (3.2790)	Training Loss4 2.9357 (2.8266)	Training Total_Loss 6.1035 (5.7812)	Training Prec@1_up 98.242 (98.426)	Training Prec@1_down 95.117 (96.119)	
2021-07-29 18:59:46,241: ============================================================
2021-07-29 19:02:05,180: time cost, forward:0.024595323473513456, backward:0.17934055826953044, data cost:1.1125022304203938 
2021-07-29 19:02:05,181: ============================================================
2021-07-29 19:02:05,181: Epoch 28/65 Batch 900/964 eta: 13:47:27.478526	Training Loss2 2.0495 (2.1828)	Training Loss3 3.0795 (3.2842)	Training Loss4 2.7315 (2.8326)	Training Total_Loss 5.4700 (5.7919)	Training Prec@1_up 98.633 (98.420)	Training Prec@1_down 96.875 (96.098)	
2021-07-29 19:02:05,181: ============================================================
2021-07-29 19:03:36,776: Epoch: 28/65 eta: 13:45:57.167438	Training Loss2 2.3431 (2.1856)	Training Loss3 3.4577 (3.2864)	Training Loss4 2.9961 (2.8357)	Training Total_Loss 6.1273 (5.7970)	Training Prec@1_up 98.047 (98.418)	Training Prec@1_down 94.922 (96.093)	
2021-07-29 19:03:36,777: ============================================================
2021-07-29 19:03:37,086: Save Checkpoint...
2021-07-29 19:03:37,087: ============================================================
2021-07-29 19:03:38,020: Save done!
2021-07-29 19:03:38,020: ============================================================
2021-07-29 19:05:58,571: time cost, forward:0.02448087509232338, backward:0.17924564534967596, data cost:1.0638902957993324 
2021-07-29 19:05:58,571: ============================================================
2021-07-29 19:05:58,571: Epoch 29/65 Batch 100/964 eta: 13:53:07.018841	Training Loss2 2.2356 (2.0832)	Training Loss3 3.4346 (3.1910)	Training Loss4 2.8636 (2.7285)	Training Total_Loss 5.9842 (5.5968)	Training Prec@1_up 98.242 (98.562)	Training Prec@1_down 95.117 (96.299)	
2021-07-29 19:05:58,572: ============================================================
2021-07-29 19:08:17,025: time cost, forward:0.024195237375384, backward:0.17923884894979658, data cost:1.0599746991641557 
2021-07-29 19:08:17,026: ============================================================
2021-07-29 19:08:17,026: Epoch 29/65 Batch 200/964 eta: 13:38:28.395640	Training Loss2 2.1202 (2.0811)	Training Loss3 3.2592 (3.1743)	Training Loss4 2.7816 (2.7254)	Training Total_Loss 5.7101 (5.5775)	Training Prec@1_up 98.242 (98.581)	Training Prec@1_down 96.484 (96.405)	
2021-07-29 19:08:17,026: ============================================================
2021-07-29 19:10:35,369: time cost, forward:0.024203046907150624, backward:0.17867978918911223, data cost:1.0536271202125678 
2021-07-29 19:10:35,371: ============================================================
2021-07-29 19:10:35,372: Epoch 29/65 Batch 300/964 eta: 13:35:31.530781	Training Loss2 2.1466 (2.0911)	Training Loss3 3.2834 (3.1818)	Training Loss4 2.7542 (2.7355)	Training Total_Loss 5.7338 (5.5951)	Training Prec@1_up 98.828 (98.550)	Training Prec@1_down 95.703 (96.361)	
2021-07-29 19:10:35,372: ============================================================
2021-07-29 19:12:53,644: time cost, forward:0.023774582640569013, backward:0.17870110736454936, data cost:1.0520715970443304 
2021-07-29 19:12:53,645: ============================================================
2021-07-29 19:12:53,645: Epoch 29/65 Batch 400/964 eta: 13:32:47.573995	Training Loss2 2.3187 (2.0978)	Training Loss3 3.4888 (3.1829)	Training Loss4 3.0154 (2.7422)	Training Total_Loss 6.1559 (5.6029)	Training Prec@1_up 98.242 (98.538)	Training Prec@1_down 94.531 (96.351)	
2021-07-29 19:12:53,645: ============================================================
2021-07-29 19:15:23,235: time cost, forward:0.023843983132280186, backward:0.19040837651025316, data cost:1.0547210807074048 
2021-07-29 19:15:23,254: ============================================================
2021-07-29 19:15:23,254: Epoch 29/65 Batch 500/964 eta: 14:36:56.116566	Training Loss2 2.3617 (2.1077)	Training Loss3 3.3710 (3.1899)	Training Loss4 3.1322 (2.7536)	Training Total_Loss 6.1180 (5.6206)	Training Prec@1_up 97.852 (98.532)	Training Prec@1_down 95.508 (96.344)	
2021-07-29 19:15:23,254: ============================================================
2021-07-29 19:17:54,979: time cost, forward:0.023664359457305755, backward:0.20607903007672904, data cost:1.044382833280229 
2021-07-29 19:17:54,979: ============================================================
2021-07-29 19:17:54,979: Epoch 29/65 Batch 600/964 eta: 14:46:48.415719	Training Loss2 2.1259 (2.1129)	Training Loss3 3.2064 (3.1935)	Training Loss4 2.7443 (2.7597)	Training Total_Loss 5.6415 (5.6298)	Training Prec@1_up 97.852 (98.523)	Training Prec@1_down 95.508 (96.347)	
2021-07-29 19:17:54,979: ============================================================
2021-07-29 19:20:13,421: time cost, forward:0.023511538348654992, backward:0.20230130439834704, data cost:1.044599367995801 
2021-07-29 19:20:13,422: ============================================================
2021-07-29 19:20:13,422: Epoch 29/65 Batch 700/964 eta: 13:26:51.962534	Training Loss2 2.2176 (2.1188)	Training Loss3 3.3342 (3.1983)	Training Loss4 2.8598 (2.7657)	Training Total_Loss 5.8729 (5.6406)	Training Prec@1_up 98.047 (98.505)	Training Prec@1_down 95.117 (96.317)	
2021-07-29 19:20:13,422: ============================================================
2021-07-29 19:22:31,385: time cost, forward:0.023433555798775263, backward:0.1992767439019844, data cost:1.0444831922743587 
2021-07-29 19:22:31,386: ============================================================
2021-07-29 19:22:31,386: Epoch 29/65 Batch 800/964 eta: 13:21:46.776144	Training Loss2 2.1199 (2.1254)	Training Loss3 3.1996 (3.2038)	Training Loss4 2.7856 (2.7738)	Training Total_Loss 5.6523 (5.6534)	Training Prec@1_up 97.852 (98.505)	Training Prec@1_down 96.289 (96.309)	
2021-07-29 19:22:31,386: ============================================================
2021-07-29 19:24:53,828: time cost, forward:0.023491008660949775, backward:0.1969184347731916, data cost:1.0474780748895596 
2021-07-29 19:24:53,828: ============================================================
2021-07-29 19:24:53,828: Epoch 29/65 Batch 900/964 eta: 13:45:25.790754	Training Loss2 2.1009 (2.1315)	Training Loss3 3.2371 (3.2100)	Training Loss4 2.7397 (2.7800)	Training Total_Loss 5.6574 (5.6657)	Training Prec@1_up 98.828 (98.495)	Training Prec@1_down 96.680 (96.289)	
2021-07-29 19:24:53,829: ============================================================
2021-07-29 19:26:24,534: Epoch: 29/65 eta: 13:43:53.203207	Training Loss2 2.4075 (2.1360)	Training Loss3 3.5110 (3.2134)	Training Loss4 3.0485 (2.7847)	Training Total_Loss 6.2390 (5.6738)	Training Prec@1_up 97.656 (98.492)	Training Prec@1_down 95.898 (96.282)	
2021-07-29 19:26:24,535: ============================================================
2021-07-29 19:26:24,536: Save Checkpoint...
2021-07-29 19:26:24,537: ============================================================
2021-07-29 19:26:25,259: Save done!
2021-07-29 19:26:25,259: ============================================================
2021-07-29 19:28:47,893: time cost, forward:0.025018569194909298, backward:0.17806313254616477, data cost:1.0856543911827936 
2021-07-29 19:28:47,894: ============================================================
2021-07-29 19:28:47,894: Epoch 30/65 Batch 100/964 eta: 13:42:38.276919	Training Loss2 2.0498 (2.0205)	Training Loss3 3.0733 (3.0885)	Training Loss4 2.6869 (2.6545)	Training Total_Loss 5.4416 (5.4260)	Training Prec@1_up 99.023 (98.572)	Training Prec@1_down 96.484 (96.457)	
2021-07-29 19:28:47,894: ============================================================
2021-07-29 19:31:09,124: time cost, forward:0.02390157277859635, backward:0.17871306889021216, data cost:1.0802698866206797 
2021-07-29 19:31:09,125: ============================================================
2021-07-29 19:31:09,125: Epoch 30/65 Batch 200/964 eta: 13:32:11.614046	Training Loss2 1.8788 (2.0132)	Training Loss3 2.8713 (3.0764)	Training Loss4 2.5238 (2.6467)	Training Total_Loss 5.0726 (5.4063)	Training Prec@1_up 99.023 (98.601)	Training Prec@1_down 98.047 (96.556)	
2021-07-29 19:31:09,125: ============================================================
2021-07-29 19:33:28,003: time cost, forward:0.023316043675145177, backward:0.17872774162420063, data cost:1.07158423347218 
2021-07-29 19:33:28,004: ============================================================
2021-07-29 19:33:28,004: Epoch 30/65 Batch 300/964 eta: 13:16:21.437724	Training Loss2 1.8970 (2.0072)	Training Loss3 2.9425 (3.0660)	Training Loss4 2.4932 (2.6431)	Training Total_Loss 5.1376 (5.3911)	Training Prec@1_up 99.219 (98.627)	Training Prec@1_down 97.266 (96.608)	
2021-07-29 19:33:28,005: ============================================================
2021-07-29 19:35:46,164: time cost, forward:0.023020250755443908, backward:0.17877507508547982, data cost:1.065674100603376 
2021-07-29 19:35:46,164: ============================================================
2021-07-29 19:35:46,164: Epoch 30/65 Batch 400/964 eta: 13:09:55.855015	Training Loss2 1.8555 (2.0069)	Training Loss3 2.9501 (3.0657)	Training Loss4 2.4893 (2.6430)	Training Total_Loss 5.1225 (5.3906)	Training Prec@1_up 99.805 (98.639)	Training Prec@1_down 98.242 (96.608)	
2021-07-29 19:35:46,165: ============================================================
2021-07-29 19:38:05,173: time cost, forward:0.022894120168590354, backward:0.17886032274586405, data cost:1.0631921148013495 
2021-07-29 19:38:05,173: ============================================================
2021-07-29 19:38:05,173: Epoch 30/65 Batch 500/964 eta: 13:12:28.007024	Training Loss2 1.8991 (2.0061)	Training Loss3 2.6846 (3.0634)	Training Loss4 2.6002 (2.6428)	Training Total_Loss 4.9343 (5.3879)	Training Prec@1_up 99.414 (98.646)	Training Prec@1_down 98.828 (96.639)	
2021-07-29 19:38:05,174: ============================================================
2021-07-29 19:40:23,585: time cost, forward:0.02270834354407003, backward:0.1787312233786352, data cost:1.061587856687568 
2021-07-29 19:40:23,586: ============================================================
2021-07-29 19:40:23,586: Epoch 30/65 Batch 600/964 eta: 13:06:45.573498	Training Loss2 1.9708 (2.0056)	Training Loss3 3.0106 (3.0625)	Training Loss4 2.6785 (2.6417)	Training Total_Loss 5.3353 (5.3861)	Training Prec@1_up 99.414 (98.653)	Training Prec@1_down 97.266 (96.640)	
2021-07-29 19:40:23,586: ============================================================
2021-07-29 19:42:42,370: time cost, forward:0.022680411864077415, backward:0.17858321676950087, data cost:1.0612087126965175 
2021-07-29 19:42:42,370: ============================================================
2021-07-29 19:42:42,370: Epoch 30/65 Batch 700/964 eta: 13:06:33.672753	Training Loss2 2.1556 (2.0042)	Training Loss3 3.2372 (3.0619)	Training Loss4 2.8202 (2.6408)	Training Total_Loss 5.7251 (5.3844)	Training Prec@1_up 97.461 (98.649)	Training Prec@1_down 95.312 (96.639)	
2021-07-29 19:42:42,371: ============================================================
2021-07-29 19:45:03,483: time cost, forward:0.022873419247222634, backward:0.17854654744211515, data cost:1.0638188865217608 
2021-07-29 19:45:03,507: ============================================================
2021-07-29 19:45:03,507: Epoch 30/65 Batch 800/964 eta: 13:17:32.441838	Training Loss2 1.9903 (2.0073)	Training Loss3 3.0712 (3.0683)	Training Loss4 2.6395 (2.6443)	Training Total_Loss 5.3861 (5.3941)	Training Prec@1_up 98.633 (98.650)	Training Prec@1_down 97.070 (96.623)	
2021-07-29 19:45:03,507: ============================================================
2021-07-29 19:47:25,872: time cost, forward:0.022885104042536956, backward:0.1784951411046759, data cost:1.0665483334173218 
2021-07-29 19:47:25,873: ============================================================
2021-07-29 19:47:25,873: Epoch 30/65 Batch 900/964 eta: 13:22:06.726096	Training Loss2 2.0284 (2.0084)	Training Loss3 3.0937 (3.0694)	Training Loss4 2.6685 (2.6454)	Training Total_Loss 5.4422 (5.3963)	Training Prec@1_up 98.633 (98.642)	Training Prec@1_down 96.680 (96.611)	
2021-07-29 19:47:25,873: ============================================================
2021-07-29 19:48:59,514: Epoch: 30/65 eta: 13:20:34.188389	Training Loss2 2.0584 (2.0062)	Training Loss3 3.0700 (3.0669)	Training Loss4 2.6872 (2.6427)	Training Total_Loss 5.4428 (5.3914)	Training Prec@1_up 98.242 (98.639)	Training Prec@1_down 96.484 (96.614)	
2021-07-29 19:48:59,514: ============================================================
2021-07-29 19:48:59,517: Save Checkpoint...
2021-07-29 19:48:59,519: ============================================================
2021-07-29 19:49:00,269: Save done!
2021-07-29 19:49:00,269: ============================================================
2021-07-29 19:51:22,818: time cost, forward:0.023493887198091756, backward:0.1795771603632455, data cost:1.1022587280080776 
2021-07-29 19:51:22,818: ============================================================
2021-07-29 19:51:22,818: Epoch 31/65 Batch 100/964 eta: 13:19:14.447831	Training Loss2 1.8085 (1.9725)	Training Loss3 2.7268 (3.0197)	Training Loss4 2.4119 (2.6043)	Training Total_Loss 4.8370 (5.3081)	Training Prec@1_up 98.633 (98.676)	Training Prec@1_down 97.070 (96.660)	
2021-07-29 19:51:22,818: ============================================================
2021-07-29 19:53:43,493: time cost, forward:0.02402901769283429, backward:0.1793460702177268, data cost:1.0872987155339227 
2021-07-29 19:53:43,493: ============================================================
2021-07-29 19:53:43,493: Epoch 31/65 Batch 200/964 eta: 13:06:23.795017	Training Loss2 2.1892 (1.9803)	Training Loss3 3.2423 (3.0342)	Training Loss4 2.8317 (2.6148)	Training Total_Loss 5.7528 (5.3318)	Training Prec@1_up 97.656 (98.637)	Training Prec@1_down 94.922 (96.632)	
2021-07-29 19:53:43,494: ============================================================
2021-07-29 19:56:04,881: time cost, forward:0.024229352689507017, backward:0.1793860272818983, data cost:1.0845054248503618 
2021-07-29 19:56:04,881: ============================================================
2021-07-29 19:56:04,882: Epoch 31/65 Batch 300/964 eta: 13:08:01.673104	Training Loss2 1.8156 (1.9825)	Training Loss3 2.8443 (3.0365)	Training Loss4 2.4762 (2.6188)	Training Total_Loss 4.9902 (5.3371)	Training Prec@1_up 99.219 (98.643)	Training Prec@1_down 97.852 (96.637)	
2021-07-29 19:56:04,882: ============================================================
2021-07-29 19:58:24,779: time cost, forward:0.024070232434380324, backward:0.1793085399426912, data cost:1.0794292099792557 
2021-07-29 19:58:24,780: ============================================================
2021-07-29 19:58:24,780: Epoch 31/65 Batch 400/964 eta: 12:57:23.494009	Training Loss2 2.1153 (1.9844)	Training Loss3 3.1263 (3.0400)	Training Loss4 2.7731 (2.6220)	Training Total_Loss 5.5705 (5.3432)	Training Prec@1_up 98.828 (98.642)	Training Prec@1_down 96.484 (96.646)	
2021-07-29 19:58:24,780: ============================================================
2021-07-29 20:00:45,368: time cost, forward:0.02403452831184219, backward:0.17917811321113297, data cost:1.0778884773025055 
2021-07-29 20:00:45,368: ============================================================
2021-07-29 20:00:45,369: Epoch 31/65 Batch 500/964 eta: 12:58:53.092797	Training Loss2 1.8122 (1.9884)	Training Loss3 2.8621 (3.0424)	Training Loss4 2.4253 (2.6253)	Training Total_Loss 4.9809 (5.3492)	Training Prec@1_up 99.414 (98.655)	Training Prec@1_down 97.656 (96.647)	
2021-07-29 20:00:45,369: ============================================================
2021-07-29 20:03:04,004: time cost, forward:0.023858283318342867, backward:0.17900978384511498, data cost:1.073651567325369 
2021-07-29 20:03:04,005: ============================================================
2021-07-29 20:03:04,005: Epoch 31/65 Batch 600/964 eta: 12:45:45.422552	Training Loss2 1.9668 (1.9924)	Training Loss3 3.1442 (3.0504)	Training Loss4 2.5478 (2.6287)	Training Total_Loss 5.4015 (5.3610)	Training Prec@1_up 99.023 (98.652)	Training Prec@1_down 96.484 (96.631)	
2021-07-29 20:03:04,005: ============================================================
2021-07-29 20:05:26,236: time cost, forward:0.02365951613124689, backward:0.17890922842448703, data cost:1.075751525649697 
2021-07-29 20:05:26,237: ============================================================
2021-07-29 20:05:26,237: Epoch 31/65 Batch 700/964 eta: 13:03:14.924392	Training Loss2 2.0457 (1.9943)	Training Loss3 3.1150 (3.0520)	Training Loss4 2.6621 (2.6304)	Training Total_Loss 5.4689 (5.3644)	Training Prec@1_up 98.047 (98.646)	Training Prec@1_down 95.703 (96.625)	
2021-07-29 20:05:26,237: ============================================================
2021-07-29 20:07:48,036: time cost, forward:0.02351518626207106, backward:0.1789232141831342, data cost:1.0766698896362725 
2021-07-29 20:07:48,054: ============================================================
2021-07-29 20:07:48,054: Epoch 31/65 Batch 800/964 eta: 12:58:35.872834	Training Loss2 1.9983 (1.9965)	Training Loss3 2.9950 (3.0544)	Training Loss4 2.6272 (2.6331)	Training Total_Loss 5.3078 (5.3692)	Training Prec@1_up 98.438 (98.646)	Training Prec@1_down 97.266 (96.637)	
2021-07-29 20:07:48,054: ============================================================
2021-07-29 20:10:09,135: time cost, forward:0.023371200009898695, backward:0.17884787913822092, data cost:1.0765593449716706 
2021-07-29 20:10:09,135: ============================================================
2021-07-29 20:10:09,135: Epoch 31/65 Batch 900/964 eta: 12:52:12.592032	Training Loss2 1.9109 (1.9960)	Training Loss3 2.9871 (3.0541)	Training Loss4 2.5486 (2.6332)	Training Total_Loss 5.2169 (5.3687)	Training Prec@1_up 98.633 (98.650)	Training Prec@1_down 97.461 (96.648)	
2021-07-29 20:10:09,136: ============================================================
2021-07-29 20:11:42,294: Epoch: 31/65 eta: 12:50:40.889024	Training Loss2 1.8270 (1.9953)	Training Loss3 2.7728 (3.0527)	Training Loss4 2.4303 (2.6322)	Training Total_Loss 4.9014 (5.3664)	Training Prec@1_up 99.023 (98.653)	Training Prec@1_down 97.656 (96.650)	
2021-07-29 20:11:42,295: ============================================================
2021-07-29 20:11:42,296: Save Checkpoint...
2021-07-29 20:11:42,297: ============================================================
2021-07-29 20:11:42,998: Save done!
2021-07-29 20:11:42,998: ============================================================
2021-07-29 20:14:02,824: time cost, forward:0.023131772725269048, backward:0.17841331404869, data cost:1.0591576870041663 
2021-07-29 20:14:02,825: ============================================================
2021-07-29 20:14:02,825: Epoch 32/65 Batch 100/964 eta: 12:41:30.629367	Training Loss2 1.9765 (1.9987)	Training Loss3 2.8164 (3.0517)	Training Loss4 2.6447 (2.6332)	Training Total_Loss 5.1270 (5.3677)	Training Prec@1_up 98.633 (98.593)	Training Prec@1_down 96.875 (96.644)	
2021-07-29 20:14:02,825: ============================================================
2021-07-29 20:16:20,311: time cost, forward:0.022511998612677032, backward:0.17873931530132964, data cost:1.0483395298521723 
2021-07-29 20:16:20,312: ============================================================
2021-07-29 20:16:20,312: Epoch 32/65 Batch 200/964 eta: 12:26:29.134472	Training Loss2 2.0275 (1.9834)	Training Loss3 3.0716 (3.0357)	Training Loss4 2.6473 (2.6175)	Training Total_Loss 5.4090 (5.3361)	Training Prec@1_up 98.438 (98.666)	Training Prec@1_down 96.289 (96.709)	
2021-07-29 20:16:20,312: ============================================================
2021-07-29 20:18:37,704: time cost, forward:0.022319284171165032, backward:0.17848272467137977, data cost:1.0464775275227218 
2021-07-29 20:18:37,705: ============================================================
2021-07-29 20:18:37,705: Epoch 32/65 Batch 300/964 eta: 12:23:41.097705	Training Loss2 1.9683 (1.9889)	Training Loss3 3.0962 (3.0404)	Training Loss4 2.5329 (2.6252)	Training Total_Loss 5.3468 (5.3475)	Training Prec@1_up 98.828 (98.671)	Training Prec@1_down 96.289 (96.706)	
2021-07-29 20:18:37,705: ============================================================
2021-07-29 20:20:55,389: time cost, forward:0.022007019597486147, backward:0.1783169564746675, data cost:1.047177499398253 
2021-07-29 20:20:55,389: ============================================================
2021-07-29 20:20:55,389: Epoch 32/65 Batch 400/964 eta: 12:22:58.071113	Training Loss2 2.0613 (1.9832)	Training Loss3 3.0343 (3.0378)	Training Loss4 2.6955 (2.6191)	Training Total_Loss 5.4127 (5.3390)	Training Prec@1_up 97.266 (98.682)	Training Prec@1_down 96.289 (96.716)	
2021-07-29 20:20:55,389: ============================================================
2021-07-29 20:23:13,443: time cost, forward:0.021978298504510243, backward:0.1782728889901079, data cost:1.04678322174745 
2021-07-29 20:23:13,443: ============================================================
2021-07-29 20:23:13,443: Epoch 32/65 Batch 500/964 eta: 12:22:39.610747	Training Loss2 2.0634 (1.9847)	Training Loss3 3.1596 (3.0384)	Training Loss4 2.6927 (2.6199)	Training Total_Loss 5.5377 (5.3407)	Training Prec@1_up 98.242 (98.676)	Training Prec@1_down 96.484 (96.702)	
2021-07-29 20:23:13,443: ============================================================
2021-07-29 20:25:30,953: time cost, forward:0.02185859306029764, backward:0.1781170296549598, data cost:1.0451266029243278 
2021-07-29 20:25:30,954: ============================================================
2021-07-29 20:25:30,954: Epoch 32/65 Batch 600/964 eta: 12:17:26.828138	Training Loss2 1.9631 (1.9831)	Training Loss3 2.9807 (3.0365)	Training Loss4 2.6119 (2.6194)	Training Total_Loss 5.2682 (5.3378)	Training Prec@1_up 98.438 (98.678)	Training Prec@1_down 96.875 (96.696)	
2021-07-29 20:25:30,954: ============================================================
2021-07-29 20:27:50,388: time cost, forward:0.021843194620462615, backward:0.17820567532158715, data cost:1.0455235445789344 
2021-07-29 20:27:50,389: ============================================================
2021-07-29 20:27:50,389: Epoch 32/65 Batch 700/964 eta: 12:25:26.552925	Training Loss2 1.9353 (1.9846)	Training Loss3 2.9287 (3.0374)	Training Loss4 2.5936 (2.6207)	Training Total_Loss 5.1932 (5.3401)	Training Prec@1_up 98.438 (98.668)	Training Prec@1_down 96.289 (96.685)	
2021-07-29 20:27:50,389: ============================================================
2021-07-29 20:30:10,214: time cost, forward:0.02187718855722974, backward:0.17811708456285308, data cost:1.0484760493897973 
2021-07-29 20:30:10,241: ============================================================
2021-07-29 20:30:10,242: Epoch 32/65 Batch 800/964 eta: 12:25:20.711905	Training Loss2 2.1542 (1.9866)	Training Loss3 3.1896 (3.0395)	Training Loss4 2.8352 (2.6230)	Training Total_Loss 5.6843 (5.3443)	Training Prec@1_up 98.633 (98.667)	Training Prec@1_down 97.266 (96.689)	
2021-07-29 20:30:10,242: ============================================================
2021-07-29 20:32:28,122: time cost, forward:0.021900271414649633, backward:0.17800910931673145, data cost:1.0474943064476412 
2021-07-29 20:32:28,123: ============================================================
2021-07-29 20:32:28,123: Epoch 32/65 Batch 900/964 eta: 12:12:32.387488	Training Loss2 1.8235 (1.9895)	Training Loss3 2.9970 (3.0431)	Training Loss4 2.4583 (2.6257)	Training Total_Loss 5.1379 (5.3507)	Training Prec@1_up 98.242 (98.660)	Training Prec@1_down 97.070 (96.670)	
2021-07-29 20:32:28,123: ============================================================
2021-07-29 20:33:59,167: Epoch: 32/65 eta: 12:11:02.764713	Training Loss2 2.2669 (1.9888)	Training Loss3 3.2400 (3.0431)	Training Loss4 2.9621 (2.6246)	Training Total_Loss 5.8545 (5.3498)	Training Prec@1_up 98.242 (98.665)	Training Prec@1_down 96.875 (96.671)	
2021-07-29 20:33:59,167: ============================================================
2021-07-29 20:33:59,170: Save Checkpoint...
2021-07-29 20:33:59,172: ============================================================
2021-07-29 20:33:59,820: Save done!
2021-07-29 20:33:59,820: ============================================================
2021-07-29 20:36:19,599: time cost, forward:0.021290661108614217, backward:0.17821464634904957, data cost:1.0564537698572332 
2021-07-29 20:36:19,617: ============================================================
2021-07-29 20:36:19,618: Epoch 33/65 Batch 100/964 eta: 12:18:53.535498	Training Loss2 1.9250 (1.9718)	Training Loss3 3.0568 (3.0290)	Training Loss4 2.6192 (2.6074)	Training Total_Loss 5.3289 (5.3186)	Training Prec@1_up 99.414 (98.656)	Training Prec@1_down 96.875 (96.654)	
2021-07-29 20:36:19,618: ============================================================
2021-07-29 20:38:38,408: time cost, forward:0.021816560371437265, backward:0.17844722618409736, data cost:1.054547170897824 
2021-07-29 20:38:38,409: ============================================================
2021-07-29 20:38:38,409: Epoch 33/65 Batch 200/964 eta: 12:11:16.035652	Training Loss2 1.8598 (1.9640)	Training Loss3 2.9898 (3.0140)	Training Loss4 2.5417 (2.6044)	Training Total_Loss 5.1905 (5.2983)	Training Prec@1_up 99.219 (98.702)	Training Prec@1_down 97.266 (96.746)	
2021-07-29 20:38:38,409: ============================================================
2021-07-29 20:40:58,308: time cost, forward:0.02210984899846208, backward:0.17862208870342344, data cost:1.0573900543327714 
2021-07-29 20:40:58,309: ============================================================
2021-07-29 20:40:58,309: Epoch 33/65 Batch 300/964 eta: 12:14:46.666819	Training Loss2 2.0575 (1.9642)	Training Loss3 3.1198 (3.0126)	Training Loss4 2.7572 (2.6035)	Training Total_Loss 5.5271 (5.2965)	Training Prec@1_up 98.242 (98.720)	Training Prec@1_down 96.680 (96.779)	
2021-07-29 20:40:58,309: ============================================================
2021-07-29 20:43:16,757: time cost, forward:0.02311177839312637, backward:0.17898570565053992, data cost:1.0548028121317239 
2021-07-29 20:43:16,757: ============================================================
2021-07-29 20:43:16,757: Epoch 33/65 Batch 400/964 eta: 12:04:50.790356	Training Loss2 2.1246 (1.9699)	Training Loss3 3.2435 (3.0208)	Training Loss4 2.8400 (2.6070)	Training Total_Loss 5.7258 (5.3093)	Training Prec@1_up 98.828 (98.713)	Training Prec@1_down 96.289 (96.756)	
2021-07-29 20:43:16,757: ============================================================
2021-07-29 20:45:34,918: time cost, forward:0.022815801337630094, backward:0.17902070152496766, data cost:1.0524979397385774 
2021-07-29 20:45:34,918: ============================================================
2021-07-29 20:45:34,918: Epoch 33/65 Batch 500/964 eta: 12:01:02.416048	Training Loss2 1.9038 (1.9730)	Training Loss3 2.9562 (3.0249)	Training Loss4 2.6075 (2.6106)	Training Total_Loss 5.2118 (5.3167)	Training Prec@1_up 98.828 (98.702)	Training Prec@1_down 97.461 (96.727)	
2021-07-29 20:45:34,919: ============================================================
2021-07-29 20:47:52,765: time cost, forward:0.02263827037333646, backward:0.17916001660596947, data cost:1.05070828277002 
2021-07-29 20:47:52,765: ============================================================
2021-07-29 20:47:52,765: Epoch 33/65 Batch 600/964 eta: 11:57:06.115610	Training Loss2 1.9418 (1.9735)	Training Loss3 2.9213 (3.0249)	Training Loss4 2.5926 (2.6109)	Training Total_Loss 5.1885 (5.3171)	Training Prec@1_up 99.023 (98.702)	Training Prec@1_down 97.461 (96.721)	
2021-07-29 20:47:52,765: ============================================================
2021-07-29 20:50:09,337: time cost, forward:0.02246383433689887, backward:0.17900583808855267, data cost:1.0487919443837221 
2021-07-29 20:50:09,338: ============================================================
2021-07-29 20:50:09,338: Epoch 33/65 Batch 700/964 eta: 11:48:11.812381	Training Loss2 1.9108 (1.9766)	Training Loss3 3.0711 (3.0275)	Training Loss4 2.5593 (2.6142)	Training Total_Loss 5.3062 (5.3229)	Training Prec@1_up 98.633 (98.689)	Training Prec@1_down 95.508 (96.714)	
2021-07-29 20:50:09,338: ============================================================
2021-07-29 20:52:28,385: time cost, forward:0.02227898533263702, backward:0.1789371970299636, data cost:1.0483747262680188 
2021-07-29 20:52:28,385: ============================================================
2021-07-29 20:52:28,386: Epoch 33/65 Batch 800/964 eta: 11:58:42.922627	Training Loss2 1.9844 (1.9804)	Training Loss3 2.9882 (3.0315)	Training Loss4 2.5845 (2.6171)	Training Total_Loss 5.2726 (5.3302)	Training Prec@1_up 98.633 (98.688)	Training Prec@1_down 96.875 (96.725)	
2021-07-29 20:52:28,386: ============================================================
2021-07-29 20:54:48,078: time cost, forward:0.022261708941687733, backward:0.1789622627720817, data cost:1.0498904142284287 
2021-07-29 20:54:48,079: ============================================================
2021-07-29 20:54:48,079: Epoch 33/65 Batch 900/964 eta: 11:59:43.398897	Training Loss2 1.8154 (1.9818)	Training Loss3 2.8480 (3.0321)	Training Loss4 2.4156 (2.6177)	Training Total_Loss 4.9635 (5.3319)	Training Prec@1_up 99.609 (98.677)	Training Prec@1_down 98.047 (96.710)	
2021-07-29 20:54:48,079: ============================================================
2021-07-29 20:56:17,241: Epoch: 33/65 eta: 11:58:12.598233	Training Loss2 1.9772 (1.9829)	Training Loss3 3.0418 (3.0345)	Training Loss4 2.6625 (2.6188)	Training Total_Loss 5.3616 (5.3353)	Training Prec@1_up 98.438 (98.675)	Training Prec@1_down 96.875 (96.696)	
2021-07-29 20:56:17,241: ============================================================
2021-07-29 20:56:17,243: Save Checkpoint...
2021-07-29 20:56:17,244: ============================================================
2021-07-29 20:56:17,891: Save done!
2021-07-29 20:56:17,892: ============================================================
2021-07-29 20:58:37,130: time cost, forward:0.02257075213422679, backward:0.1781868477060337, data cost:1.0523353610375914 
2021-07-29 20:58:37,130: ============================================================
2021-07-29 20:58:37,131: Epoch 34/65 Batch 100/964 eta: 11:53:34.147951	Training Loss2 1.9060 (1.9572)	Training Loss3 3.0787 (3.0132)	Training Loss4 2.4936 (2.5837)	Training Total_Loss 5.2785 (5.2836)	Training Prec@1_up 99.023 (98.708)	Training Prec@1_down 96.680 (96.697)	
2021-07-29 20:58:37,131: ============================================================
2021-07-29 21:00:55,040: time cost, forward:0.022346921901607034, backward:0.1786534259067708, data cost:1.0473219749316498 
2021-07-29 21:00:55,041: ============================================================
2021-07-29 21:00:55,041: Epoch 34/65 Batch 200/964 eta: 11:44:28.071586	Training Loss2 1.8620 (1.9533)	Training Loss3 2.9753 (3.0012)	Training Loss4 2.5289 (2.5850)	Training Total_Loss 5.1707 (5.2703)	Training Prec@1_up 98.633 (98.729)	Training Prec@1_down 97.461 (96.816)	
2021-07-29 21:00:55,041: ============================================================
2021-07-29 21:03:11,619: time cost, forward:0.0220639522259052, backward:0.1787562155005924, data cost:1.041315636905938 
2021-07-29 21:03:11,620: ============================================================
2021-07-29 21:03:11,620: Epoch 34/65 Batch 300/964 eta: 11:35:23.563429	Training Loss2 1.9411 (1.9616)	Training Loss3 3.0710 (3.0109)	Training Loss4 2.5991 (2.5944)	Training Total_Loss 5.3410 (5.2890)	Training Prec@1_up 98.242 (98.709)	Training Prec@1_down 96.875 (96.766)	
2021-07-29 21:03:11,620: ============================================================
2021-07-29 21:05:27,793: time cost, forward:0.02207426499005846, backward:0.1788131742549122, data cost:1.0375169548474457 
2021-07-29 21:05:27,794: ============================================================
2021-07-29 21:05:27,794: Epoch 34/65 Batch 400/964 eta: 11:31:03.571467	Training Loss2 1.8610 (1.9685)	Training Loss3 2.9173 (3.0199)	Training Loss4 2.5175 (2.6025)	Training Total_Loss 5.1066 (5.3054)	Training Prec@1_up 99.023 (98.697)	Training Prec@1_down 97.852 (96.719)	
2021-07-29 21:05:27,794: ============================================================
2021-07-29 21:07:44,661: time cost, forward:0.021965997252531184, backward:0.17855879730117583, data cost:1.0390297109951714 
2021-07-29 21:07:44,661: ============================================================
2021-07-29 21:07:44,661: Epoch 34/65 Batch 500/964 eta: 11:32:17.933152	Training Loss2 1.9646 (1.9711)	Training Loss3 2.9304 (3.0231)	Training Loss4 2.5329 (2.6048)	Training Total_Loss 5.1791 (5.3111)	Training Prec@1_up 99.023 (98.693)	Training Prec@1_down 98.047 (96.733)	
2021-07-29 21:07:44,661: ============================================================
2021-07-29 21:10:01,986: time cost, forward:0.02216178029526852, backward:0.17837446321032083, data cost:1.0377510310413443 
2021-07-29 21:10:01,987: ============================================================
2021-07-29 21:10:01,987: Epoch 34/65 Batch 600/964 eta: 11:32:19.596556	Training Loss2 2.1742 (1.9727)	Training Loss3 3.2501 (3.0247)	Training Loss4 2.8651 (2.6065)	Training Total_Loss 5.7697 (5.3143)	Training Prec@1_up 98.828 (98.699)	Training Prec@1_down 96.094 (96.723)	
2021-07-29 21:10:01,987: ============================================================
2021-07-29 21:12:17,675: time cost, forward:0.02212487781508286, backward:0.17826848207454654, data cost:1.0370688015469836 
2021-07-29 21:12:17,676: ============================================================
2021-07-29 21:12:17,676: Epoch 34/65 Batch 700/964 eta: 11:21:48.860045	Training Loss2 2.0528 (1.9749)	Training Loss3 3.0364 (3.0245)	Training Loss4 2.7301 (2.6096)	Training Total_Loss 5.4278 (5.3168)	Training Prec@1_up 97.852 (98.695)	Training Prec@1_down 96.484 (96.725)	
2021-07-29 21:12:17,676: ============================================================
2021-07-29 21:14:35,134: time cost, forward:0.02209878833183508, backward:0.17822289257980556, data cost:1.036977435829344 
2021-07-29 21:14:35,135: ============================================================
2021-07-29 21:14:35,135: Epoch 34/65 Batch 800/964 eta: 11:28:25.093773	Training Loss2 1.8448 (1.9763)	Training Loss3 2.9397 (3.0255)	Training Loss4 2.4446 (2.6109)	Training Total_Loss 5.0844 (5.3191)	Training Prec@1_up 98.633 (98.684)	Training Prec@1_down 97.070 (96.712)	
2021-07-29 21:14:35,135: ============================================================
2021-07-29 21:16:52,741: time cost, forward:0.022087234013337845, backward:0.1781363818748907, data cost:1.0378001664451284 
2021-07-29 21:16:52,741: ============================================================
2021-07-29 21:16:52,741: Epoch 34/65 Batch 900/964 eta: 11:26:51.700468	Training Loss2 2.1174 (1.9776)	Training Loss3 3.2935 (3.0260)	Training Loss4 2.7019 (2.6130)	Training Total_Loss 5.7032 (5.3213)	Training Prec@1_up 98.438 (98.686)	Training Prec@1_down 96.094 (96.711)	
2021-07-29 21:16:52,741: ============================================================
2021-07-29 21:18:23,231: Epoch: 34/65 eta: 11:25:22.256395	Training Loss2 1.8549 (1.9775)	Training Loss3 2.8652 (3.0261)	Training Loss4 2.4760 (2.6129)	Training Total_Loss 5.0307 (5.3213)	Training Prec@1_up 98.828 (98.682)	Training Prec@1_down 96.875 (96.711)	
2021-07-29 21:18:23,232: ============================================================
2021-07-29 21:18:23,234: Save Checkpoint...
2021-07-29 21:18:23,234: ============================================================
2021-07-29 21:18:23,859: Save done!
2021-07-29 21:18:23,859: ============================================================
2021-07-29 21:20:40,976: time cost, forward:0.02379081225154376, backward:0.17746100040397259, data cost:1.043599429756704 
2021-07-29 21:20:40,976: ============================================================
2021-07-29 21:20:40,976: Epoch 35/65 Batch 100/964 eta: 11:20:39.914585	Training Loss2 2.1496 (1.9592)	Training Loss3 3.3066 (3.0008)	Training Loss4 2.7926 (2.5892)	Training Total_Loss 5.7776 (5.2750)	Training Prec@1_up 98.047 (98.690)	Training Prec@1_down 95.312 (96.731)	
2021-07-29 21:20:40,976: ============================================================
2021-07-29 21:22:58,644: time cost, forward:0.022652461900183902, backward:0.17774050439422454, data cost:1.0443466368632102 
2021-07-29 21:22:58,645: ============================================================
2021-07-29 21:22:58,645: Epoch 35/65 Batch 200/964 eta: 11:21:06.904755	Training Loss2 1.8797 (1.9611)	Training Loss3 2.8226 (3.0104)	Training Loss4 2.5747 (2.5948)	Training Total_Loss 5.0498 (5.2884)	Training Prec@1_up 98.828 (98.680)	Training Prec@1_down 97.461 (96.704)	
2021-07-29 21:22:58,645: ============================================================
2021-07-29 21:25:15,349: time cost, forward:0.022499208067571837, backward:0.17783197112705396, data cost:1.038908133139977 
2021-07-29 21:25:15,350: ============================================================
2021-07-29 21:25:15,350: Epoch 35/65 Batch 300/964 eta: 11:14:04.260352	Training Loss2 2.1590 (1.9665)	Training Loss3 3.2002 (3.0164)	Training Loss4 2.8085 (2.6022)	Training Total_Loss 5.6839 (5.3008)	Training Prec@1_up 97.656 (98.675)	Training Prec@1_down 95.898 (96.695)	
2021-07-29 21:25:15,350: ============================================================
2021-07-29 21:27:32,404: time cost, forward:0.022367824587905615, backward:0.17785159746805826, data cost:1.0373481521032806 
2021-07-29 21:27:32,404: ============================================================
2021-07-29 21:27:32,404: Epoch 35/65 Batch 400/964 eta: 11:13:30.470006	Training Loss2 1.9829 (1.9649)	Training Loss3 3.0536 (3.0118)	Training Loss4 2.5941 (2.6000)	Training Total_Loss 5.3420 (5.2943)	Training Prec@1_up 98.633 (98.692)	Training Prec@1_down 96.484 (96.734)	
2021-07-29 21:27:32,405: ============================================================
2021-07-29 21:29:49,761: time cost, forward:0.022321931345907146, backward:0.17799881560530117, data cost:1.037500665756409 
2021-07-29 21:29:49,762: ============================================================
2021-07-29 21:29:49,762: Epoch 35/65 Batch 500/964 eta: 11:12:42.441076	Training Loss2 2.1842 (1.9645)	Training Loss3 3.1392 (3.0111)	Training Loss4 2.8206 (2.5995)	Training Total_Loss 5.6416 (5.2931)	Training Prec@1_up 98.438 (98.694)	Training Prec@1_down 96.289 (96.723)	
2021-07-29 21:29:49,762: ============================================================
2021-07-29 21:32:08,646: time cost, forward:0.02235873075876889, backward:0.17803543636914287, data cost:1.0398650897763209 
2021-07-29 21:32:08,646: ============================================================
2021-07-29 21:32:08,647: Epoch 35/65 Batch 600/964 eta: 11:17:52.452488	Training Loss2 2.0348 (1.9651)	Training Loss3 3.0845 (3.0107)	Training Loss4 2.7281 (2.6012)	Training Total_Loss 5.4660 (5.2939)	Training Prec@1_up 99.219 (98.694)	Training Prec@1_down 97.070 (96.735)	
2021-07-29 21:32:08,647: ============================================================
2021-07-29 21:34:26,917: time cost, forward:0.022341559373258008, backward:0.17800279509526637, data cost:1.0397755611949042 
2021-07-29 21:34:26,918: ============================================================
2021-07-29 21:34:26,918: Epoch 35/65 Batch 700/964 eta: 11:12:34.406353	Training Loss2 1.7939 (1.9651)	Training Loss3 2.8448 (3.0113)	Training Loss4 2.4161 (2.6008)	Training Total_Loss 4.9498 (5.2942)	Training Prec@1_up 99.414 (98.707)	Training Prec@1_down 98.242 (96.754)	
2021-07-29 21:34:26,918: ============================================================
2021-07-29 21:36:43,955: time cost, forward:0.02240914844899661, backward:0.1780522758880157, data cost:1.038642836750971 
2021-07-29 21:36:43,956: ============================================================
2021-07-29 21:36:43,956: Epoch 35/65 Batch 800/964 eta: 11:04:17.602117	Training Loss2 1.8601 (1.9680)	Training Loss3 2.8641 (3.0134)	Training Loss4 2.4423 (2.6029)	Training Total_Loss 5.0153 (5.2989)	Training Prec@1_up 98.828 (98.699)	Training Prec@1_down 96.680 (96.740)	
2021-07-29 21:36:43,956: ============================================================
2021-07-29 21:39:01,098: time cost, forward:0.022435402578453598, backward:0.1780844811470278, data cost:1.0382331855039841 
2021-07-29 21:39:01,099: ============================================================
2021-07-29 21:39:01,099: Epoch 35/65 Batch 900/964 eta: 11:02:30.899554	Training Loss2 1.9172 (1.9725)	Training Loss3 3.0076 (3.0194)	Training Loss4 2.5670 (2.6075)	Training Total_Loss 5.2496 (5.3094)	Training Prec@1_up 99.219 (98.686)	Training Prec@1_down 96.875 (96.719)	
2021-07-29 21:39:01,099: ============================================================
2021-07-29 21:40:30,725: Epoch: 35/65 eta: 11:01:01.756602	Training Loss2 1.8859 (1.9720)	Training Loss3 3.0199 (3.0178)	Training Loss4 2.4881 (2.6071)	Training Total_Loss 5.2069 (5.3074)	Training Prec@1_up 98.828 (98.692)	Training Prec@1_down 97.070 (96.729)	
2021-07-29 21:40:30,725: ============================================================
2021-07-29 21:40:30,727: Save Checkpoint...
2021-07-29 21:40:30,728: ============================================================
2021-07-29 21:40:31,383: Save done!
2021-07-29 21:40:31,383: ============================================================
2021-07-29 21:42:54,223: time cost, forward:0.02389675679832998, backward:0.1775117980109321, data cost:1.0868867214279945 
2021-07-29 21:42:54,224: ============================================================
2021-07-29 21:42:54,224: Epoch 36/65 Batch 100/964 eta: 11:26:07.654504	Training Loss2 1.8607 (1.9451)	Training Loss3 2.8314 (2.9807)	Training Loss4 2.4430 (2.5860)	Training Total_Loss 4.9832 (5.2463)	Training Prec@1_up 98.828 (98.739)	Training Prec@1_down 97.070 (96.804)	
2021-07-29 21:42:54,224: ============================================================
2021-07-29 21:45:16,786: time cost, forward:0.023822605909414627, backward:0.17854107923843154, data cost:1.0880586861366004 
2021-07-29 21:45:16,786: ============================================================
2021-07-29 21:45:16,786: Epoch 36/65 Batch 200/964 eta: 11:22:25.259147	Training Loss2 2.0734 (1.9575)	Training Loss3 3.0881 (2.9947)	Training Loss4 2.7296 (2.5918)	Training Total_Loss 5.4897 (5.2693)	Training Prec@1_up 97.656 (98.715)	Training Prec@1_down 97.266 (96.800)	
2021-07-29 21:45:16,786: ============================================================
2021-07-29 21:47:37,711: time cost, forward:0.02344602167008314, backward:0.17841600255423964, data cost:1.083359841120283 
2021-07-29 21:47:37,711: ============================================================
2021-07-29 21:47:37,711: Epoch 36/65 Batch 300/964 eta: 11:12:14.157884	Training Loss2 1.8266 (1.9602)	Training Loss3 2.9755 (3.0038)	Training Loss4 2.4940 (2.5948)	Training Total_Loss 5.1357 (5.2813)	Training Prec@1_up 98.828 (98.708)	Training Prec@1_down 96.875 (96.775)	
2021-07-29 21:47:37,711: ============================================================
2021-07-29 21:49:58,493: time cost, forward:0.02341872289365993, backward:0.17859484856588798, data cost:1.0799133867249453 
2021-07-29 21:49:58,493: ============================================================
2021-07-29 21:49:58,493: Epoch 36/65 Batch 400/964 eta: 11:09:12.409487	Training Loss2 1.8800 (1.9567)	Training Loss3 2.8466 (2.9986)	Training Loss4 2.5311 (2.5904)	Training Total_Loss 5.0522 (5.2721)	Training Prec@1_up 99.023 (98.719)	Training Prec@1_down 97.656 (96.789)	
2021-07-29 21:49:58,493: ============================================================
2021-07-29 21:52:18,712: time cost, forward:0.023563733320675774, backward:0.17861734936853688, data cost:1.0773826037236827 
2021-07-29 21:52:18,713: ============================================================
2021-07-29 21:52:18,713: Epoch 36/65 Batch 500/964 eta: 11:04:11.775711	Training Loss2 2.0758 (1.9574)	Training Loss3 3.1144 (3.0010)	Training Loss4 2.7724 (2.5918)	Training Total_Loss 5.5385 (5.2756)	Training Prec@1_up 99.023 (98.724)	Training Prec@1_down 97.070 (96.776)	
2021-07-29 21:52:18,713: ============================================================
2021-07-29 21:54:38,562: time cost, forward:0.02348739356548838, backward:0.17850037926624535, data cost:1.0747210649098697 
2021-07-29 21:54:38,563: ============================================================
2021-07-29 21:54:38,563: Epoch 36/65 Batch 600/964 eta: 11:00:06.952707	Training Loss2 2.1604 (1.9612)	Training Loss3 3.3288 (3.0050)	Training Loss4 2.7977 (2.5956)	Training Total_Loss 5.8079 (5.2835)	Training Prec@1_up 98.047 (98.716)	Training Prec@1_down 95.508 (96.761)	
2021-07-29 21:54:38,563: ============================================================
2021-07-29 21:56:57,602: time cost, forward:0.023309242060256107, backward:0.17847992831545326, data cost:1.0718028903519135 
2021-07-29 21:56:57,603: ============================================================
2021-07-29 21:56:57,603: Epoch 36/65 Batch 700/964 eta: 10:53:58.505774	Training Loss2 2.0269 (1.9642)	Training Loss3 3.1300 (3.0068)	Training Loss4 2.6760 (2.5995)	Training Total_Loss 5.4815 (5.2887)	Training Prec@1_up 98.828 (98.709)	Training Prec@1_down 96.875 (96.765)	
2021-07-29 21:56:57,603: ============================================================
2021-07-29 21:59:31,314: time cost, forward:0.023294300251221926, backward:0.17854823576792309, data cost:1.0881194832626362 
2021-07-29 21:59:31,314: ============================================================
2021-07-29 21:59:31,315: Epoch 36/65 Batch 800/964 eta: 12:00:25.255816	Training Loss2 1.9809 (1.9651)	Training Loss3 3.0090 (3.0079)	Training Loss4 2.6049 (2.6009)	Training Total_Loss 5.3019 (5.2909)	Training Prec@1_up 98.633 (98.703)	Training Prec@1_down 96.484 (96.762)	
2021-07-29 21:59:31,315: ============================================================
2021-07-29 22:01:47,049: time cost, forward:0.023362892488218653, backward:0.1786567562811897, data cost:1.0804032637624772 
2021-07-29 22:01:47,050: ============================================================
2021-07-29 22:01:47,050: Epoch 36/65 Batch 900/964 eta: 10:33:54.482028	Training Loss2 2.0091 (1.9666)	Training Loss3 3.1425 (3.0090)	Training Loss4 2.6004 (2.6017)	Training Total_Loss 5.4473 (5.2932)	Training Prec@1_up 98.438 (98.698)	Training Prec@1_down 95.508 (96.756)	
2021-07-29 22:01:47,051: ============================================================
2021-07-29 22:03:15,832: Epoch: 36/65 eta: 10:32:26.253866	Training Loss2 2.1701 (1.9669)	Training Loss3 3.2753 (3.0099)	Training Loss4 2.7733 (2.6024)	Training Total_Loss 5.7470 (5.2945)	Training Prec@1_up 97.656 (98.698)	Training Prec@1_down 95.508 (96.751)	
2021-07-29 22:03:15,833: ============================================================
2021-07-29 22:03:15,835: Save Checkpoint...
2021-07-29 22:03:15,836: ============================================================
2021-07-29 22:03:16,624: Save done!
2021-07-29 22:03:16,624: ============================================================
2021-07-29 22:05:32,097: time cost, forward:0.02384167006521514, backward:0.17899491811039472, data cost:1.0322618002843376 
2021-07-29 22:05:32,097: ============================================================
2021-07-29 22:05:32,097: Epoch 37/65 Batch 100/964 eta: 10:28:58.361418	Training Loss2 2.0913 (1.9528)	Training Loss3 3.1371 (2.9885)	Training Loss4 2.6942 (2.5948)	Training Total_Loss 5.5298 (5.2623)	Training Prec@1_up 98.242 (98.694)	Training Prec@1_down 96.094 (96.893)	
2021-07-29 22:05:32,098: ============================================================
2021-07-29 22:07:46,315: time cost, forward:0.023939229735177966, backward:0.1790499483520661, data cost:1.0200090468229361 
2021-07-29 22:07:46,315: ============================================================
2021-07-29 22:07:46,315: Epoch 37/65 Batch 200/964 eta: 10:20:54.842462	Training Loss2 2.1250 (1.9572)	Training Loss3 3.1317 (2.9959)	Training Loss4 2.7155 (2.5938)	Training Total_Loss 5.5520 (5.2714)	Training Prec@1_up 98.633 (98.673)	Training Prec@1_down 96.484 (96.766)	
2021-07-29 22:07:46,315: ============================================================
2021-07-29 22:10:00,179: time cost, forward:0.024029527619530925, backward:0.1794511147566065, data cost:1.0143040087709458 
2021-07-29 22:10:00,180: ============================================================
2021-07-29 22:10:00,180: Epoch 37/65 Batch 300/964 eta: 10:17:02.879259	Training Loss2 1.9208 (1.9522)	Training Loss3 2.9810 (2.9896)	Training Loss4 2.5198 (2.5898)	Training Total_Loss 5.2013 (5.2606)	Training Prec@1_up 98.633 (98.706)	Training Prec@1_down 97.070 (96.827)	
2021-07-29 22:10:00,180: ============================================================
2021-07-29 22:12:12,658: time cost, forward:0.024033247677604657, backward:0.17943165953595536, data cost:1.0081993266753386 
2021-07-29 22:12:12,658: ============================================================
2021-07-29 22:12:12,659: Epoch 37/65 Batch 400/964 eta: 10:08:27.221433	Training Loss2 2.0251 (1.9516)	Training Loss3 3.1076 (2.9866)	Training Loss4 2.6720 (2.5876)	Training Total_Loss 5.4561 (5.2562)	Training Prec@1_up 99.414 (98.707)	Training Prec@1_down 96.680 (96.815)	
2021-07-29 22:12:12,659: ============================================================
2021-07-29 22:14:25,211: time cost, forward:0.023888411646137735, backward:0.17930835903527026, data cost:1.0048308291272792 
2021-07-29 22:14:25,212: ============================================================
2021-07-29 22:14:25,212: Epoch 37/65 Batch 500/964 eta: 10:06:35.108690	Training Loss2 1.9455 (1.9544)	Training Loss3 3.1223 (2.9915)	Training Loss4 2.5792 (2.5896)	Training Total_Loss 5.3846 (5.2635)	Training Prec@1_up 99.219 (98.695)	Training Prec@1_down 97.070 (96.794)	
2021-07-29 22:14:25,212: ============================================================
2021-07-29 22:16:37,595: time cost, forward:0.023643714955732698, backward:0.17925927077788542, data cost:1.0021999777855977 
2021-07-29 22:16:37,596: ============================================================
2021-07-29 22:16:37,596: Epoch 37/65 Batch 600/964 eta: 10:03:36.315030	Training Loss2 2.1591 (1.9577)	Training Loss3 3.2656 (2.9953)	Training Loss4 2.8070 (2.5926)	Training Total_Loss 5.7487 (5.2705)	Training Prec@1_up 99.023 (98.690)	Training Prec@1_down 97.070 (96.780)	
2021-07-29 22:16:37,596: ============================================================
2021-07-29 22:18:51,362: time cost, forward:0.023568812358021225, backward:0.1791991570135725, data cost:1.0023852823800454 
2021-07-29 22:18:51,363: ============================================================
2021-07-29 22:18:51,363: Epoch 37/65 Batch 700/964 eta: 10:07:40.849669	Training Loss2 2.0755 (1.9574)	Training Loss3 3.1592 (2.9956)	Training Loss4 2.7324 (2.5931)	Training Total_Loss 5.5631 (5.2709)	Training Prec@1_up 97.852 (98.701)	Training Prec@1_down 95.703 (96.793)	
2021-07-29 22:18:51,363: ============================================================
2021-07-29 22:21:03,497: time cost, forward:0.023424515586919867, backward:0.17912510518586083, data cost:1.0005035125866104 
2021-07-29 22:21:03,497: ============================================================
2021-07-29 22:21:03,498: Epoch 37/65 Batch 800/964 eta: 9:58:03.836358	Training Loss2 2.0377 (1.9602)	Training Loss3 3.0145 (3.0002)	Training Loss4 2.6570 (2.5961)	Training Total_Loss 5.3619 (5.2783)	Training Prec@1_up 99.023 (98.705)	Training Prec@1_down 96.289 (96.790)	
2021-07-29 22:21:03,498: ============================================================
2021-07-29 22:23:17,570: time cost, forward:0.023323704323858784, backward:0.1791128889472121, data cost:1.0011790570480805 
2021-07-29 22:23:17,571: ============================================================
2021-07-29 22:23:17,571: Epoch 37/65 Batch 900/964 eta: 10:04:36.230425	Training Loss2 2.0161 (1.9607)	Training Loss3 3.1144 (3.0004)	Training Loss4 2.6962 (2.5965)	Training Total_Loss 5.4706 (5.2790)	Training Prec@1_up 98.828 (98.706)	Training Prec@1_down 96.094 (96.776)	
2021-07-29 22:23:17,571: ============================================================
2021-07-29 22:24:44,765: Epoch: 37/65 eta: 10:03:09.082738	Training Loss2 2.0307 (1.9617)	Training Loss3 3.0396 (3.0020)	Training Loss4 2.6859 (2.5978)	Training Total_Loss 5.3979 (5.2817)	Training Prec@1_up 98.242 (98.705)	Training Prec@1_down 96.094 (96.769)	
2021-07-29 22:24:44,765: ============================================================
2021-07-29 22:24:44,768: Save Checkpoint...
2021-07-29 22:24:44,770: ============================================================
2021-07-29 22:24:45,468: Save done!
2021-07-29 22:24:45,469: ============================================================
2021-07-29 22:27:00,644: time cost, forward:0.024177462163597646, backward:0.179380436136265, data cost:1.0268843173980713 
2021-07-29 22:27:00,645: ============================================================
2021-07-29 22:27:00,645: Epoch 38/65 Batch 100/964 eta: 10:05:52.472255	Training Loss2 1.9174 (1.9454)	Training Loss3 2.8827 (2.9900)	Training Loss4 2.5285 (2.5778)	Training Total_Loss 5.1057 (5.2515)	Training Prec@1_up 98.633 (98.649)	Training Prec@1_down 97.266 (96.774)	
2021-07-29 22:27:00,645: ============================================================
2021-07-29 22:29:14,579: time cost, forward:0.02421412156454882, backward:0.17908719915840493, data cost:1.0156798674233596 
2021-07-29 22:29:14,580: ============================================================
2021-07-29 22:29:14,580: Epoch 38/65 Batch 200/964 eta: 9:58:05.300098	Training Loss2 1.8990 (1.9426)	Training Loss3 2.8624 (2.9830)	Training Loss4 2.6027 (2.5781)	Training Total_Loss 5.1132 (5.2433)	Training Prec@1_up 99.023 (98.700)	Training Prec@1_down 96.680 (96.808)	
2021-07-29 22:29:14,580: ============================================================
2021-07-29 22:31:28,841: time cost, forward:0.02393206066909841, backward:0.17890647581987157, data cost:1.0131635027984314 
2021-07-29 22:31:28,841: ============================================================
2021-07-29 22:31:28,858: Epoch 38/65 Batch 300/964 eta: 9:57:22.701071	Training Loss2 1.9909 (1.9472)	Training Loss3 3.0774 (2.9866)	Training Loss4 2.6194 (2.5817)	Training Total_Loss 5.3826 (5.2510)	Training Prec@1_up 99.414 (98.687)	Training Prec@1_down 97.656 (96.791)	
2021-07-29 22:31:28,858: ============================================================
2021-07-29 22:33:43,251: time cost, forward:0.024004526305616947, backward:0.1790059556937158, data cost:1.0121037189225506 
2021-07-29 22:33:43,252: ============================================================
2021-07-29 22:33:43,252: Epoch 38/65 Batch 400/964 eta: 9:55:39.445888	Training Loss2 1.9783 (1.9488)	Training Loss3 2.9546 (2.9903)	Training Loss4 2.5713 (2.5815)	Training Total_Loss 5.2293 (5.2555)	Training Prec@1_up 99.023 (98.697)	Training Prec@1_down 97.461 (96.774)	
2021-07-29 22:33:43,252: ============================================================
2021-07-29 22:35:56,254: time cost, forward:0.023970071203961878, backward:0.17916520134002747, data cost:1.0085015167931994 
2021-07-29 22:35:56,254: ============================================================
2021-07-29 22:35:56,254: Epoch 38/65 Batch 500/964 eta: 9:47:16.384554	Training Loss2 2.0706 (1.9517)	Training Loss3 3.2491 (2.9923)	Training Loss4 2.6829 (2.5856)	Training Total_Loss 5.6258 (5.2609)	Training Prec@1_up 99.023 (98.695)	Training Prec@1_down 95.703 (96.771)	
2021-07-29 22:35:56,255: ============================================================
2021-07-29 22:38:09,794: time cost, forward:0.02414132358633815, backward:0.17926764607628518, data cost:1.0070385101044517 
2021-07-29 22:38:09,794: ============================================================
2021-07-29 22:38:09,794: Epoch 38/65 Batch 600/964 eta: 9:47:25.144545	Training Loss2 1.6773 (1.9551)	Training Loss3 2.6853 (2.9943)	Training Loss4 2.2567 (2.5891)	Training Total_Loss 4.6523 (5.2664)	Training Prec@1_up 99.219 (98.706)	Training Prec@1_down 98.242 (96.783)	
2021-07-29 22:38:09,794: ============================================================
2021-07-29 22:40:22,706: time cost, forward:0.024064683436665242, backward:0.17923585880808907, data cost:1.005199012838208 
2021-07-29 22:40:22,706: ============================================================
2021-07-29 22:40:22,706: Epoch 38/65 Batch 700/964 eta: 9:42:26.589318	Training Loss2 1.9168 (1.9539)	Training Loss3 2.9125 (2.9912)	Training Loss4 2.5661 (2.5880)	Training Total_Loss 5.1539 (5.2622)	Training Prec@1_up 99.219 (98.710)	Training Prec@1_down 97.656 (96.792)	
2021-07-29 22:40:22,706: ============================================================
2021-07-29 22:42:35,364: time cost, forward:0.023908166921183523, backward:0.17916676637079001, data cost:1.003556184983522 
2021-07-29 22:42:35,364: ============================================================
2021-07-29 22:42:35,365: Epoch 38/65 Batch 800/964 eta: 9:39:07.167454	Training Loss2 2.0020 (1.9572)	Training Loss3 3.1294 (2.9954)	Training Loss4 2.6132 (2.5919)	Training Total_Loss 5.4370 (5.2700)	Training Prec@1_up 98.242 (98.709)	Training Prec@1_down 95.703 (96.788)	
2021-07-29 22:42:35,365: ============================================================
2021-07-29 22:44:48,525: time cost, forward:0.023686252791306067, backward:0.17914627709563768, data cost:1.0026926424134692 
2021-07-29 22:44:48,525: ============================================================
2021-07-29 22:44:48,525: Epoch 38/65 Batch 900/964 eta: 9:39:05.668379	Training Loss2 1.8144 (1.9573)	Training Loss3 2.8419 (2.9954)	Training Loss4 2.4482 (2.5920)	Training Total_Loss 4.9732 (5.2700)	Training Prec@1_up 99.023 (98.706)	Training Prec@1_down 97.461 (96.780)	
2021-07-29 22:44:48,526: ============================================================
2021-07-29 22:46:16,825: Epoch: 38/65 eta: 9:37:39.113807	Training Loss2 1.9410 (1.9565)	Training Loss3 3.0748 (2.9939)	Training Loss4 2.5226 (2.5918)	Training Total_Loss 5.3066 (5.2681)	Training Prec@1_up 98.828 (98.713)	Training Prec@1_down 96.680 (96.787)	
2021-07-29 22:46:16,826: ============================================================
2021-07-29 22:46:16,828: Save Checkpoint...
2021-07-29 22:46:16,829: ============================================================
2021-07-29 22:46:17,586: Save done!
2021-07-29 22:46:17,587: ============================================================
2021-07-29 22:48:33,445: time cost, forward:0.024749043011906172, backward:0.17939266050704802, data cost:1.0352264149020416 
2021-07-29 22:48:33,445: ============================================================
2021-07-29 22:48:33,445: Epoch 39/65 Batch 100/964 eta: 9:47:05.922269	Training Loss2 1.8802 (1.9413)	Training Loss3 2.9282 (2.9705)	Training Loss4 2.4792 (2.5776)	Training Total_Loss 5.1078 (5.2300)	Training Prec@1_up 98.633 (98.728)	Training Prec@1_down 97.070 (96.869)	
2021-07-29 22:48:33,445: ============================================================
2021-07-29 22:50:47,430: time cost, forward:0.02536315294965428, backward:0.17974436702440733, data cost:1.0193251628971578 
2021-07-29 22:50:47,430: ============================================================
2021-07-29 22:50:47,431: Epoch 39/65 Batch 200/964 eta: 9:36:47.079657	Training Loss2 2.0739 (1.9334)	Training Loss3 3.2572 (2.9674)	Training Loss4 2.6524 (2.5688)	Training Total_Loss 5.6203 (5.2185)	Training Prec@1_up 98.047 (98.753)	Training Prec@1_down 96.094 (96.902)	
2021-07-29 22:50:47,431: ============================================================
2021-07-29 22:53:02,586: time cost, forward:0.025296365935666903, backward:0.17951505319729297, data cost:1.0185616789852896 
2021-07-29 22:53:02,586: ============================================================
2021-07-29 22:53:02,586: Epoch 39/65 Batch 300/964 eta: 9:39:34.256297	Training Loss2 1.7912 (1.9338)	Training Loss3 2.8357 (2.9652)	Training Loss4 2.4490 (2.5695)	Training Total_Loss 4.9558 (5.2169)	Training Prec@1_up 98.633 (98.776)	Training Prec@1_down 96.484 (96.905)	
2021-07-29 22:53:02,587: ============================================================
2021-07-29 22:55:16,788: time cost, forward:0.02522229610529161, backward:0.17941420299367497, data cost:1.0156754383765965 
2021-07-29 22:55:16,788: ============================================================
2021-07-29 22:55:16,788: Epoch 39/65 Batch 400/964 eta: 9:33:14.632388	Training Loss2 2.0362 (1.9390)	Training Loss3 3.1266 (2.9704)	Training Loss4 2.6772 (2.5753)	Training Total_Loss 5.4833 (5.2276)	Training Prec@1_up 98.633 (98.763)	Training Prec@1_down 96.094 (96.873)	
2021-07-29 22:55:16,789: ============================================================
2021-07-29 22:57:31,886: time cost, forward:0.02550400951821245, backward:0.17955086799805053, data cost:1.0152279719083248 
2021-07-29 22:57:31,886: ============================================================
2021-07-29 22:57:31,886: Epoch 39/65 Batch 500/964 eta: 9:34:49.063743	Training Loss2 2.0445 (1.9431)	Training Loss3 3.0093 (2.9728)	Training Loss4 2.7379 (2.5798)	Training Total_Loss 5.4005 (5.2342)	Training Prec@1_up 98.633 (98.750)	Training Prec@1_down 97.266 (96.867)	
2021-07-29 22:57:31,886: ============================================================
2021-07-29 22:59:46,142: time cost, forward:0.02506238908719937, backward:0.1794513418200816, data cost:1.0136446323936092 
2021-07-29 22:59:46,142: ============================================================
2021-07-29 22:59:46,143: Epoch 39/65 Batch 600/964 eta: 9:29:00.083386	Training Loss2 1.7296 (1.9459)	Training Loss3 2.6045 (2.9768)	Training Loss4 2.2890 (2.5818)	Training Total_Loss 4.6138 (5.2406)	Training Prec@1_up 99.023 (98.742)	Training Prec@1_down 97.461 (96.845)	
2021-07-29 22:59:46,143: ============================================================
2021-07-29 23:02:00,279: time cost, forward:0.02451534912480476, backward:0.17936129153883337, data cost:1.0123321836086814 
2021-07-29 23:02:00,279: ============================================================
2021-07-29 23:02:00,279: Epoch 39/65 Batch 700/964 eta: 9:26:15.522827	Training Loss2 1.9708 (1.9475)	Training Loss3 2.8355 (2.9799)	Training Loss4 2.6028 (2.5833)	Training Total_Loss 5.1223 (5.2453)	Training Prec@1_up 99.219 (98.740)	Training Prec@1_down 97.656 (96.840)	
2021-07-29 23:02:00,280: ============================================================
2021-07-29 23:04:13,760: time cost, forward:0.02415979729128421, backward:0.17935384528359424, data cost:1.0104743873371798 
2021-07-29 23:04:13,760: ============================================================
2021-07-29 23:04:13,760: Epoch 39/65 Batch 800/964 eta: 9:21:15.841777	Training Loss2 1.8688 (1.9491)	Training Loss3 2.8571 (2.9831)	Training Loss4 2.3921 (2.5840)	Training Total_Loss 4.9876 (5.2496)	Training Prec@1_up 98.047 (98.728)	Training Prec@1_down 96.875 (96.821)	
2021-07-29 23:04:13,760: ============================================================
2021-07-29 23:06:27,970: time cost, forward:0.02387092243444933, backward:0.17932146595370385, data cost:1.0098884031425197 
2021-07-29 23:06:27,987: ============================================================
2021-07-29 23:06:27,988: Epoch 39/65 Batch 900/964 eta: 9:22:10.008035	Training Loss2 2.0599 (1.9504)	Training Loss3 3.0984 (2.9843)	Training Loss4 2.6587 (2.5848)	Training Total_Loss 5.4577 (5.2519)	Training Prec@1_up 98.828 (98.718)	Training Prec@1_down 96.680 (96.822)	
2021-07-29 23:06:27,988: ============================================================
2021-07-29 23:07:57,204: Epoch: 39/65 eta: 9:20:42.760213	Training Loss2 2.0083 (1.9512)	Training Loss3 3.0153 (2.9859)	Training Loss4 2.6479 (2.5855)	Training Total_Loss 5.3433 (5.2542)	Training Prec@1_up 98.242 (98.720)	Training Prec@1_down 97.070 (96.812)	
2021-07-29 23:07:57,204: ============================================================
2021-07-29 23:07:57,206: Save Checkpoint...
2021-07-29 23:07:57,208: ============================================================
2021-07-29 23:07:57,961: Save done!
2021-07-29 23:07:57,961: ============================================================
2021-07-29 23:10:14,111: time cost, forward:0.02285508435181897, backward:0.17847433716359765, data cost:1.0188707317968813 
2021-07-29 23:10:14,111: ============================================================
2021-07-29 23:10:14,111: Epoch 40/65 Batch 100/964 eta: 9:26:29.393020	Training Loss2 2.0242 (1.9610)	Training Loss3 2.9536 (3.0054)	Training Loss4 2.6209 (2.5891)	Training Total_Loss 5.2762 (5.2805)	Training Prec@1_up 98.047 (98.580)	Training Prec@1_down 96.875 (96.697)	
2021-07-29 23:10:14,112: ============================================================
2021-07-29 23:12:27,509: time cost, forward:0.023326069865394476, backward:0.1787468272836963, data cost:1.008665548497109 
2021-07-29 23:12:27,510: ============================================================
2021-07-29 23:12:27,510: Epoch 40/65 Batch 200/964 eta: 9:12:49.509272	Training Loss2 2.0970 (1.9444)	Training Loss3 3.1032 (2.9874)	Training Loss4 2.7406 (2.5763)	Training Total_Loss 5.5221 (5.2477)	Training Prec@1_up 98.633 (98.674)	Training Prec@1_down 96.094 (96.797)	
2021-07-29 23:12:27,510: ============================================================
2021-07-29 23:14:40,011: time cost, forward:0.023606179151247974, backward:0.17861066933060968, data cost:1.0045671646411602 
2021-07-29 23:14:40,011: ============================================================
2021-07-29 23:14:40,011: Epoch 40/65 Batch 300/964 eta: 9:06:54.057087	Training Loss2 2.0073 (1.9458)	Training Loss3 2.9524 (2.9852)	Training Loss4 2.6330 (2.5791)	Training Total_Loss 5.2725 (5.2477)	Training Prec@1_up 98.828 (98.678)	Training Prec@1_down 97.266 (96.825)	
2021-07-29 23:14:40,012: ============================================================
2021-07-29 23:16:53,764: time cost, forward:0.023898812463707793, backward:0.17863362713863976, data cost:1.0056032662403613 
2021-07-29 23:16:53,765: ============================================================
2021-07-29 23:16:53,765: Epoch 40/65 Batch 400/964 eta: 9:09:50.292656	Training Loss2 2.1845 (1.9398)	Training Loss3 3.3190 (2.9754)	Training Loss4 2.8501 (2.5732)	Training Total_Loss 5.8364 (5.2319)	Training Prec@1_up 98.828 (98.723)	Training Prec@1_down 96.094 (96.843)	
2021-07-29 23:16:53,765: ============================================================
2021-07-29 23:19:08,774: time cost, forward:0.023894965051410193, backward:0.17883983929314928, data cost:1.0067873392888682 
2021-07-29 23:19:08,774: ============================================================
2021-07-29 23:19:08,774: Epoch 40/65 Batch 500/964 eta: 9:12:45.035656	Training Loss2 1.9269 (1.9370)	Training Loss3 2.9067 (2.9724)	Training Loss4 2.6077 (2.5696)	Training Total_Loss 5.1740 (5.2257)	Training Prec@1_up 97.852 (98.732)	Training Prec@1_down 97.461 (96.867)	
2021-07-29 23:19:08,775: ============================================================
2021-07-29 23:21:24,346: time cost, forward:0.024024931138664336, backward:0.17893689383250444, data cost:1.0084474767388805 
2021-07-29 23:21:24,346: ============================================================
2021-07-29 23:21:24,347: Epoch 40/65 Batch 600/964 eta: 9:12:47.753502	Training Loss2 1.9413 (1.9336)	Training Loss3 3.0650 (2.9674)	Training Loss4 2.6036 (2.5657)	Training Total_Loss 5.3374 (5.2170)	Training Prec@1_up 98.633 (98.740)	Training Prec@1_down 96.680 (96.874)	
2021-07-29 23:21:24,347: ============================================================
2021-07-29 23:23:38,654: time cost, forward:0.024202809995506624, backward:0.17905971113704305, data cost:1.0078218416424098 
2021-07-29 23:23:38,654: ============================================================
2021-07-29 23:23:38,655: Epoch 40/65 Batch 700/964 eta: 9:05:24.149508	Training Loss2 1.7244 (1.9339)	Training Loss3 2.6745 (2.9674)	Training Loss4 2.3549 (2.5660)	Training Total_Loss 4.7141 (5.2173)	Training Prec@1_up 98.828 (98.740)	Training Prec@1_down 98.047 (96.863)	
2021-07-29 23:23:38,655: ============================================================
2021-07-29 23:25:51,961: time cost, forward:0.0241072210710547, backward:0.17906592754607506, data cost:1.0062790361602554 
2021-07-29 23:25:51,961: ============================================================
2021-07-29 23:25:51,961: Epoch 40/65 Batch 800/964 eta: 8:59:06.892852	Training Loss2 1.8827 (1.9336)	Training Loss3 2.8321 (2.9668)	Training Loss4 2.4930 (2.5663)	Training Total_Loss 5.0200 (5.2167)	Training Prec@1_up 98.242 (98.753)	Training Prec@1_down 96.484 (96.863)	
2021-07-29 23:25:51,961: ============================================================
2021-07-29 23:28:05,509: time cost, forward:0.024002373019633223, backward:0.17903111057896767, data cost:1.0038647367903866 
2021-07-29 23:28:05,509: ============================================================
2021-07-29 23:28:05,510: Epoch 40/65 Batch 900/964 eta: 8:57:51.928841	Training Loss2 1.9326 (1.9341)	Training Loss3 2.9248 (2.9661)	Training Loss4 2.5638 (2.5670)	Training Total_Loss 5.1730 (5.2167)	Training Prec@1_up 98.242 (98.747)	Training Prec@1_down 96.484 (96.859)	
2021-07-29 23:28:05,510: ============================================================
2021-07-29 23:29:32,128: Epoch: 40/65 eta: 8:56:25.122494	Training Loss2 2.1099 (1.9367)	Training Loss3 3.2101 (2.9690)	Training Loss4 2.7867 (2.5697)	Training Total_Loss 5.6584 (5.2222)	Training Prec@1_up 98.828 (98.732)	Training Prec@1_down 95.898 (96.848)	
2021-07-29 23:29:32,128: ============================================================
2021-07-29 23:29:32,131: Save Checkpoint...
2021-07-29 23:29:32,134: ============================================================
2021-07-29 23:29:32,793: Save done!
2021-07-29 23:29:32,793: ============================================================
2021-07-29 23:31:49,730: time cost, forward:0.024026538386489407, backward:0.17841899515402437, data cost:1.0474669885153722 
2021-07-29 23:31:49,731: ============================================================
2021-07-29 23:31:49,731: Epoch 41/65 Batch 100/964 eta: 9:07:46.168866	Training Loss2 1.9399 (1.9303)	Training Loss3 2.9385 (2.9650)	Training Loss4 2.6613 (2.5579)	Training Total_Loss 5.2391 (5.2091)	Training Prec@1_up 99.023 (98.686)	Training Prec@1_down 97.656 (96.816)	
2021-07-29 23:31:49,731: ============================================================
2021-07-29 23:34:05,010: time cost, forward:0.024168824430686145, backward:0.17900331295914387, data cost:1.0322337102650398 
2021-07-29 23:34:05,010: ============================================================
2021-07-29 23:34:05,011: Epoch 41/65 Batch 200/964 eta: 8:58:53.165435	Training Loss2 1.9959 (1.9291)	Training Loss3 2.9092 (2.9620)	Training Loss4 2.5978 (2.5586)	Training Total_Loss 5.2060 (5.2059)	Training Prec@1_up 99.219 (98.703)	Training Prec@1_down 97.266 (96.831)	
2021-07-29 23:34:05,011: ============================================================
2021-07-29 23:36:20,392: time cost, forward:0.024571794331273107, backward:0.17936109539657133, data cost:1.0274528564019347 
2021-07-29 23:36:20,392: ============================================================
2021-07-29 23:36:20,393: Epoch 41/65 Batch 300/964 eta: 8:57:02.252959	Training Loss2 1.9774 (1.9330)	Training Loss3 3.0490 (2.9677)	Training Loss4 2.6385 (2.5641)	Training Total_Loss 5.3570 (5.2163)	Training Prec@1_up 98.242 (98.713)	Training Prec@1_down 96.094 (96.843)	
2021-07-29 23:36:20,393: ============================================================
2021-07-29 23:38:34,779: time cost, forward:0.024677368632534093, backward:0.17939829049552591, data cost:1.022622393486195 
2021-07-29 23:38:34,779: ============================================================
2021-07-29 23:38:34,779: Epoch 41/65 Batch 400/964 eta: 8:50:51.053171	Training Loss2 1.8426 (1.9336)	Training Loss3 2.9477 (2.9644)	Training Loss4 2.4158 (2.5675)	Training Total_Loss 5.0769 (5.2149)	Training Prec@1_up 99.023 (98.731)	Training Prec@1_down 96.875 (96.840)	
2021-07-29 23:38:34,780: ============================================================
2021-07-29 23:40:49,012: time cost, forward:0.02459441444916811, backward:0.17940078326360973, data cost:1.0194719229528086 
2021-07-29 23:40:49,012: ============================================================
2021-07-29 23:40:49,012: Epoch 41/65 Batch 500/964 eta: 8:48:00.306087	Training Loss2 2.0344 (1.9332)	Training Loss3 3.0116 (2.9649)	Training Loss4 2.6778 (2.5660)	Training Total_Loss 5.3677 (5.2145)	Training Prec@1_up 99.219 (98.738)	Training Prec@1_down 97.461 (96.849)	
2021-07-29 23:40:49,012: ============================================================
2021-07-29 23:43:01,458: time cost, forward:0.024481668297157862, backward:0.17942082145576285, data cost:1.0143721465873399 
2021-07-29 23:43:01,458: ============================================================
2021-07-29 23:43:01,459: Epoch 41/65 Batch 600/964 eta: 8:38:46.171800	Training Loss2 1.9818 (1.9337)	Training Loss3 3.1642 (2.9659)	Training Loss4 2.6461 (2.5671)	Training Total_Loss 5.4782 (5.2163)	Training Prec@1_up 98.828 (98.746)	Training Prec@1_down 96.875 (96.854)	
2021-07-29 23:43:01,459: ============================================================
2021-07-29 23:45:14,151: time cost, forward:0.024303142945994977, backward:0.17926860640147896, data cost:1.0113587519300513 
2021-07-29 23:45:14,152: ============================================================
2021-07-29 23:45:14,152: Epoch 41/65 Batch 700/964 eta: 8:37:31.524990	Training Loss2 2.0007 (1.9344)	Training Loss3 3.0788 (2.9651)	Training Loss4 2.6429 (2.5676)	Training Total_Loss 5.4006 (5.2162)	Training Prec@1_up 97.656 (98.743)	Training Prec@1_down 95.703 (96.859)	
2021-07-29 23:45:14,152: ============================================================
2021-07-29 23:47:26,810: time cost, forward:0.02409950812558209, backward:0.17916551101789605, data cost:1.0090160158011732 
2021-07-29 23:47:26,810: ============================================================
2021-07-29 23:47:26,810: Epoch 41/65 Batch 800/964 eta: 8:35:10.772352	Training Loss2 1.8610 (1.9334)	Training Loss3 3.0405 (2.9651)	Training Loss4 2.5157 (2.5665)	Training Total_Loss 5.2289 (5.2151)	Training Prec@1_up 99.219 (98.742)	Training Prec@1_down 96.680 (96.862)	
2021-07-29 23:47:26,810: ============================================================
2021-07-29 23:49:39,789: time cost, forward:0.023915672196164414, backward:0.17910566902797134, data cost:1.007512783977691 
2021-07-29 23:49:39,790: ============================================================
2021-07-29 23:49:39,790: Epoch 41/65 Batch 900/964 eta: 8:34:12.563077	Training Loss2 1.8789 (1.9353)	Training Loss3 2.8762 (2.9677)	Training Loss4 2.5722 (2.5684)	Training Total_Loss 5.1018 (5.2196)	Training Prec@1_up 99.414 (98.731)	Training Prec@1_down 97.266 (96.848)	
2021-07-29 23:49:39,790: ============================================================
2021-07-29 23:51:08,811: Epoch: 41/65 eta: 8:32:46.126432	Training Loss2 2.0019 (1.9359)	Training Loss3 3.1330 (2.9677)	Training Loss4 2.5750 (2.5693)	Training Total_Loss 5.4214 (5.2203)	Training Prec@1_up 99.609 (98.734)	Training Prec@1_down 96.680 (96.849)	
2021-07-29 23:51:08,811: ============================================================
2021-07-29 23:51:08,813: Save Checkpoint...
2021-07-29 23:51:08,814: ============================================================
2021-07-29 23:51:09,482: Save done!
2021-07-29 23:51:09,482: ============================================================
2021-07-29 23:53:23,905: time cost, forward:0.02358473190153488, backward:0.17717574100301722, data cost:1.0191056511618874 
2021-07-29 23:53:23,905: ============================================================
2021-07-29 23:53:23,905: Epoch 42/65 Batch 100/964 eta: 8:36:06.631515	Training Loss2 2.0249 (1.9473)	Training Loss3 3.0714 (2.9855)	Training Loss4 2.6554 (2.5885)	Training Total_Loss 5.4115 (5.2534)	Training Prec@1_up 99.023 (98.637)	Training Prec@1_down 97.461 (96.782)	
2021-07-29 23:53:23,905: ============================================================
2021-07-29 23:55:38,160: time cost, forward:0.024090909478652417, backward:0.17789707111952893, data cost:1.0149048105556162 
2021-07-29 23:55:38,160: ============================================================
2021-07-29 23:55:38,160: Epoch 42/65 Batch 200/964 eta: 8:33:14.051439	Training Loss2 2.0676 (1.9471)	Training Loss3 3.1395 (2.9833)	Training Loss4 2.7631 (2.5812)	Training Total_Loss 5.5549 (5.2474)	Training Prec@1_up 98.438 (98.666)	Training Prec@1_down 96.680 (96.782)	
2021-07-29 23:55:38,160: ============================================================
2021-07-29 23:57:50,861: time cost, forward:0.02391363306587755, backward:0.1783240232180592, data cost:1.0070994912979994 
2021-07-29 23:57:50,861: ============================================================
2021-07-29 23:57:50,879: Epoch 42/65 Batch 300/964 eta: 8:25:09.056946	Training Loss2 1.8003 (1.9422)	Training Loss3 2.7998 (2.9775)	Training Loss4 2.4242 (2.5771)	Training Total_Loss 4.9120 (5.2372)	Training Prec@1_up 99.023 (98.714)	Training Prec@1_down 98.047 (96.780)	
2021-07-29 23:57:50,879: ============================================================
2021-07-30 00:00:03,646: time cost, forward:0.023845589549320385, backward:0.17854881226867064, data cost:1.0034912259955155 
2021-07-30 00:00:03,646: ============================================================
2021-07-30 00:00:03,647: Epoch 42/65 Batch 400/964 eta: 8:23:07.317781	Training Loss2 1.9513 (1.9428)	Training Loss3 2.9510 (2.9770)	Training Loss4 2.5702 (2.5774)	Training Total_Loss 5.2118 (5.2371)	Training Prec@1_up 99.023 (98.717)	Training Prec@1_down 96.875 (96.806)	
2021-07-30 00:00:03,647: ============================================================
2021-07-30 00:02:16,238: time cost, forward:0.023923294816561833, backward:0.17858125403792205, data cost:1.0010998163051261 
2021-07-30 00:02:16,238: ============================================================
2021-07-30 00:02:16,238: Epoch 42/65 Batch 500/964 eta: 8:20:14.802792	Training Loss2 1.9973 (1.9420)	Training Loss3 3.0378 (2.9738)	Training Loss4 2.6264 (2.5761)	Training Total_Loss 5.3496 (5.2329)	Training Prec@1_up 99.219 (98.716)	Training Prec@1_down 96.680 (96.822)	
2021-07-30 00:02:16,239: ============================================================
2021-07-30 00:04:31,055: time cost, forward:0.024048457360625865, backward:0.17857108967929133, data cost:1.0031465986535226 
2021-07-30 00:04:31,055: ============================================================
2021-07-30 00:04:31,055: Epoch 42/65 Batch 600/964 eta: 8:26:23.630702	Training Loss2 2.2728 (1.9416)	Training Loss3 3.3072 (2.9729)	Training Loss4 2.8726 (2.5757)	Training Total_Loss 5.8799 (5.2316)	Training Prec@1_up 97.461 (98.722)	Training Prec@1_down 94.727 (96.828)	
2021-07-30 00:04:31,055: ============================================================
2021-07-30 00:06:44,904: time cost, forward:0.024120124795065076, backward:0.17865782131964555, data cost:1.0031659313196446 
2021-07-30 00:06:44,904: ============================================================
2021-07-30 00:06:44,904: Epoch 42/65 Batch 700/964 eta: 8:20:31.767367	Training Loss2 1.9583 (1.9398)	Training Loss3 2.9614 (2.9715)	Training Loss4 2.6548 (2.5735)	Training Total_Loss 5.2679 (5.2281)	Training Prec@1_up 99.023 (98.725)	Training Prec@1_down 97.266 (96.838)	
2021-07-30 00:06:44,905: ============================================================
2021-07-30 00:08:57,594: time cost, forward:0.024212314727458547, backward:0.17880272208823728, data cost:1.0016189618164368 
2021-07-30 00:08:57,594: ============================================================
2021-07-30 00:08:57,594: Epoch 42/65 Batch 800/964 eta: 8:13:58.907531	Training Loss2 1.9568 (1.9381)	Training Loss3 3.0010 (2.9701)	Training Loss4 2.5105 (2.5715)	Training Total_Loss 5.2347 (5.2249)	Training Prec@1_up 98.828 (98.730)	Training Prec@1_down 97.266 (96.842)	
2021-07-30 00:08:57,594: ============================================================
2021-07-30 00:11:10,161: time cost, forward:0.024220012584172845, backward:0.1790166686188525, data cost:1.0001665156197892 
2021-07-30 00:11:10,161: ============================================================
2021-07-30 00:11:10,161: Epoch 42/65 Batch 900/964 eta: 8:11:18.986596	Training Loss2 1.8410 (1.9361)	Training Loss3 2.8631 (2.9675)	Training Loss4 2.4397 (2.5694)	Training Total_Loss 5.0034 (5.2202)	Training Prec@1_up 98.633 (98.738)	Training Prec@1_down 97.070 (96.851)	
2021-07-30 00:11:10,161: ============================================================
2021-07-30 00:12:35,859: Epoch: 42/65 eta: 8:09:52.817862	Training Loss2 1.7456 (1.9353)	Training Loss3 2.7795 (2.9669)	Training Loss4 2.4006 (2.5685)	Training Total_Loss 4.8526 (5.2188)	Training Prec@1_up 99.414 (98.737)	Training Prec@1_down 97.852 (96.851)	
2021-07-30 00:12:35,860: ============================================================
2021-07-30 00:12:35,862: Save Checkpoint...
2021-07-30 00:12:35,863: ============================================================
2021-07-30 00:12:36,605: Save done!
2021-07-30 00:12:36,606: ============================================================
2021-07-30 00:14:49,603: time cost, forward:0.02398206970908425, backward:0.178389806940098, data cost:1.0071552883494983 
2021-07-30 00:14:49,604: ============================================================
2021-07-30 00:14:49,604: Epoch 43/65 Batch 100/964 eta: 8:09:16.329289	Training Loss2 2.1208 (1.9358)	Training Loss3 3.0958 (2.9689)	Training Loss4 2.7681 (2.5683)	Training Total_Loss 5.5403 (5.2210)	Training Prec@1_up 98.242 (98.696)	Training Prec@1_down 96.289 (96.863)	
2021-07-30 00:14:49,604: ============================================================
2021-07-30 00:17:03,506: time cost, forward:0.024942109333210855, backward:0.17877367513263645, data cost:1.0057763945517229 
2021-07-30 00:17:03,506: ============================================================
2021-07-30 00:17:03,506: Epoch 43/65 Batch 200/964 eta: 8:10:22.347690	Training Loss2 1.9642 (1.9285)	Training Loss3 2.9677 (2.9607)	Training Loss4 2.6338 (2.5609)	Training Total_Loss 5.2667 (5.2054)	Training Prec@1_up 99.023 (98.706)	Training Prec@1_down 97.266 (96.882)	
2021-07-30 00:17:03,506: ============================================================
2021-07-30 00:19:15,873: time cost, forward:0.02474994723214752, backward:0.17847113146829766, data cost:1.0008252041794385 
2021-07-30 00:19:15,873: ============================================================
2021-07-30 00:19:15,873: Epoch 43/65 Batch 300/964 eta: 8:02:32.580219	Training Loss2 1.9906 (1.9250)	Training Loss3 3.0591 (2.9530)	Training Loss4 2.6587 (2.5563)	Training Total_Loss 5.3837 (5.1936)	Training Prec@1_up 98.828 (98.722)	Training Prec@1_down 96.484 (96.893)	
2021-07-30 00:19:15,873: ============================================================
2021-07-30 00:21:27,642: time cost, forward:0.024572609062481644, backward:0.17821549891230457, data cost:0.9966105452755041 
2021-07-30 00:21:27,643: ============================================================
2021-07-30 00:21:27,643: Epoch 43/65 Batch 400/964 eta: 7:58:10.220945	Training Loss2 1.6143 (1.9264)	Training Loss3 2.6141 (2.9554)	Training Loss4 2.3159 (2.5573)	Training Total_Loss 4.5792 (5.1973)	Training Prec@1_up 99.414 (98.740)	Training Prec@1_down 98.242 (96.899)	
2021-07-30 00:21:27,643: ============================================================
2021-07-30 00:23:40,628: time cost, forward:0.024446802769968648, backward:0.1782437230876548, data cost:0.996311820819526 
2021-07-30 00:23:40,629: ============================================================
2021-07-30 00:23:40,629: Epoch 43/65 Batch 500/964 eta: 8:00:22.088915	Training Loss2 1.9317 (1.9283)	Training Loss3 2.8469 (2.9575)	Training Loss4 2.5488 (2.5609)	Training Total_Loss 5.0872 (5.2021)	Training Prec@1_up 99.609 (98.733)	Training Prec@1_down 97.852 (96.890)	
2021-07-30 00:23:40,629: ============================================================
2021-07-30 00:25:53,594: time cost, forward:0.024264851476194862, backward:0.17822859840520436, data cost:0.9965299883350506 
2021-07-30 00:25:53,594: ============================================================
2021-07-30 00:25:53,594: Epoch 43/65 Batch 600/964 eta: 7:58:04.571526	Training Loss2 1.9145 (1.9319)	Training Loss3 2.7975 (2.9637)	Training Loss4 2.5559 (2.5646)	Training Total_Loss 5.0327 (5.2119)	Training Prec@1_up 99.023 (98.736)	Training Prec@1_down 97.070 (96.876)	
2021-07-30 00:25:53,594: ============================================================
2021-07-30 00:28:07,268: time cost, forward:0.024197539546459017, backward:0.17834166020623946, data cost:0.9973189755741277 
2021-07-30 00:28:07,268: ============================================================
2021-07-30 00:28:07,268: Epoch 43/65 Batch 700/964 eta: 7:58:23.871934	Training Loss2 1.8632 (1.9308)	Training Loss3 2.8592 (2.9613)	Training Loss4 2.4737 (2.5629)	Training Total_Loss 5.0276 (5.2082)	Training Prec@1_up 98.828 (98.744)	Training Prec@1_down 97.852 (96.883)	
2021-07-30 00:28:07,268: ============================================================
2021-07-30 00:30:20,423: time cost, forward:0.024119428161983942, backward:0.17839324011820576, data cost:0.9967894199046683 
2021-07-30 00:30:20,423: ============================================================
2021-07-30 00:30:20,423: Epoch 43/65 Batch 800/964 eta: 7:54:19.168308	Training Loss2 1.8850 (1.9331)	Training Loss3 2.7489 (2.9640)	Training Loss4 2.5890 (2.5663)	Training Total_Loss 4.9859 (5.2137)	Training Prec@1_up 99.023 (98.740)	Training Prec@1_down 97.656 (96.876)	
2021-07-30 00:30:20,423: ============================================================
2021-07-30 00:32:33,027: time cost, forward:0.024046159029272162, backward:0.17844562774505446, data cost:0.996307534556235 
2021-07-30 00:32:33,028: ============================================================
2021-07-30 00:32:33,028: Epoch 43/65 Batch 900/964 eta: 7:50:09.018746	Training Loss2 1.8907 (1.9329)	Training Loss3 2.9772 (2.9649)	Training Loss4 2.5085 (2.5658)	Training Total_Loss 5.1768 (5.2143)	Training Prec@1_up 99.414 (98.744)	Training Prec@1_down 96.094 (96.863)	
2021-07-30 00:32:33,028: ============================================================
2021-07-30 00:34:00,023: Epoch: 43/65 eta: 7:48:42.825627	Training Loss2 2.0803 (1.9347)	Training Loss3 3.0166 (2.9660)	Training Loss4 2.7082 (2.5682)	Training Total_Loss 5.4108 (5.2175)	Training Prec@1_up 98.242 (98.737)	Training Prec@1_down 95.898 (96.857)	
2021-07-30 00:34:00,024: ============================================================
2021-07-30 00:34:00,025: Save Checkpoint...
2021-07-30 00:34:00,025: ============================================================
2021-07-30 00:34:00,716: Save done!
2021-07-30 00:34:00,716: ============================================================
2021-07-30 00:36:16,172: time cost, forward:0.024590362202037464, backward:0.17930233839786414, data cost:1.030117150508996 
2021-07-30 00:36:16,172: ============================================================
2021-07-30 00:36:16,172: Epoch 44/65 Batch 100/964 eta: 7:56:33.156477	Training Loss2 2.0427 (1.9424)	Training Loss3 3.0847 (2.9739)	Training Loss4 2.6431 (2.5742)	Training Total_Loss 5.4276 (5.2321)	Training Prec@1_up 98.438 (98.735)	Training Prec@1_down 96.484 (96.841)	
2021-07-30 00:36:16,172: ============================================================
2021-07-30 00:38:29,095: time cost, forward:0.02488228663727267, backward:0.17907404779788838, data cost:1.0118954001958647 
2021-07-30 00:38:29,095: ============================================================
2021-07-30 00:38:29,096: Epoch 44/65 Batch 200/964 eta: 7:45:25.878393	Training Loss2 1.9741 (1.9352)	Training Loss3 3.0753 (2.9627)	Training Loss4 2.5910 (2.5689)	Training Total_Loss 5.3579 (5.2148)	Training Prec@1_up 98.047 (98.767)	Training Prec@1_down 97.070 (96.899)	
2021-07-30 00:38:29,096: ============================================================
2021-07-30 00:40:43,007: time cost, forward:0.024735944326904704, backward:0.17933154504833412, data cost:1.0088663922504437 
2021-07-30 00:40:43,008: ============================================================
2021-07-30 00:40:43,008: Epoch 44/65 Batch 300/964 eta: 7:46:39.712162	Training Loss2 1.8363 (1.9311)	Training Loss3 2.8132 (2.9577)	Training Loss4 2.4191 (2.5636)	Training Total_Loss 4.9409 (5.2051)	Training Prec@1_up 99.219 (98.744)	Training Prec@1_down 97.461 (96.889)	
2021-07-30 00:40:43,008: ============================================================
2021-07-30 00:42:54,850: time cost, forward:0.024506568908691406, backward:0.1795564994477389, data cost:1.0020802636493118 
2021-07-30 00:42:54,851: ============================================================
2021-07-30 00:42:54,851: Epoch 44/65 Batch 400/964 eta: 7:37:15.245429	Training Loss2 2.0642 (1.9279)	Training Loss3 3.0715 (2.9547)	Training Loss4 2.6886 (2.5604)	Training Total_Loss 5.4479 (5.1988)	Training Prec@1_up 98.438 (98.743)	Training Prec@1_down 96.094 (96.892)	
2021-07-30 00:42:54,851: ============================================================
2021-07-30 00:45:08,586: time cost, forward:0.02442220982186541, backward:0.17964754219284515, data cost:1.0017296919125116 
2021-07-30 00:45:08,586: ============================================================
2021-07-30 00:45:08,586: Epoch 44/65 Batch 500/964 eta: 7:41:35.174068	Training Loss2 1.9260 (1.9279)	Training Loss3 2.9731 (2.9551)	Training Loss4 2.5630 (2.5604)	Training Total_Loss 5.2176 (5.1993)	Training Prec@1_up 98.438 (98.748)	Training Prec@1_down 97.070 (96.910)	
2021-07-30 00:45:08,586: ============================================================
2021-07-30 00:47:22,027: time cost, forward:0.02436970908176123, backward:0.17947450981713298, data cost:1.0014083938726002 
2021-07-30 00:47:22,027: ============================================================
2021-07-30 00:47:22,027: Epoch 44/65 Batch 600/964 eta: 7:38:20.845417	Training Loss2 2.0196 (1.9321)	Training Loss3 3.0477 (2.9599)	Training Loss4 2.6593 (2.5638)	Training Total_Loss 5.3871 (5.2079)	Training Prec@1_up 97.852 (98.735)	Training Prec@1_down 96.484 (96.876)	
2021-07-30 00:47:22,027: ============================================================
2021-07-30 00:49:34,414: time cost, forward:0.024310057425874155, backward:0.17939222455877432, data cost:0.9996700587020241 
2021-07-30 00:49:34,414: ============================================================
2021-07-30 00:49:34,415: Epoch 44/65 Batch 700/964 eta: 7:32:31.389742	Training Loss2 2.0161 (1.9333)	Training Loss3 3.1499 (2.9627)	Training Loss4 2.7032 (2.5654)	Training Total_Loss 5.5095 (5.2121)	Training Prec@1_up 98.828 (98.736)	Training Prec@1_down 97.266 (96.865)	
2021-07-30 00:49:34,415: ============================================================
2021-07-30 00:51:46,589: time cost, forward:0.02408788947199701, backward:0.17928203414468205, data cost:0.9981893138384192 
2021-07-30 00:51:46,589: ============================================================
2021-07-30 00:51:46,589: Epoch 44/65 Batch 800/964 eta: 7:29:35.484944	Training Loss2 1.7860 (1.9350)	Training Loss3 2.8098 (2.9649)	Training Loss4 2.4276 (2.5679)	Training Total_Loss 4.9166 (5.2163)	Training Prec@1_up 98.242 (98.737)	Training Prec@1_down 97.461 (96.866)	
2021-07-30 00:51:46,589: ============================================================
2021-07-30 00:53:59,983: time cost, forward:0.023870965662734528, backward:0.17914911955429264, data cost:0.9984455890994979 
2021-07-30 00:53:59,983: ============================================================
2021-07-30 00:53:59,984: Epoch 44/65 Batch 900/964 eta: 7:31:31.112444	Training Loss2 1.9825 (1.9349)	Training Loss3 3.1369 (2.9657)	Training Loss4 2.6171 (2.5679)	Training Total_Loss 5.4367 (5.2171)	Training Prec@1_up 98.242 (98.739)	Training Prec@1_down 96.484 (96.865)	
2021-07-30 00:53:59,984: ============================================================
2021-07-30 00:55:27,634: Epoch: 44/65 eta: 7:30:04.405944	Training Loss2 2.0007 (1.9342)	Training Loss3 3.1790 (2.9651)	Training Loss4 2.5923 (2.5668)	Training Total_Loss 5.4755 (5.2155)	Training Prec@1_up 98.242 (98.737)	Training Prec@1_down 96.484 (96.857)	
2021-07-30 00:55:27,635: ============================================================
2021-07-30 00:55:27,637: Save Checkpoint...
2021-07-30 00:55:27,640: ============================================================
2021-07-30 00:55:28,346: Save done!
2021-07-30 00:55:28,347: ============================================================
2021-07-30 00:57:41,889: time cost, forward:0.02623979009763159, backward:0.17975716639046718, data cost:1.011114968193902 
2021-07-30 00:57:41,889: ============================================================
2021-07-30 00:57:41,889: Epoch 45/65 Batch 100/964 eta: 7:28:21.918383	Training Loss2 1.9174 (1.9476)	Training Loss3 3.0377 (2.9733)	Training Loss4 2.5748 (2.5810)	Training Total_Loss 5.2838 (5.2375)	Training Prec@1_up 98.633 (98.607)	Training Prec@1_down 96.875 (96.766)	
2021-07-30 00:57:41,890: ============================================================
2021-07-30 00:59:54,263: time cost, forward:0.026229617583691776, backward:0.1797402048829812, data cost:0.9992726352346603 
2021-07-30 00:59:54,264: ============================================================
2021-07-30 00:59:54,264: Epoch 45/65 Batch 200/964 eta: 7:22:14.415962	Training Loss2 2.0618 (1.9381)	Training Loss3 3.0626 (2.9624)	Training Loss4 2.7511 (2.5771)	Training Total_Loss 5.4691 (5.2200)	Training Prec@1_up 98.828 (98.702)	Training Prec@1_down 96.875 (96.850)	
2021-07-30 00:59:54,264: ============================================================
2021-07-30 01:02:07,469: time cost, forward:0.0261106977494664, backward:0.1797806682395297, data cost:0.9982746915275038 
2021-07-30 01:02:07,470: ============================================================
2021-07-30 01:02:07,470: Epoch 45/65 Batch 300/964 eta: 7:22:47.937621	Training Loss2 1.8521 (1.9289)	Training Loss3 2.8323 (2.9555)	Training Loss4 2.4482 (2.5657)	Training Total_Loss 4.9824 (5.2028)	Training Prec@1_up 99.414 (98.722)	Training Prec@1_down 97.461 (96.881)	
2021-07-30 01:02:07,470: ============================================================
2021-07-30 01:04:21,011: time cost, forward:0.02571565704536916, backward:0.17975892877220212, data cost:0.9986537345369956 
2021-07-30 01:04:21,011: ============================================================
2021-07-30 01:04:21,011: Epoch 45/65 Batch 400/964 eta: 7:21:41.336479	Training Loss2 1.9639 (1.9275)	Training Loss3 3.0348 (2.9558)	Training Loss4 2.6056 (2.5625)	Training Total_Loss 5.3196 (5.2008)	Training Prec@1_up 98.633 (98.741)	Training Prec@1_down 96.680 (96.877)	
2021-07-30 01:04:21,011: ============================================================
2021-07-30 01:06:33,488: time cost, forward:0.025810489673652726, backward:0.17982932322010967, data cost:0.9966473880416167 
2021-07-30 01:06:33,488: ============================================================
2021-07-30 01:06:33,488: Epoch 45/65 Batch 500/964 eta: 7:15:57.532949	Training Loss2 2.0031 (1.9269)	Training Loss3 3.0660 (2.9550)	Training Loss4 2.6118 (2.5607)	Training Total_Loss 5.3734 (5.1988)	Training Prec@1_up 98.242 (98.737)	Training Prec@1_down 96.094 (96.877)	
2021-07-30 01:06:33,488: ============================================================
2021-07-30 01:08:48,453: time cost, forward:0.025715750326497328, backward:0.17984577212389402, data cost:0.9993215064970599 
2021-07-30 01:08:48,454: ============================================================
2021-07-30 01:08:48,454: Epoch 45/65 Batch 600/964 eta: 7:21:54.020928	Training Loss2 1.9472 (1.9286)	Training Loss3 3.0387 (2.9565)	Training Loss4 2.5944 (2.5623)	Training Total_Loss 5.3095 (5.2020)	Training Prec@1_up 99.023 (98.735)	Training Prec@1_down 96.289 (96.868)	
2021-07-30 01:08:48,454: ============================================================
2021-07-30 01:11:01,486: time cost, forward:0.02562770925366316, backward:0.17985529824558416, data cost:0.9987143013780891 
2021-07-30 01:11:01,487: ============================================================
2021-07-30 01:11:01,487: Epoch 45/65 Batch 700/964 eta: 7:13:21.358144	Training Loss2 1.7146 (1.9283)	Training Loss3 2.6851 (2.9575)	Training Loss4 2.3255 (2.5609)	Training Total_Loss 4.7051 (5.2021)	Training Prec@1_up 99.609 (98.747)	Training Prec@1_down 98.047 (96.869)	
2021-07-30 01:11:01,487: ============================================================
2021-07-30 01:13:14,130: time cost, forward:0.025564843632551248, backward:0.1797912592285118, data cost:0.9978321998080563 
2021-07-30 01:13:14,130: ============================================================
2021-07-30 01:13:14,130: Epoch 45/65 Batch 800/964 eta: 7:09:52.430123	Training Loss2 1.9245 (1.9317)	Training Loss3 2.9784 (2.9615)	Training Loss4 2.5599 (2.5643)	Training Total_Loss 5.2206 (5.2095)	Training Prec@1_up 98.633 (98.734)	Training Prec@1_down 96.875 (96.861)	
2021-07-30 01:13:14,130: ============================================================
2021-07-30 01:15:26,845: time cost, forward:0.02546165305064438, backward:0.17973868785896344, data cost:0.9972440578515855 
2021-07-30 01:15:26,846: ============================================================
2021-07-30 01:15:26,846: Epoch 45/65 Batch 900/964 eta: 7:07:53.863217	Training Loss2 1.9341 (1.9326)	Training Loss3 2.9161 (2.9632)	Training Loss4 2.5437 (2.5652)	Training Total_Loss 5.1550 (5.2120)	Training Prec@1_up 98.438 (98.734)	Training Prec@1_down 97.656 (96.858)	
2021-07-30 01:15:26,846: ============================================================
2021-07-30 01:16:54,940: Epoch: 45/65 eta: 7:06:27.597975	Training Loss2 2.2658 (1.9337)	Training Loss3 3.4035 (2.9644)	Training Loss4 2.8190 (2.5666)	Training Total_Loss 5.9459 (5.2146)	Training Prec@1_up 97.852 (98.738)	Training Prec@1_down 95.703 (96.859)	
2021-07-30 01:16:54,940: ============================================================
2021-07-30 01:16:54,943: Save Checkpoint...
2021-07-30 01:16:54,944: ============================================================
2021-07-30 01:16:55,635: Save done!
2021-07-30 01:16:55,635: ============================================================
2021-07-30 01:19:11,209: time cost, forward:0.02443483622387202, backward:0.17927897337711218, data cost:1.0326520890900583 
2021-07-30 01:19:11,210: ============================================================
2021-07-30 01:19:11,210: Epoch 46/65 Batch 100/964 eta: 7:13:24.246420	Training Loss2 1.8433 (1.9273)	Training Loss3 2.7655 (2.9612)	Training Loss4 2.4662 (2.5638)	Training Total_Loss 4.9203 (5.2068)	Training Prec@1_up 99.023 (98.714)	Training Prec@1_down 97.266 (96.863)	
2021-07-30 01:19:11,210: ============================================================
2021-07-30 01:21:25,102: time cost, forward:0.024458172333300412, backward:0.179416654097974, data cost:1.0180378415476736 
2021-07-30 01:21:25,102: ============================================================
2021-07-30 01:21:25,102: Epoch 46/65 Batch 200/964 eta: 7:05:47.958580	Training Loss2 1.9265 (1.9236)	Training Loss3 2.9343 (2.9530)	Training Loss4 2.5772 (2.5589)	Training Total_Loss 5.1862 (5.1943)	Training Prec@1_up 99.414 (98.748)	Training Prec@1_down 97.070 (96.912)	
2021-07-30 01:21:25,102: ============================================================
2021-07-30 01:23:36,977: time cost, forward:0.024614291047571495, backward:0.1795560101601591, data cost:1.0064044620679773 
2021-07-30 01:23:36,977: ============================================================
2021-07-30 01:23:36,977: Epoch 46/65 Batch 300/964 eta: 6:57:11.258011	Training Loss2 2.0408 (1.9278)	Training Loss3 2.9676 (2.9551)	Training Loss4 2.6149 (2.5649)	Training Total_Loss 5.2955 (5.2015)	Training Prec@1_up 98.438 (98.732)	Training Prec@1_down 96.875 (96.902)	
2021-07-30 01:23:36,978: ============================================================
2021-07-30 01:25:50,249: time cost, forward:0.025049323127383276, backward:0.17972695917115175, data cost:1.0038876348270809 
2021-07-30 01:25:50,249: ============================================================
2021-07-30 01:25:50,249: Epoch 46/65 Batch 400/964 eta: 6:59:23.063511	Training Loss2 1.9122 (1.9275)	Training Loss3 2.8203 (2.9598)	Training Loss4 2.5846 (2.5635)	Training Total_Loss 5.0687 (5.2053)	Training Prec@1_up 98.828 (98.749)	Training Prec@1_down 97.266 (96.898)	
2021-07-30 01:25:50,249: ============================================================
2021-07-30 01:28:03,689: time cost, forward:0.025018761296549397, backward:0.17955429233864456, data cost:1.0030976435942258 
2021-07-30 01:28:03,689: ============================================================
2021-07-30 01:28:03,689: Epoch 46/65 Batch 500/964 eta: 6:57:41.301260	Training Loss2 1.9479 (1.9258)	Training Loss3 2.9620 (2.9590)	Training Loss4 2.5927 (2.5601)	Training Total_Loss 5.2323 (5.2020)	Training Prec@1_up 98.242 (98.767)	Training Prec@1_down 96.289 (96.892)	
2021-07-30 01:28:03,689: ============================================================
2021-07-30 01:30:16,118: time cost, forward:0.02486909450792112, backward:0.17948413969877366, data cost:1.0008372989839225 
2021-07-30 01:30:16,119: ============================================================
2021-07-30 01:30:16,119: Epoch 46/65 Batch 600/964 eta: 6:52:19.211924	Training Loss2 1.8056 (1.9290)	Training Loss3 2.8541 (2.9621)	Training Loss4 2.3899 (2.5630)	Training Total_Loss 4.9518 (5.2081)	Training Prec@1_up 98.828 (98.743)	Training Prec@1_down 97.461 (96.870)	
2021-07-30 01:30:16,119: ============================================================
2021-07-30 01:32:27,923: time cost, forward:0.02495439915527431, backward:0.17946816684520978, data cost:0.9982855548503914 
2021-07-30 01:32:27,923: ============================================================
2021-07-30 01:32:27,923: Epoch 46/65 Batch 700/964 eta: 6:48:10.596778	Training Loss2 2.0063 (1.9318)	Training Loss3 3.1218 (2.9646)	Training Loss4 2.6490 (2.5655)	Training Total_Loss 5.4495 (5.2132)	Training Prec@1_up 97.461 (98.729)	Training Prec@1_down 96.094 (96.850)	
2021-07-30 01:32:27,923: ============================================================
2021-07-30 01:34:41,343: time cost, forward:0.025093884879865396, backward:0.17958038709638116, data cost:0.9982189398086414 
2021-07-30 01:34:41,343: ============================================================
2021-07-30 01:34:41,343: Epoch 46/65 Batch 800/964 eta: 6:50:57.345367	Training Loss2 2.0409 (1.9332)	Training Loss3 3.0629 (2.9644)	Training Loss4 2.7163 (2.5660)	Training Total_Loss 5.4414 (5.2140)	Training Prec@1_up 98.438 (98.733)	Training Prec@1_down 96.875 (96.853)	
2021-07-30 01:34:41,343: ============================================================
2021-07-30 01:36:54,282: time cost, forward:0.02519241376501302, backward:0.1796387337206203, data cost:0.9976233232008072 
2021-07-30 01:36:54,283: ============================================================
2021-07-30 01:36:54,283: Epoch 46/65 Batch 900/964 eta: 6:47:15.609425	Training Loss2 1.7905 (1.9312)	Training Loss3 2.8083 (2.9616)	Training Loss4 2.3767 (2.5649)	Training Total_Loss 4.8919 (5.2097)	Training Prec@1_up 98.047 (98.744)	Training Prec@1_down 97.656 (96.870)	
2021-07-30 01:36:54,283: ============================================================
2021-07-30 01:38:21,751: Epoch: 46/65 eta: 6:45:49.198750	Training Loss2 2.0094 (1.9332)	Training Loss3 3.0034 (2.9636)	Training Loss4 2.6386 (2.5667)	Training Total_Loss 5.3274 (5.2135)	Training Prec@1_up 98.828 (98.740)	Training Prec@1_down 96.875 (96.861)	
2021-07-30 01:38:21,751: ============================================================
2021-07-30 01:38:21,754: Save Checkpoint...
2021-07-30 01:38:21,755: ============================================================
2021-07-30 01:38:22,384: Save done!
2021-07-30 01:38:22,384: ============================================================
2021-07-30 01:40:35,319: time cost, forward:0.025977389981048277, backward:0.18009124380169492, data cost:1.004587532293917 
2021-07-30 01:40:35,320: ============================================================
2021-07-30 01:40:35,320: Epoch 47/65 Batch 100/964 eta: 6:43:36.596077	Training Loss2 2.0259 (1.9359)	Training Loss3 3.0298 (2.9658)	Training Loss4 2.6722 (2.5706)	Training Total_Loss 5.3789 (5.2190)	Training Prec@1_up 98.242 (98.712)	Training Prec@1_down 96.875 (96.798)	
2021-07-30 01:40:35,320: ============================================================
2021-07-30 01:42:46,714: time cost, forward:0.02711962934714466, backward:0.1802876858255971, data cost:0.9908404829514087 
2021-07-30 01:42:46,714: ============================================================
2021-07-30 01:42:46,714: Epoch 47/65 Batch 200/964 eta: 6:36:44.722908	Training Loss2 2.1021 (1.9380)	Training Loss3 3.0897 (2.9709)	Training Loss4 2.7980 (2.5717)	Training Total_Loss 5.5397 (5.2257)	Training Prec@1_up 99.023 (98.709)	Training Prec@1_down 96.484 (96.788)	
2021-07-30 01:42:46,714: ============================================================
2021-07-30 01:44:59,594: time cost, forward:0.027303829639651705, backward:0.1803718211260129, data cost:0.9913080408421647 
2021-07-30 01:44:59,595: ============================================================
2021-07-30 01:44:59,595: Epoch 47/65 Batch 300/964 eta: 6:39:01.183352	Training Loss2 1.9791 (1.9372)	Training Loss3 2.9293 (2.9677)	Training Loss4 2.6048 (2.5709)	Training Total_Loss 5.2212 (5.2218)	Training Prec@1_up 98.633 (98.707)	Training Prec@1_down 96.875 (96.807)	
2021-07-30 01:44:59,596: ============================================================
2021-07-30 01:47:10,152: time cost, forward:0.027240754368908722, backward:0.18039799991406893, data cost:0.9857122874200195 
2021-07-30 01:47:10,153: ============================================================
2021-07-30 01:47:10,153: Epoch 47/65 Batch 400/964 eta: 6:29:52.002975	Training Loss2 1.9284 (1.9298)	Training Loss3 2.8381 (2.9595)	Training Loss4 2.5725 (2.5625)	Training Total_Loss 5.0885 (5.2056)	Training Prec@1_up 98.438 (98.735)	Training Prec@1_down 96.484 (96.828)	
2021-07-30 01:47:10,153: ============================================================
2021-07-30 01:49:23,379: time cost, forward:0.02685633355486608, backward:0.18027403932773994, data cost:0.9877154769782791 
2021-07-30 01:49:23,379: ============================================================
2021-07-30 01:49:23,379: Epoch 47/65 Batch 500/964 eta: 6:35:36.955614	Training Loss2 1.9046 (1.9299)	Training Loss3 2.8221 (2.9598)	Training Loss4 2.4996 (2.5616)	Training Total_Loss 5.0242 (5.2055)	Training Prec@1_up 99.023 (98.731)	Training Prec@1_down 97.070 (96.841)	
2021-07-30 01:49:23,379: ============================================================
2021-07-30 01:51:35,855: time cost, forward:0.026568313273842226, backward:0.18020646918397118, data cost:0.9877505358153074 
2021-07-30 01:51:35,855: ============================================================
2021-07-30 01:51:35,855: Epoch 47/65 Batch 600/964 eta: 6:31:10.786999	Training Loss2 1.7930 (1.9279)	Training Loss3 2.8593 (2.9569)	Training Loss4 2.4262 (2.5595)	Training Total_Loss 4.9689 (5.2006)	Training Prec@1_up 99.414 (98.742)	Training Prec@1_down 98.242 (96.870)	
2021-07-30 01:51:35,856: ============================================================
2021-07-30 01:53:49,759: time cost, forward:0.026192451580740694, backward:0.18005032055026643, data cost:0.9899961177542145 
2021-07-30 01:53:49,759: ============================================================
2021-07-30 01:53:49,760: Epoch 47/65 Batch 700/964 eta: 6:33:09.888311	Training Loss2 1.8549 (1.9321)	Training Loss3 2.8787 (2.9617)	Training Loss4 2.5118 (2.5652)	Training Total_Loss 5.0621 (5.2104)	Training Prec@1_up 98.828 (98.736)	Training Prec@1_down 97.461 (96.866)	
2021-07-30 01:53:49,760: ============================================================
2021-07-30 01:56:01,243: time cost, forward:0.026065984566011776, backward:0.18008385193959645, data cost:0.9885649943679982 
2021-07-30 01:56:01,244: ============================================================
2021-07-30 01:56:01,244: Epoch 47/65 Batch 800/964 eta: 6:23:52.103310	Training Loss2 1.9574 (1.9327)	Training Loss3 3.0193 (2.9631)	Training Loss4 2.6423 (2.5654)	Training Total_Loss 5.3192 (5.2122)	Training Prec@1_up 98.633 (98.740)	Training Prec@1_down 96.484 (96.855)	
2021-07-30 01:56:01,244: ============================================================
2021-07-30 01:58:15,023: time cost, forward:0.026021937772349867, backward:0.18012943230693676, data cost:0.9899143822598908 
2021-07-30 01:58:15,023: ============================================================
2021-07-30 01:58:15,023: Epoch 47/65 Batch 900/964 eta: 6:28:20.374354	Training Loss2 1.9340 (1.9327)	Training Loss3 2.9490 (2.9624)	Training Loss4 2.5710 (2.5659)	Training Total_Loss 5.2015 (5.2117)	Training Prec@1_up 98.047 (98.739)	Training Prec@1_down 96.289 (96.869)	
2021-07-30 01:58:15,023: ============================================================
2021-07-30 01:59:42,762: Epoch: 47/65 eta: 6:26:53.417683	Training Loss2 2.2427 (1.9325)	Training Loss3 3.3541 (2.9626)	Training Loss4 2.8739 (2.5656)	Training Total_Loss 5.9123 (5.2116)	Training Prec@1_up 97.656 (98.740)	Training Prec@1_down 94.727 (96.864)	
2021-07-30 01:59:42,762: ============================================================
2021-07-30 01:59:42,765: Save Checkpoint...
2021-07-30 01:59:42,766: ============================================================
2021-07-30 01:59:43,416: Save done!
2021-07-30 01:59:43,416: ============================================================
2021-07-30 02:01:59,696: time cost, forward:0.023765961329142254, backward:0.17884154994078358, data cost:1.0348296309962417 
2021-07-30 02:01:59,696: ============================================================
2021-07-30 02:01:59,696: Epoch 48/65 Batch 100/964 eta: 6:31:52.128467	Training Loss2 1.9163 (1.9166)	Training Loss3 2.8905 (2.9338)	Training Loss4 2.4999 (2.5521)	Training Total_Loss 5.0986 (5.1681)	Training Prec@1_up 98.633 (98.812)	Training Prec@1_down 97.070 (96.938)	
2021-07-30 02:01:59,697: ============================================================
2021-07-30 02:04:14,018: time cost, forward:0.02357526280772147, backward:0.17845553129761663, data cost:1.0214491669257082 
2021-07-30 02:04:14,018: ============================================================
2021-07-30 02:04:14,018: Epoch 48/65 Batch 200/964 eta: 6:24:00.235120	Training Loss2 2.1174 (1.9245)	Training Loss3 3.1603 (2.9456)	Training Loss4 2.8221 (2.5592)	Training Total_Loss 5.6300 (5.1874)	Training Prec@1_up 98.633 (98.775)	Training Prec@1_down 96.875 (96.855)	
2021-07-30 02:04:14,018: ============================================================
2021-07-30 02:06:29,030: time cost, forward:0.023526509071273547, backward:0.17833618176820684, data cost:1.0200158911803894 
2021-07-30 02:06:29,030: ============================================================
2021-07-30 02:06:29,031: Epoch 48/65 Batch 300/964 eta: 6:23:43.626867	Training Loss2 1.8434 (1.9267)	Training Loss3 2.7966 (2.9492)	Training Loss4 2.4281 (2.5588)	Training Total_Loss 4.9323 (5.1919)	Training Prec@1_up 99.023 (98.756)	Training Prec@1_down 97.266 (96.853)	
2021-07-30 02:06:29,031: ============================================================
2021-07-30 02:08:42,836: time cost, forward:0.023698562966253525, backward:0.17859251397594175, data cost:1.01669866996899 
2021-07-30 02:08:42,836: ============================================================
2021-07-30 02:08:42,836: Epoch 48/65 Batch 400/964 eta: 6:18:04.112546	Training Loss2 1.9404 (1.9310)	Training Loss3 2.9055 (2.9571)	Training Loss4 2.5906 (2.5650)	Training Total_Loss 5.1711 (5.2051)	Training Prec@1_up 99.023 (98.754)	Training Prec@1_down 97.266 (96.848)	
2021-07-30 02:08:42,837: ============================================================
2021-07-30 02:10:59,493: time cost, forward:0.023801706119147475, backward:0.17857466527598653, data cost:1.0198266453637868 
2021-07-30 02:10:59,493: ============================================================
2021-07-30 02:10:59,493: Epoch 48/65 Batch 500/964 eta: 6:23:50.755401	Training Loss2 1.9280 (1.9310)	Training Loss3 2.9107 (2.9573)	Training Loss4 2.6059 (2.5652)	Training Total_Loss 5.1777 (5.2054)	Training Prec@1_up 98.633 (98.751)	Training Prec@1_down 97.070 (96.867)	
2021-07-30 02:10:59,493: ============================================================
2021-07-30 02:13:16,464: time cost, forward:0.023821626561312922, backward:0.1786321053321851, data cost:1.0224069840521963 
2021-07-30 02:13:16,465: ============================================================
2021-07-30 02:13:16,465: Epoch 48/65 Batch 600/964 eta: 6:22:26.856711	Training Loss2 1.8277 (1.9301)	Training Loss3 2.8852 (2.9578)	Training Loss4 2.4359 (2.5638)	Training Total_Loss 5.0170 (5.2047)	Training Prec@1_up 99.023 (98.746)	Training Prec@1_down 98.047 (96.853)	
2021-07-30 02:13:16,465: ============================================================
2021-07-30 02:15:34,642: time cost, forward:0.023833936205578804, backward:0.1787702017417111, data cost:1.0256707832025356 
2021-07-30 02:15:34,642: ============================================================
2021-07-30 02:15:34,642: Epoch 48/65 Batch 700/964 eta: 6:23:30.716110	Training Loss2 1.7829 (1.9326)	Training Loss3 2.8546 (2.9612)	Training Loss4 2.4669 (2.5656)	Training Total_Loss 4.9795 (5.2103)	Training Prec@1_up 99.219 (98.735)	Training Prec@1_down 96.875 (96.849)	
2021-07-30 02:15:34,642: ============================================================
2021-07-30 02:17:51,531: time cost, forward:0.023816621945110222, backward:0.17879714685327866, data cost:1.026771079315262 
2021-07-30 02:17:51,531: ============================================================
2021-07-30 02:17:51,531: Epoch 48/65 Batch 800/964 eta: 6:17:39.251236	Training Loss2 2.0819 (1.9322)	Training Loss3 3.1162 (2.9609)	Training Loss4 2.8019 (2.5651)	Training Total_Loss 5.5581 (5.2095)	Training Prec@1_up 97.656 (98.732)	Training Prec@1_down 96.289 (96.853)	
2021-07-30 02:17:51,532: ============================================================
2021-07-30 02:20:09,238: time cost, forward:0.023854714214337682, backward:0.1788438538688176, data cost:1.0284784062950973 
2021-07-30 02:20:09,238: ============================================================
2021-07-30 02:20:09,238: Epoch 48/65 Batch 900/964 eta: 6:17:36.934949	Training Loss2 1.8617 (1.9336)	Training Loss3 2.7975 (2.9640)	Training Loss4 2.5048 (2.5673)	Training Total_Loss 4.9807 (5.2144)	Training Prec@1_up 98.828 (98.734)	Training Prec@1_down 97.461 (96.847)	
2021-07-30 02:20:09,239: ============================================================
2021-07-30 02:21:40,198: Epoch: 48/65 eta: 6:16:07.425390	Training Loss2 1.8390 (1.9321)	Training Loss3 2.9104 (2.9618)	Training Loss4 2.3972 (2.5653)	Training Total_Loss 5.0285 (5.2105)	Training Prec@1_up 99.219 (98.741)	Training Prec@1_down 97.070 (96.866)	
2021-07-30 02:21:40,199: ============================================================
2021-07-30 02:21:40,201: Save Checkpoint...
2021-07-30 02:21:40,204: ============================================================
2021-07-30 02:21:40,899: Save done!
2021-07-30 02:21:40,900: ============================================================
2021-07-30 02:23:56,315: time cost, forward:0.024763073584046027, backward:0.17900289670385497, data cost:1.0305550242915298 
2021-07-30 02:23:56,315: ============================================================
2021-07-30 02:23:56,315: Epoch 49/65 Batch 100/964 eta: 6:07:37.609340	Training Loss2 1.9258 (1.9543)	Training Loss3 3.0812 (2.9910)	Training Loss4 2.5753 (2.5882)	Training Total_Loss 5.3317 (5.2622)	Training Prec@1_up 98.828 (98.643)	Training Prec@1_down 95.898 (96.701)	
2021-07-30 02:23:56,315: ============================================================
2021-07-30 02:26:12,337: time cost, forward:0.024773776231698654, backward:0.1791707451020054, data cost:1.0276765392054266 
2021-07-30 02:26:12,338: ============================================================
2021-07-30 02:26:12,338: Epoch 49/65 Batch 200/964 eta: 6:07:00.673384	Training Loss2 2.0561 (1.9388)	Training Loss3 3.1937 (2.9692)	Training Loss4 2.6553 (2.5718)	Training Total_Loss 5.5494 (5.2245)	Training Prec@1_up 98.828 (98.717)	Training Prec@1_down 95.312 (96.796)	
2021-07-30 02:26:12,338: ============================================================
2021-07-30 02:28:27,678: time cost, forward:0.024397785448310366, backward:0.17905857012822077, data cost:1.0246680061952724 
2021-07-30 02:28:27,678: ============================================================
2021-07-30 02:28:27,678: Epoch 49/65 Batch 300/964 eta: 6:02:54.937322	Training Loss2 1.8459 (1.9388)	Training Loss3 2.8545 (2.9710)	Training Loss4 2.4142 (2.5725)	Training Total_Loss 4.9846 (5.2267)	Training Prec@1_up 98.438 (98.705)	Training Prec@1_down 97.266 (96.821)	
2021-07-30 02:28:27,678: ============================================================
2021-07-30 02:30:45,000: time cost, forward:0.02423656673957232, backward:0.17899000077020555, data cost:1.0281942111806464 
2021-07-30 02:30:45,001: ============================================================
2021-07-30 02:30:45,001: Epoch 49/65 Batch 400/964 eta: 6:05:56.474815	Training Loss2 1.9040 (1.9410)	Training Loss3 2.8788 (2.9734)	Training Loss4 2.5298 (2.5748)	Training Total_Loss 5.0957 (5.2313)	Training Prec@1_up 98.828 (98.716)	Training Prec@1_down 97.461 (96.828)	
2021-07-30 02:30:45,001: ============================================================
2021-07-30 02:33:03,225: time cost, forward:0.024221250671661927, backward:0.1792560306961885, data cost:1.0317278897356175 
2021-07-30 02:33:03,225: ============================================================
2021-07-30 02:33:03,226: Epoch 49/65 Batch 500/964 eta: 6:06:02.546180	Training Loss2 1.9860 (1.9358)	Training Loss3 2.8787 (2.9647)	Training Loss4 2.6669 (2.5696)	Training Total_Loss 5.2051 (5.2174)	Training Prec@1_up 98.438 (98.725)	Training Prec@1_down 96.875 (96.854)	
2021-07-30 02:33:03,226: ============================================================
2021-07-30 02:35:19,889: time cost, forward:0.024203174301300305, backward:0.1793523869649795, data cost:1.0315346212339322 
2021-07-30 02:35:19,890: ============================================================
2021-07-30 02:35:19,890: Epoch 49/65 Batch 600/964 eta: 5:59:37.900385	Training Loss2 1.7708 (1.9359)	Training Loss3 2.8231 (2.9656)	Training Loss4 2.4140 (2.5704)	Training Total_Loss 4.9155 (5.2188)	Training Prec@1_up 98.828 (98.731)	Training Prec@1_down 97.852 (96.856)	
2021-07-30 02:35:19,890: ============================================================
2021-07-30 02:37:38,121: time cost, forward:0.024174190215627864, backward:0.17925190175210629, data cost:1.0339336255418725 
2021-07-30 02:37:38,121: ============================================================
2021-07-30 02:37:38,121: Epoch 49/65 Batch 700/964 eta: 6:01:27.167701	Training Loss2 1.9420 (1.9354)	Training Loss3 2.9597 (2.9664)	Training Loss4 2.5768 (2.5703)	Training Total_Loss 5.2191 (5.2192)	Training Prec@1_up 99.414 (98.738)	Training Prec@1_down 97.070 (96.858)	
2021-07-30 02:37:38,121: ============================================================
2021-07-30 02:39:57,622: time cost, forward:0.02421982356991726, backward:0.1792656697976276, data cost:1.0371742475316283 
2021-07-30 02:39:57,622: ============================================================
2021-07-30 02:39:57,622: Epoch 49/65 Batch 800/964 eta: 6:02:26.795306	Training Loss2 1.8582 (1.9316)	Training Loss3 2.7746 (2.9626)	Training Loss4 2.4841 (2.5647)	Training Total_Loss 4.9457 (5.2107)	Training Prec@1_up 99.023 (98.742)	Training Prec@1_down 98.242 (96.861)	
2021-07-30 02:39:57,622: ============================================================
2021-07-30 02:42:14,860: time cost, forward:0.024450226010416982, backward:0.17929487923228568, data cost:1.0371045660521747 
2021-07-30 02:42:14,860: ============================================================
2021-07-30 02:42:14,860: Epoch 49/65 Batch 900/964 eta: 5:54:16.757211	Training Loss2 1.9043 (1.9328)	Training Loss3 2.8920 (2.9631)	Training Loss4 2.5266 (2.5663)	Training Total_Loss 5.1074 (5.2126)	Training Prec@1_up 98.633 (98.741)	Training Prec@1_down 97.070 (96.860)	
2021-07-30 02:42:14,860: ============================================================
2021-07-30 02:43:44,547: Epoch: 49/65 eta: 5:52:47.552664	Training Loss2 1.9397 (1.9317)	Training Loss3 3.0046 (2.9613)	Training Loss4 2.5675 (2.5650)	Training Total_Loss 5.2582 (5.2096)	Training Prec@1_up 99.609 (98.739)	Training Prec@1_down 96.680 (96.866)	
2021-07-30 02:43:44,548: ============================================================
2021-07-30 02:43:44,550: Save Checkpoint...
2021-07-30 02:43:44,551: ============================================================
2021-07-30 02:43:45,202: Save done!
2021-07-30 02:43:45,202: ============================================================
2021-07-30 02:46:02,222: time cost, forward:0.023746138871318163, backward:0.17751509733874388, data cost:1.0417721343762947 
2021-07-30 02:46:02,223: ============================================================
2021-07-30 02:46:02,223: Epoch 50/65 Batch 100/964 eta: 5:49:58.126828	Training Loss2 1.8361 (1.9251)	Training Loss3 2.8682 (2.9548)	Training Loss4 2.5131 (2.5637)	Training Total_Loss 5.0428 (5.1992)	Training Prec@1_up 98.828 (98.724)	Training Prec@1_down 97.461 (96.934)	
2021-07-30 02:46:02,223: ============================================================
2021-07-30 02:48:20,413: time cost, forward:0.02318159299879218, backward:0.17794666098589873, data cost:1.0385310134696002 
2021-07-30 02:48:20,414: ============================================================
2021-07-30 02:48:20,414: Epoch 50/65 Batch 200/964 eta: 5:50:39.545772	Training Loss2 2.0471 (1.9307)	Training Loss3 2.9958 (2.9595)	Training Loss4 2.6912 (2.5659)	Training Total_Loss 5.3649 (5.2079)	Training Prec@1_up 98.438 (98.716)	Training Prec@1_down 97.070 (96.894)	
2021-07-30 02:48:20,414: ============================================================
2021-07-30 02:50:38,759: time cost, forward:0.022494468401905686, backward:0.1781991405231897, data cost:1.041767928911292 
2021-07-30 02:50:38,759: ============================================================
2021-07-30 02:50:38,759: Epoch 50/65 Batch 300/964 eta: 5:48:44.731920	Training Loss2 1.8743 (1.9315)	Training Loss3 2.9376 (2.9607)	Training Loss4 2.5145 (2.5674)	Training Total_Loss 5.1320 (5.2102)	Training Prec@1_up 99.023 (98.740)	Training Prec@1_down 97.070 (96.880)	
2021-07-30 02:50:38,759: ============================================================
2021-07-30 02:52:56,336: time cost, forward:0.02216430355731706, backward:0.17861681892758324, data cost:1.0403133334970116 
2021-07-30 02:52:56,336: ============================================================
2021-07-30 02:52:56,336: Epoch 50/65 Batch 400/964 eta: 5:44:30.974469	Training Loss2 1.9314 (1.9295)	Training Loss3 3.0144 (2.9560)	Training Loss4 2.5872 (2.5647)	Training Total_Loss 5.2737 (5.2031)	Training Prec@1_up 99.023 (98.725)	Training Prec@1_down 96.875 (96.900)	
2021-07-30 02:52:56,336: ============================================================
2021-07-30 02:55:15,072: time cost, forward:0.0219019735026694, backward:0.1785495897572122, data cost:1.0430282519193355 
2021-07-30 02:55:15,072: ============================================================
2021-07-30 02:55:15,072: Epoch 50/65 Batch 500/964 eta: 5:45:06.332139	Training Loss2 1.9601 (1.9310)	Training Loss3 3.1213 (2.9581)	Training Loss4 2.5906 (2.5651)	Training Total_Loss 5.3966 (5.2061)	Training Prec@1_up 98.047 (98.739)	Training Prec@1_down 95.898 (96.895)	
2021-07-30 02:55:15,072: ============================================================
2021-07-30 02:57:32,380: time cost, forward:0.021820177020931086, backward:0.1786335724622061, data cost:1.0421961002636433 
2021-07-30 02:57:32,380: ============================================================
2021-07-30 02:57:32,380: Epoch 50/65 Batch 600/964 eta: 5:39:15.931337	Training Loss2 2.0631 (1.9289)	Training Loss3 3.1136 (2.9567)	Training Loss4 2.7252 (2.5618)	Training Total_Loss 5.5078 (5.2020)	Training Prec@1_up 98.828 (98.747)	Training Prec@1_down 96.680 (96.886)	
2021-07-30 02:57:32,380: ============================================================
2021-07-30 02:59:50,408: time cost, forward:0.02177940520094188, backward:0.17871310714317834, data cost:1.042031573294229 
2021-07-30 02:59:50,409: ============================================================
2021-07-30 02:59:50,409: Epoch 50/65 Batch 700/964 eta: 5:38:44.680172	Training Loss2 1.7901 (1.9296)	Training Loss3 2.7862 (2.9572)	Training Loss4 2.4399 (2.5635)	Training Total_Loss 4.9012 (5.2037)	Training Prec@1_up 99.023 (98.745)	Training Prec@1_down 98.633 (96.887)	
2021-07-30 02:59:50,409: ============================================================
2021-07-30 03:02:06,120: time cost, forward:0.021674613033576363, backward:0.17867025177231122, data cost:1.0396502880339926 
2021-07-30 03:02:06,121: ============================================================
2021-07-30 03:02:06,121: Epoch 50/65 Batch 800/964 eta: 5:30:47.910878	Training Loss2 1.9991 (1.9298)	Training Loss3 2.9832 (2.9574)	Training Loss4 2.6256 (2.5629)	Training Total_Loss 5.2955 (5.2038)	Training Prec@1_up 98.438 (98.743)	Training Prec@1_down 96.680 (96.881)	
2021-07-30 03:02:06,121: ============================================================
2021-07-30 03:04:24,348: time cost, forward:0.02158122174068871, backward:0.17873220210345886, data cost:1.0403918570220405 
2021-07-30 03:04:24,349: ============================================================
2021-07-30 03:04:24,349: Epoch 50/65 Batch 900/964 eta: 5:34:37.621249	Training Loss2 1.9311 (1.9285)	Training Loss3 3.0104 (2.9567)	Training Loss4 2.5528 (2.5614)	Training Total_Loss 5.2523 (5.2017)	Training Prec@1_up 98.242 (98.748)	Training Prec@1_down 95.898 (96.880)	
2021-07-30 03:04:24,349: ============================================================
2021-07-30 03:05:54,468: Epoch: 50/65 eta: 5:33:07.773030	Training Loss2 1.9146 (1.9300)	Training Loss3 2.7422 (2.9593)	Training Loss4 2.5960 (2.5626)	Training Total_Loss 4.9974 (5.2056)	Training Prec@1_up 99.414 (98.743)	Training Prec@1_down 98.633 (96.869)	
2021-07-30 03:05:54,468: ============================================================
2021-07-30 03:05:54,471: Save Checkpoint...
2021-07-30 03:05:54,472: ============================================================
2021-07-30 03:05:55,103: Save done!
2021-07-30 03:05:55,103: ============================================================
2021-07-30 03:08:10,661: time cost, forward:0.02353261937998762, backward:0.1781470101289075, data cost:1.0287175226693202 
2021-07-30 03:08:10,662: ============================================================
2021-07-30 03:08:10,662: Epoch 51/65 Batch 100/964 eta: 5:24:27.388591	Training Loss2 1.8660 (1.9391)	Training Loss3 2.7653 (2.9791)	Training Loss4 2.5094 (2.5687)	Training Total_Loss 4.9530 (5.2330)	Training Prec@1_up 98.633 (98.690)	Training Prec@1_down 97.266 (96.765)	
2021-07-30 03:08:10,662: ============================================================
2021-07-30 03:10:25,482: time cost, forward:0.02352448204653946, backward:0.17813757076934356, data cost:1.0200064721418984 
2021-07-30 03:10:25,482: ============================================================
2021-07-30 03:10:25,483: Epoch 51/65 Batch 200/964 eta: 5:20:26.764479	Training Loss2 1.6911 (1.9333)	Training Loss3 2.7193 (2.9710)	Training Loss4 2.3181 (2.5660)	Training Total_Loss 4.7239 (5.2206)	Training Prec@1_up 99.219 (98.762)	Training Prec@1_down 98.633 (96.826)	
2021-07-30 03:10:25,483: ============================================================
2021-07-30 03:12:38,156: time cost, forward:0.02329783216367996, backward:0.17824872201900419, data cost:1.0126973665677583 
2021-07-30 03:12:38,156: ============================================================
2021-07-30 03:12:38,156: Epoch 51/65 Batch 300/964 eta: 5:13:07.926013	Training Loss2 1.9464 (1.9310)	Training Loss3 2.9223 (2.9605)	Training Loss4 2.6449 (2.5641)	Training Total_Loss 5.2180 (5.2080)	Training Prec@1_up 99.414 (98.748)	Training Prec@1_down 97.070 (96.883)	
2021-07-30 03:12:38,156: ============================================================
2021-07-30 03:14:52,846: time cost, forward:0.022895796257152892, backward:0.17781065101910354, data cost:1.0108239483415036 
2021-07-30 03:14:52,846: ============================================================
2021-07-30 03:14:52,847: Epoch 51/65 Batch 400/964 eta: 5:15:38.810591	Training Loss2 1.7665 (1.9287)	Training Loss3 2.7871 (2.9598)	Training Loss4 2.3589 (2.5601)	Training Total_Loss 4.8498 (5.2042)	Training Prec@1_up 99.023 (98.739)	Training Prec@1_down 96.875 (96.869)	
2021-07-30 03:14:52,847: ============================================================
2021-07-30 03:17:08,695: time cost, forward:0.022506203106744494, backward:0.17791806050914083, data cost:1.0115312555271065 
2021-07-30 03:17:08,696: ============================================================
2021-07-30 03:17:08,696: Epoch 51/65 Batch 500/964 eta: 5:16:05.953613	Training Loss2 1.9053 (1.9266)	Training Loss3 2.8007 (2.9560)	Training Loss4 2.5377 (2.5596)	Training Total_Loss 5.0222 (5.1990)	Training Prec@1_up 98.633 (98.742)	Training Prec@1_down 97.070 (96.888)	
2021-07-30 03:17:08,696: ============================================================
2021-07-30 03:19:23,573: time cost, forward:0.02219922514710084, backward:0.17798900843064655, data cost:1.0127323354424937 
2021-07-30 03:19:23,573: ============================================================
2021-07-30 03:19:23,573: Epoch 51/65 Batch 600/964 eta: 5:11:35.326396	Training Loss2 1.7107 (1.9285)	Training Loss3 2.6322 (2.9573)	Training Loss4 2.3874 (2.5618)	Training Total_Loss 4.6813 (5.2025)	Training Prec@1_up 99.805 (98.739)	Training Prec@1_down 98.438 (96.880)	
2021-07-30 03:19:23,573: ============================================================
2021-07-30 03:21:37,293: time cost, forward:0.021969185707736254, backward:0.17789079430106713, data cost:1.0127874440560185 
2021-07-30 03:21:37,293: ============================================================
2021-07-30 03:21:37,293: Epoch 51/65 Batch 700/964 eta: 5:06:41.185286	Training Loss2 1.9321 (1.9293)	Training Loss3 2.8621 (2.9571)	Training Loss4 2.5591 (2.5626)	Training Total_Loss 5.1077 (5.2031)	Training Prec@1_up 98.633 (98.737)	Training Prec@1_down 97.461 (96.872)	
2021-07-30 03:21:37,293: ============================================================
2021-07-30 03:23:52,304: time cost, forward:0.022144947839767972, backward:0.1779962006737204, data cost:1.0127100860968101 
2021-07-30 03:23:52,305: ============================================================
2021-07-30 03:23:52,305: Epoch 51/65 Batch 800/964 eta: 5:07:23.943147	Training Loss2 2.1653 (1.9297)	Training Loss3 3.1962 (2.9583)	Training Loss4 2.7723 (2.5633)	Training Total_Loss 5.6650 (5.2048)	Training Prec@1_up 97.852 (98.742)	Training Prec@1_down 95.703 (96.874)	
2021-07-30 03:23:52,305: ============================================================
2021-07-30 03:26:07,863: time cost, forward:0.02222885809697882, backward:0.1780174107387148, data cost:1.0136839478909638 
2021-07-30 03:26:07,864: ============================================================
2021-07-30 03:26:07,864: Epoch 51/65 Batch 900/964 eta: 5:06:23.135009	Training Loss2 1.7149 (1.9297)	Training Loss3 2.8186 (2.9584)	Training Loss4 2.3235 (2.5634)	Training Total_Loss 4.8378 (5.2050)	Training Prec@1_up 99.023 (98.747)	Training Prec@1_down 97.070 (96.872)	
2021-07-30 03:26:07,864: ============================================================
2021-07-30 03:27:35,762: Epoch: 51/65 eta: 5:04:55.021759	Training Loss2 1.9465 (1.9300)	Training Loss3 3.0709 (2.9592)	Training Loss4 2.5695 (2.5639)	Training Total_Loss 5.3289 (5.2061)	Training Prec@1_up 98.438 (98.743)	Training Prec@1_down 96.484 (96.870)	
2021-07-30 03:27:35,763: ============================================================
2021-07-30 03:27:35,765: Save Checkpoint...
2021-07-30 03:27:35,766: ============================================================
2021-07-30 03:27:36,501: Save done!
2021-07-30 03:27:36,501: ============================================================
2021-07-30 03:29:51,562: time cost, forward:0.022950001437254626, backward:0.17710037905760487, data cost:1.0230803224775527 
2021-07-30 03:29:51,563: ============================================================
2021-07-30 03:29:51,563: Epoch 52/65 Batch 100/964 eta: 5:01:33.955836	Training Loss2 2.1400 (1.9130)	Training Loss3 3.1114 (2.9309)	Training Loss4 2.7804 (2.5400)	Training Total_Loss 5.5716 (5.1574)	Training Prec@1_up 97.852 (98.749)	Training Prec@1_down 95.703 (96.912)	
2021-07-30 03:29:51,563: ============================================================
2021-07-30 03:32:06,810: time cost, forward:0.023380474828595493, backward:0.17776911342563342, data cost:1.019538087461462 
2021-07-30 03:32:06,811: ============================================================
2021-07-30 03:32:06,811: Epoch 52/65 Batch 200/964 eta: 4:59:43.945094	Training Loss2 1.9605 (1.9190)	Training Loss3 3.0048 (2.9386)	Training Loss4 2.5724 (2.5500)	Training Total_Loss 5.2712 (5.1731)	Training Prec@1_up 98.633 (98.778)	Training Prec@1_down 96.289 (96.919)	
2021-07-30 03:32:06,811: ============================================================
2021-07-30 03:34:23,919: time cost, forward:0.023130115458000464, backward:0.17789090357496587, data cost:1.0217935488774226 
2021-07-30 03:34:23,919: ============================================================
2021-07-30 03:34:23,920: Epoch 52/65 Batch 300/964 eta: 5:01:34.260567	Training Loss2 1.9683 (1.9217)	Training Loss3 2.9564 (2.9474)	Training Loss4 2.6352 (2.5530)	Training Total_Loss 5.2582 (5.1848)	Training Prec@1_up 98.828 (98.769)	Training Prec@1_down 97.852 (96.890)	
2021-07-30 03:34:23,920: ============================================================
2021-07-30 03:36:38,839: time cost, forward:0.0227541212449994, backward:0.1780422964789216, data cost:1.0196836173982549 
2021-07-30 03:36:38,840: ============================================================
2021-07-30 03:36:38,840: Epoch 52/65 Batch 400/964 eta: 4:54:30.480248	Training Loss2 1.9376 (1.9229)	Training Loss3 2.7919 (2.9489)	Training Loss4 2.5570 (2.5549)	Training Total_Loss 5.0392 (5.1879)	Training Prec@1_up 98.633 (98.774)	Training Prec@1_down 97.266 (96.871)	
2021-07-30 03:36:38,840: ============================================================
2021-07-30 03:38:53,141: time cost, forward:0.022774341350089093, backward:0.17816936420295426, data cost:1.0172871124290512 
2021-07-30 03:38:53,141: ============================================================
2021-07-30 03:38:53,142: Epoch 52/65 Batch 500/964 eta: 4:50:55.190848	Training Loss2 1.8432 (1.9272)	Training Loss3 2.7657 (2.9538)	Training Loss4 2.4502 (2.5605)	Training Total_Loss 4.9124 (5.1977)	Training Prec@1_up 99.219 (98.776)	Training Prec@1_down 97.656 (96.855)	
2021-07-30 03:38:53,142: ============================================================
2021-07-30 03:41:07,999: time cost, forward:0.022718434341761823, backward:0.1781501049589434, data cost:1.0177151894131566 
2021-07-30 03:41:08,000: ============================================================
2021-07-30 03:41:08,000: Epoch 52/65 Batch 600/964 eta: 4:49:52.682173	Training Loss2 2.0020 (1.9286)	Training Loss3 2.8927 (2.9551)	Training Loss4 2.6789 (2.5631)	Training Total_Loss 5.2331 (5.2009)	Training Prec@1_up 98.633 (98.772)	Training Prec@1_down 96.875 (96.865)	
2021-07-30 03:41:08,000: ============================================================
2021-07-30 03:43:23,655: time cost, forward:0.022555901427807896, backward:0.1781217577801924, data cost:1.0187324914809461 
2021-07-30 03:43:23,656: ============================================================
2021-07-30 03:43:23,656: Epoch 52/65 Batch 700/964 eta: 4:49:19.924363	Training Loss2 1.8024 (1.9287)	Training Loss3 2.8312 (2.9564)	Training Loss4 2.4194 (2.5634)	Training Total_Loss 4.9420 (5.2024)	Training Prec@1_up 99.023 (98.766)	Training Prec@1_down 98.242 (96.885)	
2021-07-30 03:43:23,656: ============================================================
2021-07-30 03:45:37,983: time cost, forward:0.02279395215651568, backward:0.178215142931598, data cost:1.0179169777785433 
2021-07-30 03:45:37,983: ============================================================
2021-07-30 03:45:37,983: Epoch 52/65 Batch 800/964 eta: 4:44:15.533385	Training Loss2 1.9154 (1.9298)	Training Loss3 2.9933 (2.9576)	Training Loss4 2.5467 (2.5641)	Training Total_Loss 5.2244 (5.2045)	Training Prec@1_up 98.047 (98.754)	Training Prec@1_down 96.484 (96.884)	
2021-07-30 03:45:37,984: ============================================================
2021-07-30 03:47:53,427: time cost, forward:0.023019164767493397, backward:0.17834060078070346, data cost:1.0179545338347438 
2021-07-30 03:47:53,428: ============================================================
2021-07-30 03:47:53,428: Epoch 52/65 Batch 900/964 eta: 4:44:21.943421	Training Loss2 1.7599 (1.9298)	Training Loss3 2.6955 (2.9584)	Training Loss4 2.3531 (2.5642)	Training Total_Loss 4.7520 (5.2053)	Training Prec@1_up 99.219 (98.750)	Training Prec@1_down 97.656 (96.884)	
2021-07-30 03:47:53,428: ============================================================
2021-07-30 03:49:20,269: Epoch: 52/65 eta: 4:42:53.904497	Training Loss2 2.1030 (1.9299)	Training Loss3 3.0924 (2.9590)	Training Loss4 2.6872 (2.5636)	Training Total_Loss 5.4875 (5.2057)	Training Prec@1_up 97.852 (98.743)	Training Prec@1_down 96.094 (96.871)	
2021-07-30 03:49:20,270: ============================================================
2021-07-30 03:49:20,272: Save Checkpoint...
2021-07-30 03:49:20,275: ============================================================
2021-07-30 03:49:21,005: Save done!
2021-07-30 03:49:21,005: ============================================================
2021-07-30 03:51:36,201: time cost, forward:0.023081126839223535, backward:0.17883034667583428, data cost:1.0102890669697462 
2021-07-30 03:51:36,202: ============================================================
2021-07-30 03:51:36,202: Epoch 53/65 Batch 100/964 eta: 4:40:08.861601	Training Loss2 1.6863 (1.9343)	Training Loss3 2.6514 (2.9530)	Training Loss4 2.3508 (2.5608)	Training Total_Loss 4.6699 (5.2005)	Training Prec@1_up 99.219 (98.700)	Training Prec@1_down 98.438 (96.820)	
2021-07-30 03:51:36,202: ============================================================
2021-07-30 03:53:48,720: time cost, forward:0.02255780732811396, backward:0.17842303208969346, data cost:1.0013325442021817 
2021-07-30 03:53:48,721: ============================================================
2021-07-30 03:53:48,721: Epoch 53/65 Batch 200/964 eta: 4:32:23.550516	Training Loss2 1.7848 (1.9268)	Training Loss3 2.7058 (2.9494)	Training Loss4 2.3747 (2.5582)	Training Total_Loss 4.7856 (5.1919)	Training Prec@1_up 99.219 (98.745)	Training Prec@1_down 98.047 (96.874)	
2021-07-30 03:53:48,721: ============================================================
2021-07-30 03:56:02,797: time cost, forward:0.022385074143425675, backward:0.1786630935094827, data cost:1.0027718767274583 
2021-07-30 03:56:02,797: ============================================================
2021-07-30 03:56:02,797: Epoch 53/65 Batch 300/964 eta: 4:33:21.539126	Training Loss2 1.7747 (1.9256)	Training Loss3 2.8005 (2.9519)	Training Loss4 2.3791 (2.5571)	Training Total_Loss 4.8774 (5.1932)	Training Prec@1_up 99.414 (98.752)	Training Prec@1_down 97.461 (96.888)	
2021-07-30 03:56:02,797: ============================================================
2021-07-30 03:58:16,582: time cost, forward:0.02212296452438622, backward:0.1788725279327622, data cost:1.0022603670756023 
2021-07-30 03:58:16,582: ============================================================
2021-07-30 03:58:16,582: Epoch 53/65 Batch 400/964 eta: 4:30:32.134755	Training Loss2 1.9219 (1.9242)	Training Loss3 2.9851 (2.9536)	Training Loss4 2.5432 (2.5551)	Training Total_Loss 5.2177 (5.1933)	Training Prec@1_up 98.633 (98.749)	Training Prec@1_down 96.875 (96.904)	
2021-07-30 03:58:16,582: ============================================================
2021-07-30 04:00:31,759: time cost, forward:0.022008613498511916, backward:0.1790789243931283, data cost:1.004511402699656 
2021-07-30 04:00:31,760: ============================================================
2021-07-30 04:00:31,760: Epoch 53/65 Batch 500/964 eta: 4:31:05.902092	Training Loss2 1.7622 (1.9260)	Training Loss3 2.6994 (2.9563)	Training Loss4 2.4298 (2.5583)	Training Total_Loss 4.7954 (5.1984)	Training Prec@1_up 99.805 (98.761)	Training Prec@1_down 99.023 (96.896)	
2021-07-30 04:00:31,760: ============================================================
2021-07-30 04:02:45,204: time cost, forward:0.02190217589695187, backward:0.17909852809619425, data cost:1.0033069930609957 
2021-07-30 04:02:45,205: ============================================================
2021-07-30 04:02:45,205: Epoch 53/65 Batch 600/964 eta: 4:25:24.001003	Training Loss2 1.8338 (1.9265)	Training Loss3 2.8632 (2.9588)	Training Loss4 2.4783 (2.5589)	Training Total_Loss 5.0193 (5.2015)	Training Prec@1_up 98.438 (98.755)	Training Prec@1_down 96.094 (96.882)	
2021-07-30 04:02:45,205: ============================================================
2021-07-30 04:04:58,688: time cost, forward:0.021929809463893907, backward:0.17915547183995936, data cost:1.0024100847339767 
2021-07-30 04:04:58,688: ============================================================
2021-07-30 04:04:58,688: Epoch 53/65 Batch 700/964 eta: 4:23:15.107662	Training Loss2 2.0028 (1.9264)	Training Loss3 3.0670 (2.9578)	Training Loss4 2.5855 (2.5591)	Training Total_Loss 5.3611 (5.2005)	Training Prec@1_up 98.242 (98.753)	Training Prec@1_down 96.680 (96.886)	
2021-07-30 04:04:58,688: ============================================================
2021-07-30 04:07:14,275: time cost, forward:0.02197625132764833, backward:0.17911844408706074, data cost:1.0046415072358743 
2021-07-30 04:07:14,275: ============================================================
2021-07-30 04:07:14,275: Epoch 53/65 Batch 800/964 eta: 4:25:08.428320	Training Loss2 2.0091 (1.9269)	Training Loss3 3.1367 (2.9578)	Training Loss4 2.6606 (2.5599)	Training Total_Loss 5.4715 (5.2012)	Training Prec@1_up 98.633 (98.751)	Training Prec@1_down 97.461 (96.883)	
2021-07-30 04:07:14,275: ============================================================
2021-07-30 04:09:28,864: time cost, forward:0.022048339695235647, backward:0.1790611171085922, data cost:1.0052028277824665 
2021-07-30 04:09:28,865: ============================================================
2021-07-30 04:09:28,865: Epoch 53/65 Batch 900/964 eta: 4:20:56.772677	Training Loss2 1.8535 (1.9277)	Training Loss3 2.9030 (2.9582)	Training Loss4 2.5024 (2.5607)	Training Total_Loss 5.0810 (5.2023)	Training Prec@1_up 99.414 (98.749)	Training Prec@1_down 97.266 (96.872)	
2021-07-30 04:09:28,865: ============================================================
2021-07-30 04:10:56,275: Epoch: 53/65 eta: 4:19:29.289635	Training Loss2 2.3017 (1.9299)	Training Loss3 3.4040 (2.9590)	Training Loss4 3.0067 (2.5626)	Training Total_Loss 6.0582 (5.2052)	Training Prec@1_up 97.656 (98.743)	Training Prec@1_down 95.898 (96.871)	
2021-07-30 04:10:56,275: ============================================================
2021-07-30 04:10:56,278: Save Checkpoint...
2021-07-30 04:10:56,278: ============================================================
2021-07-30 04:10:56,928: Save done!
2021-07-30 04:10:56,929: ============================================================
2021-07-30 04:13:12,442: time cost, forward:0.024254360584297564, backward:0.17986035346984863, data cost:1.030621056604867 
2021-07-30 04:13:12,443: ============================================================
2021-07-30 04:13:12,443: Epoch 54/65 Batch 100/964 eta: 4:19:02.000874	Training Loss2 1.7906 (1.9243)	Training Loss3 2.7658 (2.9501)	Training Loss4 2.4135 (2.5550)	Training Total_Loss 4.8678 (5.1898)	Training Prec@1_up 99.023 (98.745)	Training Prec@1_down 97.461 (96.924)	
2021-07-30 04:13:12,443: ============================================================
2021-07-30 04:15:25,542: time cost, forward:0.02398717702932693, backward:0.17941425912943318, data cost:1.0131471384709805 
2021-07-30 04:15:25,543: ============================================================
2021-07-30 04:15:25,543: Epoch 54/65 Batch 200/964 eta: 4:12:12.093750	Training Loss2 2.1128 (1.9391)	Training Loss3 3.1869 (2.9742)	Training Loss4 2.7709 (2.5713)	Training Total_Loss 5.6287 (5.2295)	Training Prec@1_up 98.633 (98.723)	Training Prec@1_down 95.703 (96.834)	
2021-07-30 04:15:25,543: ============================================================
2021-07-30 04:17:38,765: time cost, forward:0.023914798845016835, backward:0.17940002301066216, data cost:1.0076285923603785 
2021-07-30 04:17:38,765: ============================================================
2021-07-30 04:17:38,765: Epoch 54/65 Batch 300/964 eta: 4:10:12.822283	Training Loss2 1.8582 (1.9392)	Training Loss3 2.9631 (2.9745)	Training Loss4 2.4429 (2.5718)	Training Total_Loss 5.1137 (5.2300)	Training Prec@1_up 98.438 (98.722)	Training Prec@1_down 96.094 (96.838)	
2021-07-30 04:17:38,765: ============================================================
2021-07-30 04:19:52,315: time cost, forward:0.023755383670778202, backward:0.1791887498439703, data cost:1.00608652038383 
2021-07-30 04:19:52,316: ============================================================
2021-07-30 04:19:52,316: Epoch 54/65 Batch 400/964 eta: 4:08:36.287637	Training Loss2 1.8123 (1.9340)	Training Loss3 2.7513 (2.9664)	Training Loss4 2.4659 (2.5675)	Training Total_Loss 4.8904 (5.2171)	Training Prec@1_up 99.219 (98.738)	Training Prec@1_down 97.461 (96.861)	
2021-07-30 04:19:52,316: ============================================================
2021-07-30 04:22:05,675: time cost, forward:0.02361554229904511, backward:0.17915245717417502, data cost:1.0045327753246667 
2021-07-30 04:22:05,676: ============================================================
2021-07-30 04:22:05,676: Epoch 54/65 Batch 500/964 eta: 4:06:01.590019	Training Loss2 2.0342 (1.9313)	Training Loss3 3.0029 (2.9611)	Training Loss4 2.7165 (2.5649)	Training Total_Loss 5.3783 (5.2092)	Training Prec@1_up 98.047 (98.744)	Training Prec@1_down 96.875 (96.866)	
2021-07-30 04:22:05,676: ============================================================
2021-07-30 04:24:19,579: time cost, forward:0.023416882564309044, backward:0.17903496466813382, data cost:1.0045189885344847 
2021-07-30 04:24:19,579: ============================================================
2021-07-30 04:24:19,580: Epoch 54/65 Batch 600/964 eta: 4:04:47.912321	Training Loss2 2.0317 (1.9334)	Training Loss3 3.1903 (2.9636)	Training Loss4 2.6159 (2.5667)	Training Total_Loss 5.5141 (5.2137)	Training Prec@1_up 98.438 (98.740)	Training Prec@1_down 96.289 (96.850)	
2021-07-30 04:24:19,580: ============================================================
2021-07-30 04:26:33,647: time cost, forward:0.023359751667246458, backward:0.17904870056457955, data cost:1.0045974107941504 
2021-07-30 04:26:33,647: ============================================================
2021-07-30 04:26:33,647: Epoch 54/65 Batch 700/964 eta: 4:02:51.840693	Training Loss2 1.7951 (1.9308)	Training Loss3 2.7944 (2.9609)	Training Loss4 2.4247 (2.5637)	Training Total_Loss 4.9043 (5.2081)	Training Prec@1_up 99.609 (98.749)	Training Prec@1_down 97.266 (96.862)	
2021-07-30 04:26:33,648: ============================================================
2021-07-30 04:28:48,004: time cost, forward:0.02325856163445044, backward:0.17905261161479544, data cost:1.005045224190952 
2021-07-30 04:28:48,004: ============================================================
2021-07-30 04:28:48,004: Epoch 54/65 Batch 800/964 eta: 4:01:08.912274	Training Loss2 1.9747 (1.9315)	Training Loss3 3.1028 (2.9622)	Training Loss4 2.6685 (2.5647)	Training Total_Loss 5.4243 (5.2103)	Training Prec@1_up 98.047 (98.749)	Training Prec@1_down 95.703 (96.862)	
2021-07-30 04:28:48,005: ============================================================
2021-07-30 04:31:02,227: time cost, forward:0.02316155979975445, backward:0.17901778327212053, data cost:1.0052884439207423 
2021-07-30 04:31:02,227: ============================================================
2021-07-30 04:31:02,227: Epoch 54/65 Batch 900/964 eta: 3:58:40.243761	Training Loss2 1.8036 (1.9305)	Training Loss3 2.8237 (2.9597)	Training Loss4 2.3981 (2.5635)	Training Total_Loss 4.9245 (5.2067)	Training Prec@1_up 98.828 (98.748)	Training Prec@1_down 97.266 (96.866)	
2021-07-30 04:31:02,228: ============================================================
2021-07-30 04:32:31,187: Epoch: 54/65 eta: 3:57:12.998860	Training Loss2 1.9563 (1.9300)	Training Loss3 3.0705 (2.9591)	Training Loss4 2.6011 (2.5627)	Training Total_Loss 5.3492 (5.2054)	Training Prec@1_up 98.438 (98.743)	Training Prec@1_down 97.070 (96.870)	
2021-07-30 04:32:31,188: ============================================================
2021-07-30 04:32:31,190: Save Checkpoint...
2021-07-30 04:32:31,191: ============================================================
2021-07-30 04:32:31,847: Save done!
2021-07-30 04:32:31,848: ============================================================
2021-07-30 04:34:47,800: time cost, forward:0.022439983155992296, backward:0.1782456792966284, data cost:1.0202275069072992 
2021-07-30 04:34:47,800: ============================================================
2021-07-30 04:34:47,800: Epoch 55/65 Batch 100/964 eta: 3:58:01.684431	Training Loss2 2.0130 (1.9339)	Training Loss3 3.1386 (2.9640)	Training Loss4 2.5790 (2.5631)	Training Total_Loss 5.4346 (5.2126)	Training Prec@1_up 98.828 (98.777)	Training Prec@1_down 96.094 (96.853)	
2021-07-30 04:34:47,801: ============================================================
2021-07-30 04:37:02,084: time cost, forward:0.022099654279162537, backward:0.17860064674262424, data cost:1.013618330260617 
2021-07-30 04:37:02,085: ============================================================
2021-07-30 04:37:02,085: Epoch 55/65 Batch 200/964 eta: 3:52:52.294523	Training Loss2 2.0280 (1.9451)	Training Loss3 3.0319 (2.9842)	Training Loss4 2.6227 (2.5766)	Training Total_Loss 5.3572 (5.2450)	Training Prec@1_up 97.852 (98.702)	Training Prec@1_down 96.484 (96.802)	
2021-07-30 04:37:02,085: ============================================================
2021-07-30 04:39:16,805: time cost, forward:0.02181983711727487, backward:0.17864622160742513, data cost:1.0129400519623006 
2021-07-30 04:39:16,805: ============================================================
2021-07-30 04:39:16,806: Epoch 55/65 Batch 300/964 eta: 3:51:22.990889	Training Loss2 2.0582 (1.9358)	Training Loss3 3.1667 (2.9704)	Training Loss4 2.6570 (2.5680)	Training Total_Loss 5.5243 (5.2223)	Training Prec@1_up 97.656 (98.740)	Training Prec@1_down 96.094 (96.855)	
2021-07-30 04:39:16,806: ============================================================
2021-07-30 04:41:30,952: time cost, forward:0.02171711276348372, backward:0.17885534028361613, data cost:1.0107205111281317 
2021-07-30 04:41:30,953: ============================================================
2021-07-30 04:41:30,953: Epoch 55/65 Batch 400/964 eta: 3:48:09.715956	Training Loss2 2.0181 (1.9348)	Training Loss3 2.9829 (2.9683)	Training Loss4 2.6847 (2.5670)	Training Total_Loss 5.3343 (5.2192)	Training Prec@1_up 98.438 (98.757)	Training Prec@1_down 96.484 (96.885)	
2021-07-30 04:41:30,953: ============================================================
2021-07-30 04:43:46,000: time cost, forward:0.021729300638477885, backward:0.17900342358377033, data cost:1.011258384268843 
2021-07-30 04:43:46,000: ============================================================
2021-07-30 04:43:46,000: Epoch 55/65 Batch 500/964 eta: 3:47:26.539601	Training Loss2 1.9433 (1.9347)	Training Loss3 3.0064 (2.9673)	Training Loss4 2.6365 (2.5673)	Training Total_Loss 5.2963 (5.2184)	Training Prec@1_up 98.633 (98.757)	Training Prec@1_down 96.680 (96.866)	
2021-07-30 04:43:46,000: ============================================================
2021-07-30 04:45:58,334: time cost, forward:0.02175937391481734, backward:0.17916669511237804, data cost:1.0076526405417263 
2021-07-30 04:45:58,334: ============================================================
2021-07-30 04:45:58,334: Epoch 55/65 Batch 600/964 eta: 3:40:40.021005	Training Loss2 1.9141 (1.9326)	Training Loss3 2.9389 (2.9633)	Training Loss4 2.6384 (2.5652)	Training Total_Loss 5.2152 (5.2121)	Training Prec@1_up 98.438 (98.758)	Training Prec@1_down 97.070 (96.867)	
2021-07-30 04:45:58,334: ============================================================
2021-07-30 04:48:12,064: time cost, forward:0.021844629566045277, backward:0.17925472320916827, data cost:1.0078107301087167 
2021-07-30 04:48:12,064: ============================================================
2021-07-30 04:48:12,064: Epoch 55/65 Batch 700/964 eta: 3:40:45.965428	Training Loss2 2.0357 (1.9313)	Training Loss3 3.1007 (2.9613)	Training Loss4 2.6420 (2.5632)	Training Total_Loss 5.4395 (5.2085)	Training Prec@1_up 98.633 (98.751)	Training Prec@1_down 96.484 (96.863)	
2021-07-30 04:48:12,064: ============================================================
2021-07-30 04:50:26,036: time cost, forward:0.021874537605218805, backward:0.1792075675778156, data cost:1.0063403333083858 
2021-07-30 04:50:26,036: ============================================================
2021-07-30 04:50:26,037: Epoch 55/65 Batch 800/964 eta: 3:38:55.975604	Training Loss2 1.9066 (1.9295)	Training Loss3 2.9384 (2.9600)	Training Loss4 2.5220 (2.5616)	Training Total_Loss 5.1527 (5.2055)	Training Prec@1_up 98.438 (98.752)	Training Prec@1_down 96.484 (96.858)	
2021-07-30 04:50:26,037: ============================================================
2021-07-30 04:52:40,444: time cost, forward:0.021837121254875344, backward:0.1791869793107963, data cost:1.0065302013422677 
2021-07-30 04:52:40,444: ============================================================
2021-07-30 04:52:40,444: Epoch 55/65 Batch 900/964 eta: 3:37:24.243667	Training Loss2 1.8304 (1.9295)	Training Loss3 2.8046 (2.9591)	Training Loss4 2.4780 (2.5616)	Training Total_Loss 4.9588 (5.2046)	Training Prec@1_up 99.023 (98.747)	Training Prec@1_down 97.656 (96.867)	
2021-07-30 04:52:40,444: ============================================================
2021-07-30 04:54:07,942: Epoch: 55/65 eta: 3:35:56.878820	Training Loss2 1.9133 (1.9297)	Training Loss3 2.9268 (2.9587)	Training Loss4 2.5079 (2.5624)	Training Total_Loss 5.1374 (5.2047)	Training Prec@1_up 98.438 (98.744)	Training Prec@1_down 97.070 (96.871)	
2021-07-30 04:54:07,943: ============================================================
2021-07-30 04:54:07,945: Save Checkpoint...
2021-07-30 04:54:07,947: ============================================================
2021-07-30 04:54:08,594: Save done!
2021-07-30 04:54:08,594: ============================================================
2021-07-30 04:56:24,123: time cost, forward:0.022615560377487028, backward:0.17779011678214024, data cost:1.0275125310878561 
2021-07-30 04:56:24,124: ============================================================
2021-07-30 04:56:24,124: Epoch 56/65 Batch 100/964 eta: 3:35:30.770803	Training Loss2 1.8910 (1.9320)	Training Loss3 2.8636 (2.9551)	Training Loss4 2.4752 (2.5605)	Training Total_Loss 5.0467 (5.2013)	Training Prec@1_up 99.219 (98.664)	Training Prec@1_down 97.656 (96.820)	
2021-07-30 04:56:24,124: ============================================================
2021-07-30 04:58:38,138: time cost, forward:0.023198192443080884, backward:0.1785042645344183, data cost:1.0185635724858424 
2021-07-30 04:58:38,138: ============================================================
2021-07-30 04:58:38,138: Epoch 56/65 Batch 200/964 eta: 3:30:52.309971	Training Loss2 1.9040 (1.9327)	Training Loss3 2.9296 (2.9586)	Training Loss4 2.5416 (2.5613)	Training Total_Loss 5.1524 (5.2056)	Training Prec@1_up 99.414 (98.700)	Training Prec@1_down 97.656 (96.831)	
2021-07-30 04:58:38,138: ============================================================
2021-07-30 05:00:51,095: time cost, forward:0.022832138482543538, backward:0.17858399196612, data cost:1.0064901149392527 
2021-07-30 05:00:51,096: ============================================================
2021-07-30 05:00:51,096: Epoch 56/65 Batch 300/964 eta: 3:26:59.574482	Training Loss2 1.8637 (1.9366)	Training Loss3 2.9429 (2.9645)	Training Loss4 2.4668 (2.5653)	Training Total_Loss 5.1082 (5.2154)	Training Prec@1_up 99.414 (98.699)	Training Prec@1_down 97.266 (96.855)	
2021-07-30 05:00:51,096: ============================================================
2021-07-30 05:03:03,478: time cost, forward:0.022818717143888164, backward:0.17864021203272923, data cost:1.002324572780676 
2021-07-30 05:03:03,479: ============================================================
2021-07-30 05:03:03,479: Epoch 56/65 Batch 400/964 eta: 3:23:53.519322	Training Loss2 1.8604 (1.9351)	Training Loss3 2.9068 (2.9633)	Training Loss4 2.4992 (2.5659)	Training Total_Loss 5.0866 (5.2138)	Training Prec@1_up 98.828 (98.710)	Training Prec@1_down 97.266 (96.844)	
2021-07-30 05:03:03,479: ============================================================
2021-07-30 05:05:15,743: time cost, forward:0.022999115601809086, backward:0.17878903224616347, data cost:1.0016929381834958 
2021-07-30 05:05:15,743: ============================================================
2021-07-30 05:05:15,743: Epoch 56/65 Batch 500/964 eta: 3:21:30.283007	Training Loss2 1.9490 (1.9344)	Training Loss3 2.9723 (2.9614)	Training Loss4 2.5772 (2.5664)	Training Total_Loss 5.2354 (5.2119)	Training Prec@1_up 98.438 (98.722)	Training Prec@1_down 96.289 (96.846)	
2021-07-30 05:05:15,743: ============================================================
2021-07-30 05:07:29,928: time cost, forward:0.023205012431327807, backward:0.1789862293631883, data cost:1.002387056366629 
2021-07-30 05:07:29,928: ============================================================
2021-07-30 05:07:29,928: Epoch 56/65 Batch 600/964 eta: 3:22:11.659270	Training Loss2 1.8164 (1.9296)	Training Loss3 2.7480 (2.9558)	Training Loss4 2.4808 (2.5621)	Training Total_Loss 4.8966 (5.2016)	Training Prec@1_up 99.414 (98.744)	Training Prec@1_down 97.656 (96.869)	
2021-07-30 05:07:29,928: ============================================================
2021-07-30 05:09:44,083: time cost, forward:0.02335143907898997, backward:0.1789174622220543, data cost:1.0030430369452175 
2021-07-30 05:09:44,083: ============================================================
2021-07-30 05:09:44,083: Epoch 56/65 Batch 700/964 eta: 3:19:54.779725	Training Loss2 1.7815 (1.9295)	Training Loss3 2.8451 (2.9557)	Training Loss4 2.4290 (2.5623)	Training Total_Loss 4.9503 (5.2016)	Training Prec@1_up 99.609 (98.741)	Training Prec@1_down 96.875 (96.868)	
2021-07-30 05:09:44,083: ============================================================
2021-07-30 05:11:57,078: time cost, forward:0.023432328793522115, backward:0.17886022244287522, data cost:1.0020470559522416 
2021-07-30 05:11:57,078: ============================================================
2021-07-30 05:11:57,078: Epoch 56/65 Batch 800/964 eta: 3:15:58.119705	Training Loss2 2.0592 (1.9298)	Training Loss3 3.0839 (2.9574)	Training Loss4 2.7313 (2.5629)	Training Total_Loss 5.4792 (5.2037)	Training Prec@1_up 99.609 (98.739)	Training Prec@1_down 96.484 (96.876)	
2021-07-30 05:11:57,079: ============================================================
2021-07-30 05:14:10,519: time cost, forward:0.023315180130344344, backward:0.17887300988856095, data cost:1.0008081493441334 
2021-07-30 05:14:10,520: ============================================================
2021-07-30 05:14:10,520: Epoch 56/65 Batch 900/964 eta: 3:14:24.103359	Training Loss2 1.9398 (1.9283)	Training Loss3 3.0234 (2.9567)	Training Loss4 2.6051 (2.5612)	Training Total_Loss 5.2959 (5.2015)	Training Prec@1_up 98.633 (98.744)	Training Prec@1_down 95.312 (96.874)	
2021-07-30 05:14:10,520: ============================================================
2021-07-30 05:15:37,506: Epoch: 56/65 eta: 3:12:57.366519	Training Loss2 1.8843 (1.9298)	Training Loss3 2.8817 (2.9588)	Training Loss4 2.5173 (2.5625)	Training Total_Loss 5.0825 (5.2049)	Training Prec@1_up 98.242 (98.743)	Training Prec@1_down 96.680 (96.871)	
2021-07-30 05:15:37,507: ============================================================
2021-07-30 05:15:37,509: Save Checkpoint...
2021-07-30 05:15:37,511: ============================================================
2021-07-30 05:15:38,185: Save done!
2021-07-30 05:15:38,186: ============================================================
2021-07-30 05:17:53,657: time cost, forward:0.023965787405919547, backward:0.1783403049815785, data cost:1.0317733528638127 
2021-07-30 05:17:53,657: ============================================================
2021-07-30 05:17:53,658: Epoch 57/65 Batch 100/964 eta: 3:13:39.296707	Training Loss2 1.9439 (1.9397)	Training Loss3 3.0260 (2.9671)	Training Loss4 2.5777 (2.5729)	Training Total_Loss 5.2868 (5.2234)	Training Prec@1_up 98.828 (98.704)	Training Prec@1_down 96.875 (96.820)	
2021-07-30 05:17:53,658: ============================================================
2021-07-30 05:20:07,673: time cost, forward:0.024147279298485223, backward:0.1789596847553349, data cost:1.0180167421024648 
2021-07-30 05:20:07,673: ============================================================
2021-07-30 05:20:07,673: Epoch 57/65 Batch 200/964 eta: 3:09:20.509988	Training Loss2 1.7666 (1.9282)	Training Loss3 2.7063 (2.9476)	Training Loss4 2.4393 (2.5610)	Training Total_Loss 4.8092 (5.1923)	Training Prec@1_up 98.242 (98.701)	Training Prec@1_down 97.266 (96.889)	
2021-07-30 05:20:07,673: ============================================================
2021-07-30 05:22:20,331: time cost, forward:0.024016459251327258, backward:0.17891069718427882, data cost:1.0094259471000238 
2021-07-30 05:22:20,331: ============================================================
2021-07-30 05:22:20,331: Epoch 57/65 Batch 300/964 eta: 3:05:12.778614	Training Loss2 1.8909 (1.9295)	Training Loss3 2.9650 (2.9520)	Training Loss4 2.5271 (2.5611)	Training Total_Loss 5.1740 (5.1973)	Training Prec@1_up 99.219 (98.698)	Training Prec@1_down 97.461 (96.865)	
2021-07-30 05:22:20,332: ============================================================
2021-07-30 05:24:33,806: time cost, forward:0.023864454494084333, backward:0.1788204923309479, data cost:1.0072359883396846 
2021-07-30 05:24:33,807: ============================================================
2021-07-30 05:24:33,807: Epoch 57/65 Batch 400/964 eta: 3:04:07.766058	Training Loss2 2.0735 (1.9299)	Training Loss3 3.0220 (2.9570)	Training Loss4 2.7218 (2.5621)	Training Total_Loss 5.4197 (5.2030)	Training Prec@1_up 98.828 (98.699)	Training Prec@1_down 97.266 (96.854)	
2021-07-30 05:24:33,807: ============================================================
2021-07-30 05:26:48,219: time cost, forward:0.023877220784495015, backward:0.17910037250939256, data cost:1.0072111467082419 
2021-07-30 05:26:48,219: ============================================================
2021-07-30 05:26:48,220: Epoch 57/65 Batch 500/964 eta: 3:03:10.922185	Training Loss2 1.9069 (1.9291)	Training Loss3 2.8787 (2.9558)	Training Loss4 2.5584 (2.5613)	Training Total_Loss 5.1114 (5.2010)	Training Prec@1_up 98.828 (98.714)	Training Prec@1_down 97.070 (96.853)	
2021-07-30 05:26:48,220: ============================================================
2021-07-30 05:29:02,330: time cost, forward:0.023903975701690318, backward:0.17929686608418002, data cost:1.0068307902060685 
2021-07-30 05:29:02,331: ============================================================
2021-07-30 05:29:02,331: Epoch 57/65 Batch 600/964 eta: 3:00:32.160110	Training Loss2 2.0669 (1.9300)	Training Loss3 3.2467 (2.9573)	Training Loss4 2.6915 (2.5624)	Training Total_Loss 5.6259 (5.2035)	Training Prec@1_up 98.633 (98.731)	Training Prec@1_down 96.094 (96.857)	
2021-07-30 05:29:02,331: ============================================================
2021-07-30 05:31:16,500: time cost, forward:0.023990786979467913, backward:0.17939524418635772, data cost:1.0065785126283615 
2021-07-30 05:31:16,501: ============================================================
2021-07-30 05:31:16,501: Epoch 57/65 Batch 700/964 eta: 2:58:22.751861	Training Loss2 1.9052 (1.9310)	Training Loss3 2.9156 (2.9599)	Training Loss4 2.4760 (2.5633)	Training Total_Loss 5.1063 (5.2071)	Training Prec@1_up 98.633 (98.733)	Training Prec@1_down 97.266 (96.851)	
2021-07-30 05:31:16,501: ============================================================
2021-07-30 05:33:30,199: time cost, forward:0.023975232665022564, backward:0.1793607859796517, data cost:1.0060170210049357 
2021-07-30 05:33:30,199: ============================================================
2021-07-30 05:33:30,200: Epoch 57/65 Batch 800/964 eta: 2:55:31.440755	Training Loss2 1.9121 (1.9328)	Training Loss3 2.8845 (2.9616)	Training Loss4 2.5534 (2.5646)	Training Total_Loss 5.1172 (5.2103)	Training Prec@1_up 98.633 (98.742)	Training Prec@1_down 97.656 (96.857)	
2021-07-30 05:33:30,200: ============================================================
2021-07-30 05:35:43,808: time cost, forward:0.02389730441822226, backward:0.1792463301551488, data cost:1.005574658131838 
2021-07-30 05:35:43,808: ============================================================
2021-07-30 05:35:43,808: Epoch 57/65 Batch 900/964 eta: 2:53:10.748713	Training Loss2 2.0039 (1.9308)	Training Loss3 3.0135 (2.9586)	Training Loss4 2.6804 (2.5634)	Training Total_Loss 5.3557 (5.2058)	Training Prec@1_up 98.828 (98.748)	Training Prec@1_down 96.289 (96.867)	
2021-07-30 05:35:43,808: ============================================================
2021-07-30 05:37:10,533: Epoch: 57/65 eta: 2:51:43.903057	Training Loss2 2.0745 (1.9296)	Training Loss3 3.2617 (2.9585)	Training Loss4 2.7576 (2.5621)	Training Total_Loss 5.6777 (5.2044)	Training Prec@1_up 98.828 (98.744)	Training Prec@1_down 96.094 (96.873)	
2021-07-30 05:37:10,533: ============================================================
2021-07-30 05:37:10,535: Save Checkpoint...
2021-07-30 05:37:10,536: ============================================================
2021-07-30 05:37:11,182: Save done!
2021-07-30 05:37:11,182: ============================================================
2021-07-30 05:39:25,992: time cost, forward:0.023021384923145025, backward:0.1787189498092189, data cost:1.0087127492885397 
2021-07-30 05:39:25,993: ============================================================
2021-07-30 05:39:25,993: Epoch 58/65 Batch 100/964 eta: 2:51:03.059086	Training Loss2 1.8512 (1.9484)	Training Loss3 2.8509 (2.9751)	Training Loss4 2.4816 (2.5869)	Training Total_Loss 5.0173 (5.2428)	Training Prec@1_up 99.023 (98.801)	Training Prec@1_down 97.070 (96.851)	
2021-07-30 05:39:25,993: ============================================================
2021-07-30 05:41:39,619: time cost, forward:0.0223932410005349, backward:0.178919170370054, data cost:1.003595656486013 
2021-07-30 05:41:39,619: ============================================================
2021-07-30 05:41:39,619: Epoch 58/65 Batch 200/964 eta: 2:47:19.352220	Training Loss2 2.2485 (1.9365)	Training Loss3 3.5172 (2.9650)	Training Loss4 2.9251 (2.5690)	Training Total_Loss 6.1040 (5.2177)	Training Prec@1_up 97.461 (98.736)	Training Prec@1_down 94.727 (96.828)	
2021-07-30 05:41:39,620: ============================================================
2021-07-30 05:43:53,497: time cost, forward:0.022116472091164477, backward:0.17910033484366428, data cost:1.0030098918289645 
2021-07-30 05:43:53,497: ============================================================
2021-07-30 05:43:53,498: Epoch 58/65 Batch 300/964 eta: 2:45:24.382416	Training Loss2 2.1007 (1.9261)	Training Loss3 3.2435 (2.9576)	Training Loss4 2.7019 (2.5553)	Training Total_Loss 5.6448 (5.1983)	Training Prec@1_up 97.656 (98.757)	Training Prec@1_down 95.312 (96.855)	
2021-07-30 05:43:53,498: ============================================================
2021-07-30 05:46:06,127: time cost, forward:0.02199751213379671, backward:0.17901389521166197, data cost:1.000954328025493 
2021-07-30 05:46:06,127: ============================================================
2021-07-30 05:46:06,127: Epoch 58/65 Batch 400/964 eta: 2:41:39.218901	Training Loss2 1.7742 (1.9265)	Training Loss3 2.8111 (2.9583)	Training Loss4 2.4466 (2.5565)	Training Total_Loss 4.9215 (5.1998)	Training Prec@1_up 99.609 (98.737)	Training Prec@1_down 97.852 (96.857)	
2021-07-30 05:46:06,127: ============================================================
2021-07-30 05:48:19,075: time cost, forward:0.021769303358150627, backward:0.17885337755054176, data cost:1.0002459490705349 
2021-07-30 05:48:19,075: ============================================================
2021-07-30 05:48:19,075: Epoch 58/65 Batch 500/964 eta: 2:39:49.517414	Training Loss2 1.8615 (1.9287)	Training Loss3 2.9466 (2.9606)	Training Loss4 2.4587 (2.5600)	Training Total_Loss 5.1067 (5.2049)	Training Prec@1_up 98.438 (98.745)	Training Prec@1_down 97.461 (96.862)	
2021-07-30 05:48:19,075: ============================================================
2021-07-30 05:50:30,729: time cost, forward:0.021534958347454294, backward:0.1787104152876865, data cost:0.9982094800531963 
2021-07-30 05:50:30,729: ============================================================
2021-07-30 05:50:30,729: Epoch 58/65 Batch 600/964 eta: 2:36:04.579803	Training Loss2 1.9210 (1.9306)	Training Loss3 2.9207 (2.9640)	Training Loss4 2.5416 (2.5620)	Training Total_Loss 5.1520 (5.2103)	Training Prec@1_up 98.047 (98.729)	Training Prec@1_down 96.680 (96.853)	
2021-07-30 05:50:30,730: ============================================================
2021-07-30 05:52:44,534: time cost, forward:0.021365903136726785, backward:0.17875113405383197, data cost:0.9986795386531322 
2021-07-30 05:52:44,535: ============================================================
2021-07-30 05:52:44,535: Epoch 58/65 Batch 700/964 eta: 2:36:23.761693	Training Loss2 1.8230 (1.9314)	Training Loss3 2.8168 (2.9634)	Training Loss4 2.4131 (2.5628)	Training Total_Loss 4.9349 (5.2105)	Training Prec@1_up 98.438 (98.735)	Training Prec@1_down 97.461 (96.857)	
2021-07-30 05:52:44,535: ============================================================
2021-07-30 05:54:56,975: time cost, forward:0.021264031473477284, backward:0.1788473191935667, data cost:0.9971581302088999 
2021-07-30 05:54:56,975: ============================================================
2021-07-30 05:54:56,975: Epoch 58/65 Batch 800/964 eta: 2:32:35.612656	Training Loss2 1.8027 (1.9284)	Training Loss3 2.8330 (2.9576)	Training Loss4 2.4061 (2.5597)	Training Total_Loss 4.9374 (5.2017)	Training Prec@1_up 98.438 (98.750)	Training Prec@1_down 97.656 (96.874)	
2021-07-30 05:54:56,975: ============================================================
2021-07-30 05:57:09,191: time cost, forward:0.021197820797115068, backward:0.17883849276584035, data cost:0.995795937613465 
2021-07-30 05:57:09,191: ============================================================
2021-07-30 05:57:09,191: Epoch 58/65 Batch 900/964 eta: 2:30:07.863416	Training Loss2 1.6645 (1.9285)	Training Loss3 2.5688 (2.9576)	Training Loss4 2.3139 (2.5603)	Training Total_Loss 4.5580 (5.2020)	Training Prec@1_up 99.219 (98.748)	Training Prec@1_down 98.047 (96.878)	
2021-07-30 05:57:09,191: ============================================================
2021-07-30 05:58:37,469: Epoch: 58/65 eta: 2:28:41.923136	Training Loss2 1.8180 (1.9296)	Training Loss3 2.8435 (2.9586)	Training Loss4 2.4238 (2.5616)	Training Total_Loss 4.9644 (5.2042)	Training Prec@1_up 98.828 (98.743)	Training Prec@1_down 97.070 (96.872)	
2021-07-30 05:58:37,469: ============================================================
2021-07-30 05:58:37,471: Save Checkpoint...
2021-07-30 05:58:37,472: ============================================================
2021-07-30 05:58:38,182: Save done!
2021-07-30 05:58:38,183: ============================================================
2021-07-30 06:00:52,645: time cost, forward:0.024641911188761394, backward:0.17957968663687657, data cost:1.0070307086212467 
2021-07-30 06:00:52,646: ============================================================
2021-07-30 06:00:52,646: Epoch 59/65 Batch 100/964 eta: 2:29:00.375963	Training Loss2 1.9967 (1.9377)	Training Loss3 3.0338 (2.9738)	Training Loss4 2.5964 (2.5677)	Training Total_Loss 5.3303 (5.2265)	Training Prec@1_up 98.047 (98.735)	Training Prec@1_down 95.508 (96.918)	
2021-07-30 06:00:52,646: ============================================================
2021-07-30 06:03:05,446: time cost, forward:0.022981408852428647, backward:0.17908957375952947, data cost:0.9986016534680697 
2021-07-30 06:03:05,447: ============================================================
2021-07-30 06:03:05,447: Epoch 59/65 Batch 200/964 eta: 2:24:57.135218	Training Loss2 1.9014 (1.9330)	Training Loss3 2.8973 (2.9644)	Training Loss4 2.5060 (2.5633)	Training Total_Loss 5.1010 (5.2126)	Training Prec@1_up 98.633 (98.760)	Training Prec@1_down 97.266 (96.909)	
2021-07-30 06:03:05,447: ============================================================
2021-07-30 06:05:18,025: time cost, forward:0.022566400642777765, backward:0.178999632098603, data cost:0.9962700059182668 
2021-07-30 06:05:18,025: ============================================================
2021-07-30 06:05:18,026: Epoch 59/65 Batch 300/964 eta: 2:22:29.985637	Training Loss2 1.8777 (1.9300)	Training Loss3 2.8429 (2.9623)	Training Loss4 2.4789 (2.5599)	Training Total_Loss 5.0212 (5.2073)	Training Prec@1_up 98.633 (98.767)	Training Prec@1_down 97.266 (96.863)	
2021-07-30 06:05:18,026: ============================================================
2021-07-30 06:07:30,788: time cost, forward:0.022259138580551723, backward:0.17896149391518498, data cost:0.9947795515371146 
2021-07-30 06:07:30,788: ============================================================
2021-07-30 06:07:30,788: Epoch 59/65 Batch 400/964 eta: 2:20:29.110504	Training Loss2 1.6881 (1.9292)	Training Loss3 2.6442 (2.9578)	Training Loss4 2.2913 (2.5600)	Training Total_Loss 4.6339 (5.2025)	Training Prec@1_up 99.219 (98.763)	Training Prec@1_down 98.242 (96.870)	
2021-07-30 06:07:30,788: ============================================================
2021-07-30 06:09:43,058: time cost, forward:0.022068480451503593, backward:0.17901708845623987, data cost:0.9932388356310093 
2021-07-30 06:09:43,058: ============================================================
2021-07-30 06:09:43,058: Epoch 59/65 Batch 500/964 eta: 2:17:45.552165	Training Loss2 1.8803 (1.9279)	Training Loss3 2.8068 (2.9534)	Training Loss4 2.5385 (2.5596)	Training Total_Loss 5.0161 (5.1971)	Training Prec@1_up 98.633 (98.760)	Training Prec@1_down 97.461 (96.867)	
2021-07-30 06:09:43,059: ============================================================
2021-07-30 06:11:55,862: time cost, forward:0.02203255463920173, backward:0.1790045071921882, data cost:0.9928869293607734 
2021-07-30 06:11:55,862: ============================================================
2021-07-30 06:11:55,863: Epoch 59/65 Batch 600/964 eta: 2:16:06.132292	Training Loss2 1.8898 (1.9303)	Training Loss3 2.8878 (2.9577)	Training Loss4 2.4868 (2.5624)	Training Total_Loss 5.0761 (5.2040)	Training Prec@1_up 98.242 (98.745)	Training Prec@1_down 97.266 (96.861)	
2021-07-30 06:11:55,863: ============================================================
2021-07-30 06:14:09,005: time cost, forward:0.022033156243516653, backward:0.17889158476746303, data cost:0.9934252082704645 
2021-07-30 06:14:09,005: ============================================================
2021-07-30 06:14:09,005: Epoch 59/65 Batch 700/964 eta: 2:14:13.813141	Training Loss2 1.9776 (1.9328)	Training Loss3 3.0540 (2.9606)	Training Loss4 2.6355 (2.5657)	Training Total_Loss 5.3605 (5.2099)	Training Prec@1_up 98.242 (98.729)	Training Prec@1_down 96.289 (96.854)	
2021-07-30 06:14:09,006: ============================================================
2021-07-30 06:16:21,816: time cost, forward:0.02199302119516461, backward:0.17889250294586295, data cost:0.9930868336793329 
2021-07-30 06:16:21,816: ============================================================
2021-07-30 06:16:21,816: Epoch 59/65 Batch 800/964 eta: 2:11:40.909660	Training Loss2 1.9174 (1.9329)	Training Loss3 2.9696 (2.9610)	Training Loss4 2.5536 (2.5659)	Training Total_Loss 5.2051 (5.2104)	Training Prec@1_up 99.023 (98.725)	Training Prec@1_down 96.484 (96.849)	
2021-07-30 06:16:21,816: ============================================================
2021-07-30 06:18:35,485: time cost, forward:0.021874053061870367, backward:0.17890035138114277, data cost:0.9940064589889747 
2021-07-30 06:18:35,485: ============================================================
2021-07-30 06:18:35,485: Epoch 59/65 Batch 900/964 eta: 2:10:18.308702	Training Loss2 2.1025 (1.9307)	Training Loss3 3.1944 (2.9591)	Training Loss4 2.7832 (2.5644)	Training Total_Loss 5.6373 (5.2067)	Training Prec@1_up 98.242 (98.738)	Training Prec@1_down 96.484 (96.863)	
2021-07-30 06:18:35,485: ============================================================
2021-07-30 06:20:01,472: Epoch: 59/65 eta: 2:08:51.423753	Training Loss2 1.9991 (1.9296)	Training Loss3 3.0867 (2.9585)	Training Loss4 2.6207 (2.5634)	Training Total_Loss 5.3966 (5.2050)	Training Prec@1_up 98.047 (98.743)	Training Prec@1_down 96.094 (96.872)	
2021-07-30 06:20:01,473: ============================================================
2021-07-30 06:20:01,475: Save Checkpoint...
2021-07-30 06:20:01,478: ============================================================
2021-07-30 06:20:02,155: Save done!
2021-07-30 06:20:02,155: ============================================================
2021-07-30 06:22:16,173: time cost, forward:0.023684595570419773, backward:0.17787106109387946, data cost:1.007113760167902 
2021-07-30 06:22:16,174: ============================================================
2021-07-30 06:22:16,174: Epoch 60/65 Batch 100/964 eta: 2:06:58.882021	Training Loss2 1.7976 (1.9379)	Training Loss3 2.8057 (2.9643)	Training Loss4 2.4748 (2.5736)	Training Total_Loss 4.9419 (5.2201)	Training Prec@1_up 99.219 (98.771)	Training Prec@1_down 97.656 (96.871)	
2021-07-30 06:22:16,174: ============================================================
2021-07-30 06:24:29,193: time cost, forward:0.023604387014954535, backward:0.17792132152384849, data cost:0.9992382394608541 
2021-07-30 06:24:29,194: ============================================================
2021-07-30 06:24:29,194: Epoch 60/65 Batch 200/964 eta: 2:03:49.166413	Training Loss2 2.1311 (1.9335)	Training Loss3 3.1660 (2.9627)	Training Loss4 2.7028 (2.5679)	Training Total_Loss 5.5830 (5.2133)	Training Prec@1_up 98.828 (98.764)	Training Prec@1_down 95.898 (96.828)	
2021-07-30 06:24:29,194: ============================================================
2021-07-30 06:26:40,901: time cost, forward:0.023551223270071787, backward:0.1783177988186329, data cost:0.9925396665681565 
2021-07-30 06:26:40,902: ============================================================
2021-07-30 06:26:40,902: Epoch 60/65 Batch 300/964 eta: 2:00:24.177284	Training Loss2 1.9327 (1.9374)	Training Loss3 2.7936 (2.9681)	Training Loss4 2.5600 (2.5727)	Training Total_Loss 5.0400 (5.2231)	Training Prec@1_up 98.633 (98.735)	Training Prec@1_down 97.852 (96.816)	
2021-07-30 06:26:40,902: ============================================================
2021-07-30 06:28:54,456: time cost, forward:0.023159530228540712, backward:0.17869154731731368, data cost:0.9927591022692228 
2021-07-30 06:28:54,457: ============================================================
2021-07-30 06:28:54,457: Epoch 60/65 Batch 400/964 eta: 1:59:51.917354	Training Loss2 1.9768 (1.9351)	Training Loss3 3.0505 (2.9665)	Training Loss4 2.5897 (2.5692)	Training Total_Loss 5.3337 (5.2186)	Training Prec@1_up 98.633 (98.745)	Training Prec@1_down 96.094 (96.822)	
2021-07-30 06:28:54,457: ============================================================
2021-07-30 06:31:07,265: time cost, forward:0.02286910532949444, backward:0.17885526387629386, data cost:0.9929377822455518 
2021-07-30 06:31:07,266: ============================================================
2021-07-30 06:31:07,266: Epoch 60/65 Batch 500/964 eta: 1:56:58.959418	Training Loss2 2.0705 (1.9345)	Training Loss3 3.2528 (2.9668)	Training Loss4 2.6733 (2.5687)	Training Total_Loss 5.6247 (5.2184)	Training Prec@1_up 98.438 (98.745)	Training Prec@1_down 95.312 (96.822)	
2021-07-30 06:31:07,266: ============================================================
2021-07-30 06:33:20,134: time cost, forward:0.022802572218523998, backward:0.17893402166478026, data cost:0.9927327955306472 
2021-07-30 06:33:20,135: ============================================================
2021-07-30 06:33:20,135: Epoch 60/65 Batch 600/964 eta: 1:54:49.267401	Training Loss2 2.0053 (1.9352)	Training Loss3 2.8760 (2.9667)	Training Loss4 2.6092 (2.5703)	Training Total_Loss 5.1833 (5.2195)	Training Prec@1_up 98.242 (98.745)	Training Prec@1_down 96.875 (96.832)	
2021-07-30 06:33:20,135: ============================================================
2021-07-30 06:35:34,099: time cost, forward:0.022711514744465273, backward:0.17895633024889682, data cost:0.993826320754612 
2021-07-30 06:35:34,100: ============================================================
2021-07-30 06:35:34,100: Epoch 60/65 Batch 700/964 eta: 1:53:32.118209	Training Loss2 1.8930 (1.9335)	Training Loss3 2.9508 (2.9647)	Training Loss4 2.5926 (2.5676)	Training Total_Loss 5.1936 (5.2153)	Training Prec@1_up 99.023 (98.744)	Training Prec@1_down 97.656 (96.839)	
2021-07-30 06:35:34,100: ============================================================
2021-07-30 06:37:45,913: time cost, forward:0.022554088146128554, backward:0.17910004258901813, data cost:0.992076676539396 
2021-07-30 06:37:45,914: ============================================================
2021-07-30 06:37:45,914: Epoch 60/65 Batch 800/964 eta: 1:49:30.928929	Training Loss2 1.9445 (1.9295)	Training Loss3 3.0240 (2.9599)	Training Loss4 2.6026 (2.5632)	Training Total_Loss 5.2976 (5.2062)	Training Prec@1_up 98.438 (98.745)	Training Prec@1_down 96.875 (96.857)	
2021-07-30 06:37:45,914: ============================================================
2021-07-30 06:39:59,970: time cost, forward:0.02253583620599698, backward:0.1793941467569455, data cost:0.9931132196717055 
2021-07-30 06:39:59,970: ============================================================
2021-07-30 06:39:59,970: Epoch 60/65 Batch 900/964 eta: 1:49:08.654421	Training Loss2 1.9724 (1.9296)	Training Loss3 3.0292 (2.9593)	Training Loss4 2.6207 (2.5631)	Training Total_Loss 5.3258 (5.2057)	Training Prec@1_up 99.023 (98.744)	Training Prec@1_down 96.289 (96.865)	
2021-07-30 06:39:59,971: ============================================================
2021-07-30 06:41:26,870: Epoch: 60/65 eta: 1:47:41.517770	Training Loss2 1.8222 (1.9295)	Training Loss3 2.7659 (2.9585)	Training Loss4 2.4888 (2.5628)	Training Total_Loss 4.9215 (5.2046)	Training Prec@1_up 99.414 (98.743)	Training Prec@1_down 97.461 (96.872)	
2021-07-30 06:41:26,871: ============================================================
2021-07-30 06:41:26,873: Save Checkpoint...
2021-07-30 06:41:26,875: ============================================================
2021-07-30 06:41:27,552: Save done!
2021-07-30 06:41:27,553: ============================================================
2021-07-30 06:43:42,724: time cost, forward:0.02379033300611708, backward:0.1783247957325945, data cost:1.0221322955507222 
2021-07-30 06:43:42,725: ============================================================
2021-07-30 06:43:42,725: Epoch 61/65 Batch 100/964 eta: 1:46:21.404530	Training Loss2 2.0787 (1.9313)	Training Loss3 3.0098 (2.9688)	Training Loss4 2.7224 (2.5571)	Training Total_Loss 5.4104 (5.2129)	Training Prec@1_up 98.438 (98.783)	Training Prec@1_down 96.484 (96.853)	
2021-07-30 06:43:42,725: ============================================================
2021-07-30 06:45:56,733: time cost, forward:0.023810457344630255, backward:0.1785045808284127, data cost:1.0167358808181992 
2021-07-30 06:45:56,734: ============================================================
2021-07-30 06:45:56,734: Epoch 61/65 Batch 200/964 eta: 1:43:12.550847	Training Loss2 1.9562 (1.9299)	Training Loss3 3.0431 (2.9631)	Training Loss4 2.5484 (2.5587)	Training Total_Loss 5.2954 (5.2074)	Training Prec@1_up 98.633 (98.768)	Training Prec@1_down 95.703 (96.852)	
2021-07-30 06:45:56,734: ============================================================
2021-07-30 06:48:10,455: time cost, forward:0.024107453017729184, backward:0.17869491880155328, data cost:1.011895741108668 
2021-07-30 06:48:10,456: ============================================================
2021-07-30 06:48:10,456: Epoch 61/65 Batch 300/964 eta: 1:40:45.586608	Training Loss2 2.0210 (1.9307)	Training Loss3 3.0513 (2.9584)	Training Loss4 2.5899 (2.5612)	Training Total_Loss 5.3568 (5.2043)	Training Prec@1_up 98.828 (98.750)	Training Prec@1_down 96.875 (96.863)	
2021-07-30 06:48:10,456: ============================================================
2021-07-30 06:50:24,869: time cost, forward:0.024028113611359942, backward:0.17883835759079247, data cost:1.011264605629713 
2021-07-30 06:50:24,870: ============================================================
2021-07-30 06:50:24,870: Epoch 61/65 Batch 400/964 eta: 1:39:02.434625	Training Loss2 1.8055 (1.9331)	Training Loss3 2.9615 (2.9629)	Training Loss4 2.5290 (2.5634)	Training Total_Loss 5.1287 (5.2111)	Training Prec@1_up 99.023 (98.735)	Training Prec@1_down 96.094 (96.844)	
2021-07-30 06:50:24,870: ============================================================
2021-07-30 06:52:38,065: time cost, forward:0.023972927926776404, backward:0.1789059256742856, data cost:1.0084239146513547 
2021-07-30 06:52:38,065: ============================================================
2021-07-30 06:52:38,066: Epoch 61/65 Batch 500/964 eta: 1:35:55.384783	Training Loss2 1.9534 (1.9338)	Training Loss3 2.9697 (2.9634)	Training Loss4 2.5694 (2.5644)	Training Total_Loss 5.2311 (5.2125)	Training Prec@1_up 98.242 (98.740)	Training Prec@1_down 96.875 (96.846)	
2021-07-30 06:52:38,066: ============================================================
2021-07-30 06:54:51,590: time cost, forward:0.023962887778306047, backward:0.1789430385837969, data cost:1.0071323013464875 
2021-07-30 06:54:51,590: ============================================================
2021-07-30 06:54:51,591: Epoch 61/65 Batch 600/964 eta: 1:33:56.086732	Training Loss2 1.7054 (1.9340)	Training Loss3 2.6761 (2.9626)	Training Loss4 2.3195 (2.5663)	Training Total_Loss 4.6886 (5.2128)	Training Prec@1_up 99.805 (98.740)	Training Prec@1_down 98.242 (96.847)	
2021-07-30 06:54:51,591: ============================================================
2021-07-30 06:57:03,246: time cost, forward:0.023917325747030146, backward:0.17895377893134076, data cost:1.0035547180066633 
2021-07-30 06:57:03,246: ============================================================
2021-07-30 06:57:03,246: Epoch 61/65 Batch 700/964 eta: 1:30:25.541044	Training Loss2 1.9581 (1.9318)	Training Loss3 3.1177 (2.9581)	Training Loss4 2.5523 (2.5639)	Training Total_Loss 5.3729 (5.2060)	Training Prec@1_up 98.047 (98.745)	Training Prec@1_down 96.094 (96.869)	
2021-07-30 06:57:03,247: ============================================================
2021-07-30 06:59:16,593: time cost, forward:0.02402667139886467, backward:0.1788873603854221, data cost:1.0030232773853631 
2021-07-30 06:59:16,593: ============================================================
2021-07-30 06:59:16,593: Epoch 61/65 Batch 800/964 eta: 1:29:21.869147	Training Loss2 2.1195 (1.9305)	Training Loss3 3.1986 (2.9576)	Training Loss4 2.7564 (2.5625)	Training Total_Loss 5.6366 (5.2041)	Training Prec@1_up 97.852 (98.739)	Training Prec@1_down 95.703 (96.865)	
2021-07-30 06:59:16,593: ============================================================
2021-07-30 07:01:29,740: time cost, forward:0.024063396241694058, backward:0.17886523967059753, data cost:1.0023710054072974 
2021-07-30 07:01:29,741: ============================================================
2021-07-30 07:01:29,741: Epoch 61/65 Batch 900/964 eta: 1:27:00.716122	Training Loss2 1.6728 (1.9306)	Training Loss3 2.6809 (2.9589)	Training Loss4 2.2829 (2.5630)	Training Total_Loss 4.6588 (5.2057)	Training Prec@1_up 99.219 (98.743)	Training Prec@1_down 98.242 (96.870)	
2021-07-30 07:01:29,741: ============================================================
2021-07-30 07:02:57,147: Epoch: 61/65 eta: 1:25:34.170203	Training Loss2 1.9053 (1.9294)	Training Loss3 2.9916 (2.9584)	Training Loss4 2.5514 (2.5621)	Training Total_Loss 5.2199 (5.2041)	Training Prec@1_up 98.438 (98.744)	Training Prec@1_down 95.508 (96.872)	
2021-07-30 07:02:57,147: ============================================================
2021-07-30 07:02:57,150: Save Checkpoint...
2021-07-30 07:02:57,152: ============================================================
2021-07-30 07:02:57,833: Save done!
2021-07-30 07:02:57,833: ============================================================
2021-07-30 07:05:11,567: time cost, forward:0.02423319672093247, backward:0.17714659132138647, data cost:1.0156009630723433 
2021-07-30 07:05:11,568: ============================================================
2021-07-30 07:05:11,568: Epoch 62/65 Batch 100/964 eta: 1:23:44.353801	Training Loss2 1.8651 (1.9414)	Training Loss3 2.9406 (2.9706)	Training Loss4 2.4920 (2.5704)	Training Total_Loss 5.1191 (5.2265)	Training Prec@1_up 99.023 (98.751)	Training Prec@1_down 97.266 (96.843)	
2021-07-30 07:05:11,568: ============================================================
2021-07-30 07:07:23,969: time cost, forward:0.023605146599774383, backward:0.1779349736831895, data cost:1.0019279664485299 
2021-07-30 07:07:23,970: ============================================================
2021-07-30 07:07:23,970: Epoch 62/65 Batch 200/964 eta: 1:20:41.939098	Training Loss2 2.0752 (1.9315)	Training Loss3 3.0437 (2.9614)	Training Loss4 2.7231 (2.5628)	Training Total_Loss 5.4428 (5.2085)	Training Prec@1_up 98.828 (98.728)	Training Prec@1_down 96.094 (96.844)	
2021-07-30 07:07:23,970: ============================================================
2021-07-30 07:09:36,126: time cost, forward:0.023426340575202254, backward:0.17810331299950846, data cost:0.9968505296420094 
2021-07-30 07:09:36,126: ============================================================
2021-07-30 07:09:36,127: Epoch 62/65 Batch 300/964 eta: 1:18:20.816919	Training Loss2 1.7479 (1.9281)	Training Loss3 2.7539 (2.9549)	Training Loss4 2.3513 (2.5600)	Training Total_Loss 4.8035 (5.1990)	Training Prec@1_up 99.219 (98.746)	Training Prec@1_down 98.242 (96.890)	
2021-07-30 07:09:36,127: ============================================================
2021-07-30 07:11:49,115: time cost, forward:0.023312982759977643, backward:0.17817499046038865, data cost:0.996512440511756 
2021-07-30 07:11:49,115: ============================================================
2021-07-30 07:11:49,115: Epoch 62/65 Batch 400/964 eta: 1:16:37.424474	Training Loss2 1.8247 (1.9271)	Training Loss3 2.7827 (2.9541)	Training Loss4 2.4279 (2.5607)	Training Total_Loss 4.9090 (5.1980)	Training Prec@1_up 98.828 (98.769)	Training Prec@1_down 98.047 (96.903)	
2021-07-30 07:11:49,115: ============================================================
2021-07-30 07:14:02,759: time cost, forward:0.023313236141013716, backward:0.17831292754423642, data cost:0.9976159804808592 
2021-07-30 07:14:02,759: ============================================================
2021-07-30 07:14:02,759: Epoch 62/65 Batch 500/964 eta: 1:14:46.426152	Training Loss2 1.8757 (1.9276)	Training Loss3 2.8448 (2.9540)	Training Loss4 2.4457 (2.5603)	Training Total_Loss 5.0055 (5.1980)	Training Prec@1_up 98.633 (98.773)	Training Prec@1_down 96.289 (96.874)	
2021-07-30 07:14:02,759: ============================================================
2021-07-30 07:16:15,415: time cost, forward:0.023846067848906097, backward:0.17844089801005011, data cost:0.9966969701005939 
2021-07-30 07:16:15,416: ============================================================
2021-07-30 07:16:15,416: Epoch 62/65 Batch 600/964 eta: 1:12:00.620531	Training Loss2 1.8897 (1.9289)	Training Loss3 2.9271 (2.9557)	Training Loss4 2.4837 (2.5619)	Training Total_Loss 5.1138 (5.2011)	Training Prec@1_up 99.023 (98.772)	Training Prec@1_down 97.461 (96.886)	
2021-07-30 07:16:15,416: ============================================================
2021-07-30 07:18:27,585: time cost, forward:0.023934107481665877, backward:0.1785551120283266, data cost:0.9951697228121996 
2021-07-30 07:18:27,585: ============================================================
2021-07-30 07:18:27,585: Epoch 62/65 Batch 700/964 eta: 1:09:32.592933	Training Loss2 1.8924 (1.9281)	Training Loss3 2.9417 (2.9568)	Training Loss4 2.4634 (2.5610)	Training Total_Loss 5.1196 (5.2014)	Training Prec@1_up 98.438 (98.757)	Training Prec@1_down 96.680 (96.874)	
2021-07-30 07:18:27,585: ============================================================
2021-07-30 07:20:38,405: time cost, forward:0.02389235102637987, backward:0.17850788364720732, data cost:0.992583802405824 
2021-07-30 07:20:38,405: ============================================================
2021-07-30 07:20:38,406: Epoch 62/65 Batch 800/964 eta: 1:06:39.173477	Training Loss2 2.0257 (1.9285)	Training Loss3 3.0696 (2.9571)	Training Loss4 2.6901 (2.5612)	Training Total_Loss 5.4274 (5.2020)	Training Prec@1_up 98.633 (98.748)	Training Prec@1_down 96.680 (96.871)	
2021-07-30 07:20:38,406: ============================================================
2021-07-30 07:22:52,808: time cost, forward:0.02381275519645784, backward:0.17845474811761874, data cost:0.9944838642145821 
2021-07-30 07:22:52,808: ============================================================
2021-07-30 07:22:52,808: Epoch 62/65 Batch 900/964 eta: 1:06:14.294634	Training Loss2 1.9464 (1.9290)	Training Loss3 3.0349 (2.9581)	Training Loss4 2.5241 (2.5616)	Training Total_Loss 5.2702 (5.2034)	Training Prec@1_up 98.633 (98.747)	Training Prec@1_down 96.289 (96.876)	
2021-07-30 07:22:52,809: ============================================================
2021-07-30 07:24:21,249: Epoch: 62/65 eta: 1:04:46.932729	Training Loss2 1.8851 (1.9294)	Training Loss3 2.7273 (2.9584)	Training Loss4 2.5061 (2.5625)	Training Total_Loss 4.9229 (5.2044)	Training Prec@1_up 98.633 (98.744)	Training Prec@1_down 97.070 (96.872)	
2021-07-30 07:24:21,249: ============================================================
2021-07-30 07:24:21,251: Save Checkpoint...
2021-07-30 07:24:21,253: ============================================================
2021-07-30 07:24:21,935: Save done!
2021-07-30 07:24:21,935: ============================================================
2021-07-30 07:26:36,643: time cost, forward:0.023541768391927082, backward:0.1780643944788461, data cost:1.0151110803238068 
2021-07-30 07:26:36,643: ============================================================
2021-07-30 07:26:36,644: Epoch 63/65 Batch 100/964 eta: 1:02:42.357494	Training Loss2 1.9023 (1.9346)	Training Loss3 2.9406 (2.9587)	Training Loss4 2.5075 (2.5614)	Training Total_Loss 5.1454 (5.2067)	Training Prec@1_up 98.047 (98.714)	Training Prec@1_down 96.094 (96.830)	
2021-07-30 07:26:36,644: ============================================================
2021-07-30 07:28:50,219: time cost, forward:0.023003769879365088, backward:0.17836034837080605, data cost:1.0081854369772139 
2021-07-30 07:28:50,219: ============================================================
2021-07-30 07:28:50,219: Epoch 63/65 Batch 200/964 eta: 0:59:57.189987	Training Loss2 1.9071 (1.9311)	Training Loss3 3.0215 (2.9580)	Training Loss4 2.5163 (2.5614)	Training Total_Loss 5.2332 (5.2043)	Training Prec@1_up 98.828 (98.752)	Training Prec@1_down 97.070 (96.843)	
2021-07-30 07:28:50,219: ============================================================
2021-07-30 07:31:03,674: time cost, forward:0.022691160540118264, backward:0.1782902945642886, data cost:1.004234439951919 
2021-07-30 07:31:03,674: ============================================================
2021-07-30 07:31:03,674: Epoch 63/65 Batch 300/964 eta: 0:57:40.488439	Training Loss2 2.0117 (1.9358)	Training Loss3 3.1884 (2.9629)	Training Loss4 2.7090 (2.5667)	Training Total_Loss 5.5487 (5.2142)	Training Prec@1_up 98.633 (98.716)	Training Prec@1_down 95.898 (96.819)	
2021-07-30 07:31:03,675: ============================================================
2021-07-30 07:33:15,507: time cost, forward:0.022642807852953, backward:0.17827543041162325, data cost:1.0005492285678261 
2021-07-30 07:33:15,508: ============================================================
2021-07-30 07:33:15,508: Epoch 63/65 Batch 400/964 eta: 0:54:46.608315	Training Loss2 1.8355 (1.9359)	Training Loss3 2.7504 (2.9641)	Training Loss4 2.4235 (2.5679)	Training Total_Loss 4.8799 (5.2160)	Training Prec@1_up 98.828 (98.711)	Training Prec@1_down 96.289 (96.825)	
2021-07-30 07:33:15,508: ============================================================
2021-07-30 07:35:30,949: time cost, forward:0.022957561966890323, backward:0.17825880652678036, data cost:1.0045344088980575 
2021-07-30 07:35:30,949: ============================================================
2021-07-30 07:35:30,950: Epoch 63/65 Batch 500/964 eta: 0:54:01.124302	Training Loss2 1.7959 (1.9317)	Training Loss3 2.7839 (2.9594)	Training Loss4 2.4787 (2.5641)	Training Total_Loss 4.9212 (5.2074)	Training Prec@1_up 99.219 (98.726)	Training Prec@1_down 97.852 (96.861)	
2021-07-30 07:35:30,950: ============================================================
2021-07-30 07:37:45,903: time cost, forward:0.02290433038256204, backward:0.17829926702534415, data cost:1.0052740824640494 
2021-07-30 07:37:45,903: ============================================================
2021-07-30 07:37:45,903: Epoch 63/65 Batch 600/964 eta: 0:51:34.480126	Training Loss2 2.0019 (1.9319)	Training Loss3 3.0774 (2.9596)	Training Loss4 2.6194 (2.5649)	Training Total_Loss 5.3881 (5.2080)	Training Prec@1_up 98.242 (98.727)	Training Prec@1_down 96.680 (96.850)	
2021-07-30 07:37:45,903: ============================================================
2021-07-30 07:40:00,993: time cost, forward:0.023064536257022098, backward:0.17837100376898637, data cost:1.007834997982085 
2021-07-30 07:40:00,993: ============================================================
2021-07-30 07:40:00,993: Epoch 63/65 Batch 700/964 eta: 0:49:22.531682	Training Loss2 1.9366 (1.9325)	Training Loss3 2.9613 (2.9610)	Training Loss4 2.5286 (2.5654)	Training Total_Loss 5.1939 (5.2099)	Training Prec@1_up 98.438 (98.724)	Training Prec@1_down 97.070 (96.852)	
2021-07-30 07:40:00,994: ============================================================
2021-07-30 07:42:15,667: time cost, forward:0.023339501310498903, backward:0.17850536667509878, data cost:1.0082159916659321 
2021-07-30 07:42:15,667: ============================================================
2021-07-30 07:42:15,668: Epoch 63/65 Batch 800/964 eta: 0:46:58.731391	Training Loss2 1.9066 (1.9308)	Training Loss3 2.8899 (2.9597)	Training Loss4 2.5047 (2.5629)	Training Total_Loss 5.0955 (5.2065)	Training Prec@1_up 98.828 (98.736)	Training Prec@1_down 97.461 (96.861)	
2021-07-30 07:42:15,668: ============================================================
2021-07-30 07:44:30,678: time cost, forward:0.023453141743932603, backward:0.17858921725704885, data cost:1.0089590215311697 
2021-07-30 07:44:30,678: ============================================================
2021-07-30 07:44:30,678: Epoch 63/65 Batch 900/964 eta: 0:44:50.762571	Training Loss2 2.0286 (1.9291)	Training Loss3 3.0765 (2.9577)	Training Loss4 2.6665 (2.5617)	Training Total_Loss 5.4241 (5.2031)	Training Prec@1_up 98.633 (98.748)	Training Prec@1_down 95.703 (96.871)	
2021-07-30 07:44:30,678: ============================================================
2021-07-30 07:45:59,092: Epoch: 63/65 eta: 0:43:23.005638	Training Loss2 1.9240 (1.9295)	Training Loss3 2.9234 (2.9585)	Training Loss4 2.5014 (2.5621)	Training Total_Loss 5.1361 (5.2043)	Training Prec@1_up 98.438 (98.743)	Training Prec@1_down 96.680 (96.872)	
2021-07-30 07:45:59,092: ============================================================
2021-07-30 07:45:59,095: Save Checkpoint...
2021-07-30 07:45:59,098: ============================================================
2021-07-30 07:45:59,782: Save done!
2021-07-30 07:45:59,783: ============================================================
2021-07-30 07:48:14,951: time cost, forward:0.026437535430445816, backward:0.17886827449605922, data cost:1.0287785650503756 
2021-07-30 07:48:14,952: ============================================================
2021-07-30 07:48:14,952: Epoch 64/65 Batch 100/964 eta: 0:41:12.210000	Training Loss2 1.9431 (1.9372)	Training Loss3 3.0427 (2.9719)	Training Loss4 2.5687 (2.5722)	Training Total_Loss 5.2986 (5.2266)	Training Prec@1_up 97.852 (98.769)	Training Prec@1_down 96.484 (96.838)	
2021-07-30 07:48:14,952: ============================================================
2021-07-30 07:50:29,136: time cost, forward:0.02586068699707338, backward:0.17910220395380527, data cost:1.0177846206492516 
2021-07-30 07:50:29,137: ============================================================
2021-07-30 07:50:29,137: Epoch 64/65 Batch 200/964 eta: 0:38:40.063258	Training Loss2 1.7547 (1.9342)	Training Loss3 2.7539 (2.9651)	Training Loss4 2.3561 (2.5705)	Training Total_Loss 4.8093 (5.2175)	Training Prec@1_up 99.805 (98.762)	Training Prec@1_down 97.461 (96.841)	
2021-07-30 07:50:29,137: ============================================================
2021-07-30 07:52:42,734: time cost, forward:0.02579610642780827, backward:0.1789776066872587, data cost:1.0123576089291269 
2021-07-30 07:52:42,734: ============================================================
2021-07-30 07:52:42,735: Epoch 64/65 Batch 300/964 eta: 0:36:16.301991	Training Loss2 1.8364 (1.9312)	Training Loss3 2.7391 (2.9622)	Training Loss4 2.4839 (2.5678)	Training Total_Loss 4.8993 (5.2116)	Training Prec@1_up 99.023 (98.778)	Training Prec@1_down 97.266 (96.847)	
2021-07-30 07:52:42,735: ============================================================
2021-07-30 07:54:55,525: time cost, forward:0.025396773689671567, backward:0.17880373730097798, data cost:1.0076509609557034 
2021-07-30 07:54:55,525: ============================================================
2021-07-30 07:54:55,526: Epoch 64/65 Batch 400/964 eta: 0:33:50.375577	Training Loss2 2.1319 (1.9273)	Training Loss3 3.2477 (2.9562)	Training Loss4 2.6989 (2.5618)	Training Total_Loss 5.6630 (5.2008)	Training Prec@1_up 97.656 (98.787)	Training Prec@1_down 95.312 (96.886)	
2021-07-30 07:54:55,526: ============================================================
2021-07-30 07:57:09,508: time cost, forward:0.024787764749928325, backward:0.1787283133887098, data cost:1.0071662002670503 
2021-07-30 07:57:09,509: ============================================================
2021-07-30 07:57:09,509: Epoch 64/65 Batch 500/964 eta: 0:31:54.618280	Training Loss2 2.1494 (1.9246)	Training Loss3 3.2575 (2.9547)	Training Loss4 2.7817 (2.5582)	Training Total_Loss 5.7231 (5.1962)	Training Prec@1_up 97.656 (98.787)	Training Prec@1_down 96.094 (96.896)	
2021-07-30 07:57:09,509: ============================================================
2021-07-30 07:59:22,112: time cost, forward:0.024273506588052232, backward:0.17866764124327392, data cost:1.004558046592496 
2021-07-30 07:59:22,113: ============================================================
2021-07-30 07:59:22,113: Epoch 64/65 Batch 600/964 eta: 0:29:22.310460	Training Loss2 2.1018 (1.9243)	Training Loss3 3.2452 (2.9562)	Training Loss4 2.7463 (2.5572)	Training Total_Loss 5.6693 (5.1970)	Training Prec@1_up 98.828 (98.774)	Training Prec@1_down 96.094 (96.886)	
2021-07-30 07:59:22,113: ============================================================
2021-07-30 08:01:36,049: time cost, forward:0.023838964143024492, backward:0.1786016507210138, data cost:1.0046475564631951 
2021-07-30 08:01:36,049: ============================================================
2021-07-30 08:01:36,049: Epoch 64/65 Batch 700/964 eta: 0:27:26.080052	Training Loss2 1.9187 (1.9257)	Training Loss3 2.9809 (2.9555)	Training Loss4 2.5535 (2.5584)	Training Total_Loss 5.2170 (5.1976)	Training Prec@1_up 98.047 (98.762)	Training Prec@1_down 97.070 (96.890)	
2021-07-30 08:01:36,050: ============================================================
2021-07-30 08:03:49,299: time cost, forward:0.023530713906127012, backward:0.17856859205959735, data cost:1.0037649608822132 
2021-07-30 08:03:49,299: ============================================================
2021-07-30 08:03:49,299: Epoch 64/65 Batch 800/964 eta: 0:25:04.390317	Training Loss2 1.8791 (1.9260)	Training Loss3 2.8727 (2.9549)	Training Loss4 2.5316 (2.5595)	Training Total_Loss 5.0781 (5.1976)	Training Prec@1_up 99.023 (98.753)	Training Prec@1_down 97.656 (96.898)	
2021-07-30 08:03:49,299: ============================================================
2021-07-30 08:06:01,817: time cost, forward:0.02336284103860314, backward:0.17856569921347137, data cost:1.0022136610792793 
2021-07-30 08:06:01,817: ============================================================
2021-07-30 08:06:01,817: Epoch 64/65 Batch 900/964 eta: 0:22:43.606995	Training Loss2 1.9612 (1.9280)	Training Loss3 2.9639 (2.9574)	Training Loss4 2.6073 (2.5613)	Training Total_Loss 5.2481 (5.2020)	Training Prec@1_up 99.219 (98.750)	Training Prec@1_down 97.656 (96.882)	
2021-07-30 08:06:01,817: ============================================================
2021-07-30 08:07:30,834: Epoch: 64/65 eta: 0:21:17.470499	Training Loss2 2.2601 (1.9294)	Training Loss3 3.3491 (2.9583)	Training Loss4 2.9089 (2.5627)	Training Total_Loss 5.9336 (5.2044)	Training Prec@1_up 96.875 (98.744)	Training Prec@1_down 94.336 (96.873)	
2021-07-30 08:07:30,834: ============================================================
2021-07-30 08:07:30,837: Save Checkpoint...
2021-07-30 08:07:30,839: ============================================================
2021-07-30 08:07:31,516: Save done!
2021-07-30 08:07:31,516: ============================================================
2021-07-30 08:09:47,138: time cost, forward:0.024649160076873473, backward:0.17821006823067714, data cost:1.0155904052233455 
2021-07-30 08:09:47,138: ============================================================
2021-07-30 08:09:47,139: Epoch 65/65 Batch 100/964 eta: 0:19:33.115857	Training Loss2 1.7337 (1.9252)	Training Loss3 2.7099 (2.9450)	Training Loss4 2.3829 (2.5576)	Training Total_Loss 4.7682 (5.1864)	Training Prec@1_up 99.023 (98.682)	Training Prec@1_down 97.461 (96.891)	
2021-07-30 08:09:47,139: ============================================================
2021-07-30 08:12:00,001: time cost, forward:0.024435046929210873, backward:0.17850334800068457, data cost:1.0053631911924736 
2021-07-30 08:12:00,001: ============================================================
2021-07-30 08:12:00,002: Epoch 65/65 Batch 200/964 eta: 0:16:56.401267	Training Loss2 1.6980 (1.9308)	Training Loss3 2.5297 (2.9545)	Training Loss4 2.2821 (2.5627)	Training Total_Loss 4.5198 (5.2012)	Training Prec@1_up 99.023 (98.721)	Training Prec@1_down 98.242 (96.865)	
2021-07-30 08:12:00,002: ============================================================
2021-07-30 08:14:13,047: time cost, forward:0.02419097287997753, backward:0.17869024053465163, data cost:1.0008515976743155 
2021-07-30 08:14:13,047: ============================================================
2021-07-30 08:14:13,047: Epoch 65/65 Batch 300/964 eta: 0:14:44.755283	Training Loss2 2.0005 (1.9277)	Training Loss3 3.0476 (2.9533)	Training Loss4 2.6796 (2.5607)	Training Total_Loss 5.3877 (5.1975)	Training Prec@1_up 98.242 (98.748)	Training Prec@1_down 96.680 (96.857)	
2021-07-30 08:14:13,048: ============================================================
2021-07-30 08:16:28,862: time cost, forward:0.02434261998437102, backward:0.178587311790103, data cost:1.0063728222572117 
2021-07-30 08:16:28,863: ============================================================
2021-07-30 08:16:28,863: Epoch 65/65 Batch 400/964 eta: 0:12:47.359720	Training Loss2 2.0715 (1.9279)	Training Loss3 3.0845 (2.9575)	Training Loss4 2.7353 (2.5613)	Training Total_Loss 5.4879 (5.2021)	Training Prec@1_up 98.438 (98.754)	Training Prec@1_down 96.094 (96.859)	
2021-07-30 08:16:28,864: ============================================================
2021-07-30 08:18:41,847: time cost, forward:0.02457183014175935, backward:0.1785521478595619, data cost:1.0040553385365703 
2021-07-30 08:18:41,847: ============================================================
2021-07-30 08:18:41,847: Epoch 65/65 Batch 500/964 eta: 0:10:18.375934	Training Loss2 2.1669 (1.9267)	Training Loss3 3.3169 (2.9571)	Training Loss4 2.8330 (2.5585)	Training Total_Loss 5.8168 (5.1997)	Training Prec@1_up 97.656 (98.763)	Training Prec@1_down 95.898 (96.859)	
2021-07-30 08:18:41,848: ============================================================
2021-07-30 08:20:54,852: time cost, forward:0.024365825525707315, backward:0.17847607728833945, data cost:1.0024564859266074 
2021-07-30 08:20:54,853: ============================================================
2021-07-30 08:20:54,853: Epoch 65/65 Batch 600/964 eta: 0:08:05.470423	Training Loss2 1.9068 (1.9282)	Training Loss3 2.8397 (2.9576)	Training Loss4 2.5918 (2.5614)	Training Total_Loss 5.0890 (5.2025)	Training Prec@1_up 99.414 (98.757)	Training Prec@1_down 97.070 (96.864)	
2021-07-30 08:20:54,853: ============================================================
2021-07-30 08:23:08,396: time cost, forward:0.024346088987904385, backward:0.17855550802828415, data cost:1.0020512105398764 
2021-07-30 08:23:08,397: ============================================================
2021-07-30 08:23:08,397: Epoch 65/65 Batch 700/964 eta: 0:05:53.891496	Training Loss2 2.0881 (1.9282)	Training Loss3 3.1123 (2.9580)	Training Loss4 2.7011 (2.5611)	Training Total_Loss 5.5069 (5.2027)	Training Prec@1_up 98.047 (98.750)	Training Prec@1_down 96.484 (96.880)	
2021-07-30 08:23:08,397: ============================================================
2021-07-30 08:25:20,811: time cost, forward:0.02431823673176676, backward:0.17846185095766756, data cost:1.0008796648925475 
2021-07-30 08:25:20,812: ============================================================
2021-07-30 08:25:20,812: Epoch 65/65 Batch 800/964 eta: 0:03:38.484605	Training Loss2 2.0176 (1.9295)	Training Loss3 2.9786 (2.9584)	Training Loss4 2.6244 (2.5625)	Training Total_Loss 5.2996 (5.2043)	Training Prec@1_up 98.828 (98.742)	Training Prec@1_down 96.680 (96.876)	
2021-07-30 08:25:20,812: ============================================================
2021-07-30 08:27:33,390: time cost, forward:0.024278632260535796, backward:0.17832129550590134, data cost:1.0012519977514418 
2021-07-30 08:27:33,390: ============================================================
2021-07-30 08:27:33,390: Epoch 65/65 Batch 900/964 eta: 0:01:26.175883	Training Loss2 2.0660 (1.9299)	Training Loss3 3.1000 (2.9592)	Training Loss4 2.6944 (2.5634)	Training Total_Loss 5.4801 (5.2058)	Training Prec@1_up 98.633 (98.739)	Training Prec@1_down 96.289 (96.875)	
2021-07-30 08:27:33,390: ============================================================
2021-07-30 08:29:01,281: Epoch: 65/65 eta: 0:00:00	Training Loss2 2.0596 (1.9293)	Training Loss3 3.2084 (2.9583)	Training Loss4 2.6925 (2.5632)	Training Total_Loss 5.5844 (5.2046)	Training Prec@1_up 98.242 (98.744)	Training Prec@1_down 96.680 (96.872)	
2021-07-30 08:29:01,281: ============================================================
2021-07-30 08:29:01,284: Save Checkpoint...
2021-07-30 08:29:01,285: ============================================================
2021-07-30 08:29:01,977: Save done!
2021-07-30 08:29:01,977: ============================================================
