2021-08-13 20:45:24,338: [('name', 'amsoft-36'), ('backbone_model_name', 'SimpleResnet_36_multiTask'), ('classify_model_name', 'MarginCosineProduct'), ('resume_net_model', None), ('resume_net_classifier', None), ('no_cuda', False), ('gpu_num', 1), ('log_interval', 100), ('log_path', './logs/SR_36_ddp_multiTask.log'), ('log_pic_path', './logs/pic/SR_36_ddp_multiTask/'), ('save_path', 'snapshot/SR_36_ddp_multiTask/'), ('lmdb_path', '/home/ubuntu/data4/lk/data/lmdb_mask_augu_full'), ('batch_size', 512), ('datanum', 3923399), ('num_class', 86876), ('lmdb_workers', 4), ('num_workers', 4), ('start_epoch', 1), ('max_epoch', 42), ('lr', 0.15), ('base', 'epoch'), ('step_size', [10, 20, 30, 40]), ('momentum', 0.9), ('gama', 0.3), ('weight_decay', 0.0005), ('rank', -1), ('dist_url', 'env://'), ('world_size', -1), ('gpu', None), ('dist_backend', 'nccl'), ('distributed', False), ('master_port', 22345), ('multiprocessing_distributed', False), ('SEED', 1337), ('local_rank', -1)]
2021-08-13 20:45:24,338: SimpleResidualBackbone_multiTask(
  (conv1): ConvPrelu(
    (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (prelu): PReLU(num_parameters=64)
  )
  (bam1): BAM(
    (channel_att): ChannelGate(
      (gate_c): Sequential(
        (flatten): Flatten()
        (gate_c_fc_0): Linear(in_features=64, out_features=4, bias=True)
        (gate_c_relu_1): ReLU()
        (gate_c_fc_final): Linear(in_features=4, out_features=64, bias=True)
      )
    )
    (spatial_att): SpatialGate(
      (gate_s): Sequential(
        (gate_s_conv_reduce0): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1))
        (gate_s_relu_reduce0): ReLU()
        (gate_s_conv_di_0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4))
        (gate_s_relu_di_0): ReLU()
        (gate_s_conv_di_1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4))
        (gate_s_relu_di_1): ReLU()
        (gate_s_conv_final): Conv2d(4, 1, kernel_size=(1, 1), stride=(1, 1))
      )
    )
  )
  (bam2): BAM(
    (channel_att): ChannelGate(
      (gate_c): Sequential(
        (flatten): Flatten()
        (gate_c_fc_0): Linear(in_features=128, out_features=8, bias=True)
        (gate_c_relu_1): ReLU()
        (gate_c_fc_final): Linear(in_features=8, out_features=128, bias=True)
      )
    )
    (spatial_att): SpatialGate(
      (gate_s): Sequential(
        (gate_s_conv_reduce0): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1))
        (gate_s_relu_reduce0): ReLU()
        (gate_s_conv_di_0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4))
        (gate_s_relu_di_0): ReLU()
        (gate_s_conv_di_1): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4))
        (gate_s_relu_di_1): ReLU()
        (gate_s_conv_final): Conv2d(8, 1, kernel_size=(1, 1), stride=(1, 1))
      )
    )
  )
  (bam3): BAM(
    (channel_att): ChannelGate(
      (gate_c): Sequential(
        (flatten): Flatten()
        (gate_c_fc_0): Linear(in_features=256, out_features=16, bias=True)
        (gate_c_relu_1): ReLU()
        (gate_c_fc_final): Linear(in_features=16, out_features=256, bias=True)
      )
    )
    (spatial_att): SpatialGate(
      (gate_s): Sequential(
        (gate_s_conv_reduce0): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))
        (gate_s_relu_reduce0): ReLU()
        (gate_s_conv_di_0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4))
        (gate_s_relu_di_0): ReLU()
        (gate_s_conv_di_1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4))
        (gate_s_relu_di_1): ReLU()
        (gate_s_conv_final): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1))
      )
    )
  )
  (layer1): Sequential(
    (0): SimpleResidualUnit(
      (conv1): ConvPrelu(
        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=64)
      )
      (conv2): ConvPrelu(
        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=64)
      )
    )
    (1): SimpleResidualUnit(
      (conv1): ConvPrelu(
        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=64)
      )
      (conv2): ConvPrelu(
        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=64)
      )
    )
  )
  (conv2): ConvPrelu(
    (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (prelu): PReLU(num_parameters=128)
  )
  (layer2): Sequential(
    (0): SimpleResidualUnit(
      (conv1): ConvPrelu(
        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=128)
      )
      (conv2): ConvPrelu(
        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=128)
      )
    )
    (1): SimpleResidualUnit(
      (conv1): ConvPrelu(
        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=128)
      )
      (conv2): ConvPrelu(
        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=128)
      )
    )
    (2): SimpleResidualUnit(
      (conv1): ConvPrelu(
        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=128)
      )
      (conv2): ConvPrelu(
        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=128)
      )
    )
    (3): SimpleResidualUnit(
      (conv1): ConvPrelu(
        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=128)
      )
      (conv2): ConvPrelu(
        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=128)
      )
    )
  )
  (conv3): ConvPrelu(
    (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (prelu): PReLU(num_parameters=256)
  )
  (layer3): Sequential(
    (0): SimpleResidualUnit(
      (conv1): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
      (conv2): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
    )
    (1): SimpleResidualUnit(
      (conv1): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
      (conv2): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
    )
    (2): SimpleResidualUnit(
      (conv1): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
      (conv2): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
    )
    (3): SimpleResidualUnit(
      (conv1): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
      (conv2): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
    )
    (4): SimpleResidualUnit(
      (conv1): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
      (conv2): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
    )
    (5): SimpleResidualUnit(
      (conv1): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
      (conv2): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
    )
    (6): SimpleResidualUnit(
      (conv1): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
      (conv2): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
    )
    (7): SimpleResidualUnit(
      (conv1): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
      (conv2): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
    )
  )
  (conv4): ConvPrelu(
    (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (prelu): PReLU(num_parameters=512)
  )
  (layer4): Sequential(
    (0): SimpleResidualUnit(
      (conv1): ConvPrelu(
        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=512)
      )
      (conv2): ConvPrelu(
        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=512)
      )
    )
    (1): SimpleResidualUnit(
      (conv1): ConvPrelu(
        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=512)
      )
      (conv2): ConvPrelu(
        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=512)
      )
    )
  )
  (fc5): Linear(in_features=25088, out_features=512, bias=True)
  (fc5_mask): Linear(in_features=25088, out_features=512, bias=True)
  (prelu_mask): PReLU(num_parameters=512)
  (mask_classify): Linear(in_features=512, out_features=2, bias=False)
)
2021-08-13 20:45:38,669: Use DP
2021-08-13 20:47:57,584: time cost, forward:0.7341353869197345, backward:0.5292027430100874, data cost:0.13555927950926502 
2021-08-13 20:47:57,584: ============================================================
2021-08-13 20:47:57,584: Epoch 1/42 Batch 100/7662 eta: 5 days, 3:55:59.508256	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.002)	
2021-08-13 20:47:57,584: ============================================================
2021-08-13 20:50:06,824: time cost, forward:0.7091697867791257, backward:0.5203645468956262, data cost:0.11541719173067179 
2021-08-13 20:50:06,825: ============================================================
2021-08-13 20:50:06,825: Epoch 1/42 Batch 200/7662 eta: 4 days, 19:27:24.455110	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.001)	
2021-08-13 20:50:06,825: ============================================================
2021-08-13 20:52:13,101: time cost, forward:0.694744761572235, backward:0.5136781679746698, data cost:0.10880756617389793 
2021-08-13 20:52:13,102: ============================================================
2021-08-13 20:52:13,102: Epoch 1/42 Batch 300/7662 eta: 4 days, 16:46:26.505356	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.001)	Training Prec@5 0.000 (0.002)	
2021-08-13 20:52:13,102: ============================================================
2021-08-13 20:54:18,731: time cost, forward:0.6871046380590377, backward:0.5093065419591459, data cost:0.10536839848472959 
2021-08-13 20:54:18,731: ============================================================
2021-08-13 20:54:18,732: Epoch 1/42 Batch 400/7662 eta: 4 days, 16:09:40.904931	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.002)	
2021-08-13 20:54:18,732: ============================================================
2021-08-13 20:56:22,818: time cost, forward:0.679570160791248, backward:0.5064641067642487, data cost:0.10339911476165832 
2021-08-13 20:56:22,818: ============================================================
2021-08-13 20:56:22,818: Epoch 1/42 Batch 500/7662 eta: 4 days, 14:44:56.762122	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.002)	
2021-08-13 20:56:22,819: ============================================================
2021-08-13 20:58:26,643: time cost, forward:0.6742020099111312, backward:0.504574829827565, data cost:0.10199156348812759 
2021-08-13 20:58:26,643: ============================================================
2021-08-13 20:58:26,643: Epoch 1/42 Batch 600/7662 eta: 4 days, 14:28:50.997057	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.002)	
2021-08-13 20:58:26,643: ============================================================
2021-08-13 21:00:30,455: time cost, forward:0.6703655006888939, backward:0.5032199779805195, data cost:0.10098558877500172 
2021-08-13 21:00:30,455: ============================================================
2021-08-13 21:00:30,456: Epoch 1/42 Batch 700/7662 eta: 4 days, 14:26:08.170232	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.002)	
2021-08-13 21:00:30,456: ============================================================
2021-08-13 21:02:34,475: time cost, forward:0.6677053493910349, backward:0.5022359517996242, data cost:0.10023970031022131 
2021-08-13 21:02:34,475: ============================================================
2021-08-13 21:02:34,475: Epoch 1/42 Batch 800/7662 eta: 4 days, 14:35:08.884809	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.002)	
2021-08-13 21:02:34,475: ============================================================
2021-08-13 21:04:38,549: time cost, forward:0.6656718132095422, backward:0.5015052892474895, data cost:0.09965130802786788 
2021-08-13 21:04:38,550: ============================================================
2021-08-13 21:04:38,550: Epoch 1/42 Batch 900/7662 eta: 4 days, 14:36:01.680761	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.002)	
2021-08-13 21:04:38,550: ============================================================
2021-08-13 21:06:43,158: time cost, forward:0.664641411096842, backward:0.5010039565322159, data cost:0.09903544658893818 
2021-08-13 21:06:43,158: ============================================================
2021-08-13 21:06:43,158: Epoch 1/42 Batch 1000/7662 eta: 4 days, 15:02:30.795595	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2021-08-13 21:06:43,159: ============================================================
2021-08-13 21:08:47,640: time cost, forward:0.6637127718348412, backward:0.5005296821264488, data cost:0.0985620820164355 
2021-08-13 21:08:47,640: ============================================================
2021-08-13 21:08:47,640: Epoch 1/42 Batch 1100/7662 eta: 4 days, 14:53:39.468579	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2021-08-13 21:08:47,640: ============================================================
2021-08-13 21:10:51,339: time cost, forward:0.662258389992352, backward:0.5001065341306786, data cost:0.09823623630183254 
2021-08-13 21:10:51,339: ============================================================
2021-08-13 21:10:51,339: Epoch 1/42 Batch 1200/7662 eta: 4 days, 14:09:44.425655	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2021-08-13 21:10:51,339: ============================================================
2021-08-13 21:12:55,399: time cost, forward:0.6612647825979654, backward:0.4997830302830932, data cost:0.09796125398772418 
2021-08-13 21:12:55,400: ============================================================
2021-08-13 21:12:55,400: Epoch 1/42 Batch 1300/7662 eta: 4 days, 14:27:01.346194	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2021-08-13 21:12:55,400: ============================================================
2021-08-13 21:15:00,085: time cost, forward:0.6609364494926339, backward:0.4994042509363923, data cost:0.0977488082165885 
2021-08-13 21:15:00,085: ============================================================
2021-08-13 21:15:00,085: Epoch 1/42 Batch 1400/7662 eta: 4 days, 14:58:18.717298	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2021-08-13 21:15:00,086: ============================================================
2021-08-13 21:17:04,821: time cost, forward:0.6606934738604524, backward:0.4990935651678654, data cost:0.09753500818808608 
2021-08-13 21:17:04,821: ============================================================
2021-08-13 21:17:04,821: Epoch 1/42 Batch 1500/7662 eta: 4 days, 14:58:54.843146	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2021-08-13 21:17:04,821: ============================================================
2021-08-13 21:19:09,557: time cost, forward:0.6604779485317228, backward:0.4988557669130842, data cost:0.09732081265357079 
2021-08-13 21:19:09,557: ============================================================
2021-08-13 21:19:09,558: Epoch 1/42 Batch 1600/7662 eta: 4 days, 14:56:51.927409	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2021-08-13 21:19:09,558: ============================================================
2021-08-13 21:21:14,277: time cost, forward:0.6602195410815458, backward:0.4986514451575321, data cost:0.09718850530407722 
2021-08-13 21:21:14,278: ============================================================
2021-08-13 21:21:14,278: Epoch 1/42 Batch 1700/7662 eta: 4 days, 14:53:55.998609	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2021-08-13 21:21:14,278: ============================================================
2021-08-13 21:23:19,140: time cost, forward:0.6601035832166009, backward:0.49847141473143547, data cost:0.09703299666590794 
2021-08-13 21:23:19,140: ============================================================
2021-08-13 21:23:19,140: Epoch 1/42 Batch 1800/7662 eta: 4 days, 14:59:25.499858	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2021-08-13 21:23:19,140: ============================================================
2021-08-13 21:25:23,893: time cost, forward:0.6599664247431462, backward:0.4982708984453343, data cost:0.09690852238040902 
2021-08-13 21:25:23,893: ============================================================
2021-08-13 21:25:23,894: Epoch 1/42 Batch 1900/7662 eta: 4 days, 14:51:32.769550	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2021-08-13 21:25:23,894: ============================================================
2021-08-13 21:27:28,939: time cost, forward:0.6599671637910554, backward:0.4981086150355909, data cost:0.09680074354957019 
2021-08-13 21:27:28,939: ============================================================
2021-08-13 21:27:28,940: Epoch 1/42 Batch 2000/7662 eta: 4 days, 15:05:03.001187	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2021-08-13 21:27:28,940: ============================================================
2021-08-13 21:29:33,458: time cost, forward:0.6597065591653112, backward:0.4979543665467472, data cost:0.09672166314109158 
2021-08-13 21:29:33,458: ============================================================
2021-08-13 21:29:33,458: Epoch 1/42 Batch 2100/7662 eta: 4 days, 14:34:51.989620	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2021-08-13 21:29:33,458: ============================================================
2021-08-13 21:31:37,793: time cost, forward:0.6593919456303255, backward:0.4978274826355119, data cost:0.09663192951554544 
2021-08-13 21:31:37,793: ============================================================
2021-08-13 21:31:37,793: Epoch 1/42 Batch 2200/7662 eta: 4 days, 14:23:00.831914	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2021-08-13 21:31:37,793: ============================================================
2021-08-13 21:33:42,074: time cost, forward:0.6590724926816219, backward:0.4977325156959776, data cost:0.09653631414004438 
2021-08-13 21:33:42,074: ============================================================
2021-08-13 21:33:42,074: Epoch 1/42 Batch 2300/7662 eta: 4 days, 14:18:04.389607	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2021-08-13 21:33:42,074: ============================================================
2021-08-13 21:35:46,229: time cost, forward:0.658715388336595, backward:0.49763969512421075, data cost:0.09646722981213231 
2021-08-13 21:35:46,229: ============================================================
2021-08-13 21:35:46,229: Epoch 1/42 Batch 2400/7662 eta: 4 days, 14:09:17.480223	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.004)	
2021-08-13 21:35:46,229: ============================================================
2021-08-13 21:37:50,879: time cost, forward:0.6585934842381777, backward:0.4975677170053202, data cost:0.09638044995372416 
2021-08-13 21:37:50,880: ============================================================
2021-08-13 21:37:50,880: Epoch 1/42 Batch 2500/7662 eta: 4 days, 14:33:35.670513	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.004)	
2021-08-13 21:37:50,880: ============================================================
2021-08-13 21:39:55,692: time cost, forward:0.6585606964517896, backward:0.49748676499296673, data cost:0.0962977620352686 
2021-08-13 21:39:55,692: ============================================================
2021-08-13 21:39:55,692: Epoch 1/42 Batch 2600/7662 eta: 4 days, 14:40:07.851737	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.004)	
2021-08-13 21:39:55,693: ============================================================
2021-08-13 21:42:00,182: time cost, forward:0.6583872080820938, backward:0.4974038455167405, data cost:0.09625645238587308 
2021-08-13 21:42:00,183: ============================================================
2021-08-13 21:42:00,183: Epoch 1/42 Batch 2700/7662 eta: 4 days, 14:20:54.905526	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2021-08-13 21:42:00,183: ============================================================
2021-08-13 21:44:05,302: time cost, forward:0.6584704145443103, backward:0.4973204493650414, data cost:0.09619840803892539 
2021-08-13 21:44:05,303: ============================================================
2021-08-13 21:44:05,303: Epoch 1/42 Batch 2800/7662 eta: 4 days, 14:52:18.762056	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2021-08-13 21:44:05,303: ============================================================
2021-08-13 21:46:10,369: time cost, forward:0.6585395057351888, backward:0.4972310726623693, data cost:0.09615077245395814 
2021-08-13 21:46:10,369: ============================================================
2021-08-13 21:46:10,369: Epoch 1/42 Batch 2900/7662 eta: 4 days, 14:47:23.713990	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2021-08-13 21:46:10,369: ============================================================
2021-08-13 21:48:15,484: time cost, forward:0.6586122446037921, backward:0.4971599039056771, data cost:0.0960987746298174 
2021-08-13 21:48:15,484: ============================================================
2021-08-13 21:48:15,484: Epoch 1/42 Batch 3000/7662 eta: 4 days, 14:47:52.203861	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2021-08-13 21:48:15,484: ============================================================
2021-08-13 21:50:20,239: time cost, forward:0.6585450267822521, backward:0.49710720775588246, data cost:0.09605728275893619 
2021-08-13 21:50:20,239: ============================================================
2021-08-13 21:50:20,239: Epoch 1/42 Batch 3100/7662 eta: 4 days, 14:26:41.402564	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2021-08-13 21:50:20,240: ============================================================
2021-08-13 21:52:25,014: time cost, forward:0.6585139406960843, backward:0.49702606241417585, data cost:0.09602548771256318 
2021-08-13 21:52:25,014: ============================================================
2021-08-13 21:52:25,014: Epoch 1/42 Batch 3200/7662 eta: 4 days, 14:25:38.669700	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2021-08-13 21:52:25,014: ============================================================
2021-08-13 21:54:30,006: time cost, forward:0.6585309190365646, backward:0.4969906217945674, data cost:0.09597274707136244 
2021-08-13 21:54:30,006: ============================================================
2021-08-13 21:54:30,006: Epoch 1/42 Batch 3300/7662 eta: 4 days, 14:35:05.710697	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2021-08-13 21:54:30,006: ============================================================
2021-08-13 21:56:34,592: time cost, forward:0.6584205130823713, backward:0.49694820914979754, data cost:0.09594076554471516 
2021-08-13 21:56:34,592: ============================================================
2021-08-13 21:56:34,592: Epoch 1/42 Batch 3400/7662 eta: 4 days, 14:11:27.776865	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2021-08-13 21:56:34,592: ============================================================
2021-08-13 21:58:38,412: time cost, forward:0.6581017824539834, backward:0.4968967618994046, data cost:0.09591714066415216 
2021-08-13 21:58:38,412: ============================================================
2021-08-13 21:58:38,412: Epoch 1/42 Batch 3500/7662 eta: 4 days, 13:28:45.776704	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2021-08-13 21:58:38,412: ============================================================
2021-08-13 22:00:42,554: time cost, forward:0.6578969265825717, backward:0.49684099675417276, data cost:0.0958945571664109 
2021-08-13 22:00:42,554: ============================================================
2021-08-13 22:00:42,554: Epoch 1/42 Batch 3600/7662 eta: 4 days, 13:43:45.888619	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2021-08-13 22:00:42,554: ============================================================
2021-08-13 22:02:47,220: time cost, forward:0.6577923772269954, backward:0.49682854278308697, data cost:0.09588689712551228 
2021-08-13 22:02:47,220: ============================================================
2021-08-13 22:02:47,221: Epoch 1/42 Batch 3700/7662 eta: 4 days, 14:09:29.890794	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2021-08-13 22:02:47,221: ============================================================
2021-08-13 22:04:52,156: time cost, forward:0.6578215406141208, backward:0.4967930041416346, data cost:0.0958440355891835 
2021-08-13 22:04:52,157: ============================================================
2021-08-13 22:04:52,157: Epoch 1/42 Batch 3800/7662 eta: 4 days, 14:21:43.321561	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2021-08-13 22:04:52,157: ============================================================
2021-08-13 22:06:56,823: time cost, forward:0.657779300417708, backward:0.49676063012453064, data cost:0.09580584684070119 
2021-08-13 22:06:56,823: ============================================================
2021-08-13 22:06:56,824: Epoch 1/42 Batch 3900/7662 eta: 4 days, 14:05:21.901931	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2021-08-13 22:06:56,824: ============================================================
2021-08-13 22:09:01,663: time cost, forward:0.6577709540929697, backward:0.4967514415716165, data cost:0.09575611169590417 
2021-08-13 22:09:01,663: ============================================================
2021-08-13 22:09:01,664: Epoch 1/42 Batch 4000/7662 eta: 4 days, 14:12:28.053444	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2021-08-13 22:09:01,664: ============================================================
2021-08-13 22:11:06,276: time cost, forward:0.6577303671900835, backward:0.4967030952488047, data cost:0.09572843855024693 
2021-08-13 22:11:06,277: ============================================================
2021-08-13 22:11:06,277: Epoch 1/42 Batch 4100/7662 eta: 4 days, 13:58:22.100270	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2021-08-13 22:11:06,277: ============================================================
2021-08-13 22:13:10,683: time cost, forward:0.6576103043289349, backward:0.49669853401456626, data cost:0.0956913794072136 
2021-08-13 22:13:10,684: ============================================================
2021-08-13 22:13:10,684: Epoch 1/42 Batch 4200/7662 eta: 4 days, 13:45:23.090326	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2021-08-13 22:13:10,684: ============================================================
2021-08-13 22:15:15,228: time cost, forward:0.6575242820299623, backward:0.4966873551391119, data cost:0.09566697510876582 
2021-08-13 22:15:15,229: ============================================================
2021-08-13 22:15:15,229: Epoch 1/42 Batch 4300/7662 eta: 4 days, 13:50:36.747834	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2021-08-13 22:15:15,229: ============================================================
2021-08-13 22:17:19,903: time cost, forward:0.6574916960461948, backward:0.49664505911729745, data cost:0.09565560106094059 
2021-08-13 22:17:19,904: ============================================================
2021-08-13 22:17:19,904: Epoch 1/42 Batch 4400/7662 eta: 4 days, 13:55:24.552680	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2021-08-13 22:17:19,904: ============================================================
2021-08-13 22:19:24,591: time cost, forward:0.6574693500902897, backward:0.4966106395187259, data cost:0.09563365658804374 
2021-08-13 22:19:24,592: ============================================================
2021-08-13 22:19:24,592: Epoch 1/42 Batch 4500/7662 eta: 4 days, 13:54:01.274928	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2021-08-13 22:19:24,592: ============================================================
2021-08-13 22:21:28,711: time cost, forward:0.6572967953048444, backward:0.4965976718509423, data cost:0.09561841636669327 
2021-08-13 22:21:28,711: ============================================================
2021-08-13 22:21:28,711: Epoch 1/42 Batch 4600/7662 eta: 4 days, 13:21:52.663782	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2021-08-13 22:21:28,711: ============================================================
2021-08-13 22:23:33,675: time cost, forward:0.6573313775278705, backward:0.49658189425698795, data cost:0.09558713732944901 
2021-08-13 22:23:33,675: ============================================================
2021-08-13 22:23:33,675: Epoch 1/42 Batch 4700/7662 eta: 4 days, 14:04:26.949267	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2021-08-13 22:23:33,675: ============================================================
2021-08-13 22:25:37,750: time cost, forward:0.657181987466353, backward:0.49655087412981025, data cost:0.09557134848282868 
2021-08-13 22:25:37,751: ============================================================
2021-08-13 22:25:37,751: Epoch 1/42 Batch 4800/7662 eta: 4 days, 13:15:25.966593	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2021-08-13 22:25:37,751: ============================================================
2021-08-13 22:27:41,637: time cost, forward:0.6569887348330004, backward:0.49652634043673105, data cost:0.09556188581427352 
2021-08-13 22:27:41,637: ============================================================
2021-08-13 22:27:41,637: Epoch 1/42 Batch 4900/7662 eta: 4 days, 13:03:21.617111	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2021-08-13 22:27:41,637: ============================================================
2021-08-13 22:29:45,729: time cost, forward:0.6568439206640156, backward:0.49651405190820574, data cost:0.09554357785276232 
2021-08-13 22:29:45,729: ============================================================
2021-08-13 22:29:45,730: Epoch 1/42 Batch 5000/7662 eta: 4 days, 13:12:11.689419	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2021-08-13 22:29:45,730: ============================================================
2021-08-13 22:31:49,902: time cost, forward:0.6567254570985126, backward:0.49649078926868967, data cost:0.09553070773468646 
2021-08-13 22:31:49,902: ============================================================
2021-08-13 22:31:49,903: Epoch 1/42 Batch 5100/7662 eta: 4 days, 13:14:21.996512	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2021-08-13 22:31:49,903: ============================================================
2021-08-13 22:33:54,225: time cost, forward:0.656649100182033, backward:0.49646254281948154, data cost:0.095516743305028 
2021-08-13 22:33:54,226: ============================================================
2021-08-13 22:33:54,226: Epoch 1/42 Batch 5200/7662 eta: 4 days, 13:20:13.295970	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2021-08-13 22:33:54,226: ============================================================
2021-08-13 22:35:58,630: time cost, forward:0.6565950327626847, backward:0.49642359618759085, data cost:0.09551025237828341 
2021-08-13 22:35:58,630: ============================================================
2021-08-13 22:35:58,631: Epoch 1/42 Batch 5300/7662 eta: 4 days, 13:22:27.356204	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2021-08-13 22:35:58,631: ============================================================
2021-08-13 22:38:02,829: time cost, forward:0.6564809789832465, backward:0.49642379876970694, data cost:0.09549042272841539 
2021-08-13 22:38:02,829: ============================================================
2021-08-13 22:38:02,829: Epoch 1/42 Batch 5400/7662 eta: 4 days, 13:09:30.780408	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2021-08-13 22:38:02,829: ============================================================
2021-08-13 22:40:07,700: time cost, forward:0.6565060545301672, backward:0.4964143295985262, data cost:0.09546829076393407 
2021-08-13 22:40:07,700: ============================================================
2021-08-13 22:40:07,701: Epoch 1/42 Batch 5500/7662 eta: 4 days, 13:42:54.639002	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2021-08-13 22:40:07,701: ============================================================
2021-08-13 22:42:12,669: time cost, forward:0.6565483321588281, backward:0.49635943141786687, data cost:0.09549060257913215 
2021-08-13 22:42:12,669: ============================================================
2021-08-13 22:42:12,669: Epoch 1/42 Batch 5600/7662 eta: 4 days, 13:45:56.645105	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2021-08-13 22:42:12,669: ============================================================
2021-08-13 22:44:17,758: time cost, forward:0.6566134178966865, backward:0.49628141813684834, data cost:0.09553535102982127 
2021-08-13 22:44:17,758: ============================================================
2021-08-13 22:44:17,759: Epoch 1/42 Batch 5700/7662 eta: 4 days, 13:50:13.755338	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2021-08-13 22:44:17,759: ============================================================
2021-08-13 22:46:22,585: time cost, forward:0.6566352973746563, backward:0.4962020521432002, data cost:0.0955782231842162 
2021-08-13 22:46:22,585: ============================================================
2021-08-13 22:46:22,585: Epoch 1/42 Batch 5800/7662 eta: 4 days, 13:34:18.383456	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2021-08-13 22:46:22,585: ============================================================
2021-08-13 22:48:27,614: time cost, forward:0.6566854194981261, backward:0.49612828678429216, data cost:0.09562185093960049 
2021-08-13 22:48:27,614: ============================================================
2021-08-13 22:48:27,614: Epoch 1/42 Batch 5900/7662 eta: 4 days, 13:42:53.794268	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2021-08-13 22:48:27,615: ============================================================
2021-08-13 22:50:32,346: time cost, forward:0.6566845892588086, backward:0.49605604365858, data cost:0.09566519558081966 
2021-08-13 22:50:32,347: ============================================================
2021-08-13 22:50:32,347: Epoch 1/42 Batch 6000/7662 eta: 4 days, 13:25:11.356666	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.004)	
2021-08-13 22:50:32,347: ============================================================
2021-08-13 22:52:36,946: time cost, forward:0.6566603587560721, backward:0.49598757848131364, data cost:0.09570698891336517 
2021-08-13 22:52:36,946: ============================================================
2021-08-13 22:52:36,946: Epoch 1/42 Batch 6100/7662 eta: 4 days, 13:16:06.607414	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2021-08-13 22:52:36,946: ============================================================
2021-08-13 22:54:41,444: time cost, forward:0.656628363131323, backward:0.49591655999657647, data cost:0.09574467210081974 
2021-08-13 22:54:41,444: ============================================================
2021-08-13 22:54:41,444: Epoch 1/42 Batch 6200/7662 eta: 4 days, 13:08:42.072127	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2021-08-13 22:54:41,444: ============================================================
2021-08-13 22:56:46,578: time cost, forward:0.6566902641493814, backward:0.49591556161637346, data cost:0.09572286673208365 
2021-08-13 22:56:46,578: ============================================================
2021-08-13 22:56:46,579: Epoch 1/42 Batch 6300/7662 eta: 4 days, 13:40:04.556463	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2021-08-13 22:56:46,579: ============================================================
2021-08-13 22:58:51,488: time cost, forward:0.6567144011273797, backward:0.4959109303801856, data cost:0.09570521610866284 
2021-08-13 22:58:51,488: ============================================================
2021-08-13 22:58:51,488: Epoch 1/42 Batch 6400/7662 eta: 4 days, 13:26:11.263810	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2021-08-13 22:58:51,488: ============================================================
2021-08-13 23:00:56,039: time cost, forward:0.6566768337128841, backward:0.495908170466167, data cost:0.09569105850327729 
2021-08-13 23:00:56,040: ============================================================
2021-08-13 23:00:56,040: Epoch 1/42 Batch 6500/7662 eta: 4 days, 13:05:17.226109	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.004)	
2021-08-13 23:00:56,040: ============================================================
2021-08-13 23:03:00,645: time cost, forward:0.6566543407486432, backward:0.4958997954923396, data cost:0.09567838745562303 
2021-08-13 23:03:00,645: ============================================================
2021-08-13 23:03:00,645: Epoch 1/42 Batch 6600/7662 eta: 4 days, 13:06:02.525739	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.004)	
2021-08-13 23:03:00,645: ============================================================
2021-08-13 23:05:05,179: time cost, forward:0.6566190998987362, backward:0.4958936102551513, data cost:0.09566681497861136 
2021-08-13 23:05:05,180: ============================================================
2021-08-13 23:05:05,180: Epoch 1/42 Batch 6700/7662 eta: 4 days, 13:00:15.324609	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.004)	
2021-08-13 23:05:05,180: ============================================================
2021-08-13 23:07:09,896: time cost, forward:0.6566108792191797, backward:0.4958922882994618, data cost:0.09565168044938464 
2021-08-13 23:07:09,897: ============================================================
2021-08-13 23:07:09,897: Epoch 1/42 Batch 6800/7662 eta: 4 days, 13:07:44.448661	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.004)	
2021-08-13 23:07:09,897: ============================================================
2021-08-13 23:09:14,218: time cost, forward:0.6565525852816574, backward:0.4958828526107206, data cost:0.09563916575167383 
2021-08-13 23:09:14,219: ============================================================
2021-08-13 23:09:14,219: Epoch 1/42 Batch 6900/7662 eta: 4 days, 12:44:56.518526	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.004)	
2021-08-13 23:09:14,219: ============================================================
2021-08-13 23:11:18,920: time cost, forward:0.6565556324180628, backward:0.49587207348759915, data cost:0.09562189321140507 
2021-08-13 23:11:18,920: ============================================================
2021-08-13 23:11:18,921: Epoch 1/42 Batch 7000/7662 eta: 4 days, 13:02:46.761784	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.004)	
2021-08-13 23:11:18,921: ============================================================
2021-08-13 23:13:23,468: time cost, forward:0.6565257969633393, backward:0.4958671045430967, data cost:0.09560797398888674 
2021-08-13 23:13:23,469: ============================================================
2021-08-13 23:13:23,469: Epoch 1/42 Batch 7100/7662 eta: 4 days, 12:52:39.931688	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.004)	
2021-08-13 23:13:23,469: ============================================================
2021-08-13 23:15:27,527: time cost, forward:0.6564340972622197, backward:0.49585403609828893, data cost:0.09560052718300176 
2021-08-13 23:15:27,527: ============================================================
2021-08-13 23:15:27,527: Epoch 1/42 Batch 7200/7662 eta: 4 days, 12:24:53.769288	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.004)	
2021-08-13 23:15:27,527: ============================================================
2021-08-13 23:17:31,508: time cost, forward:0.6563290352200071, backward:0.49584073526497685, data cost:0.09559894427973566 
2021-08-13 23:17:31,508: ============================================================
2021-08-13 23:17:31,508: Epoch 1/42 Batch 7300/7662 eta: 4 days, 12:18:45.814018	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.004)	
2021-08-13 23:17:31,508: ============================================================
2021-08-13 23:19:35,894: time cost, forward:0.6562867242591673, backward:0.4958352093310562, data cost:0.095585143152065 
2021-08-13 23:19:35,894: ============================================================
2021-08-13 23:19:35,894: Epoch 1/42 Batch 7400/7662 eta: 4 days, 12:37:57.093853	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.004)	
2021-08-13 23:19:35,895: ============================================================
2021-08-13 23:21:40,191: time cost, forward:0.6562392905261106, backward:0.4958173736824578, data cost:0.0955771348495168 
2021-08-13 23:21:40,191: ============================================================
2021-08-13 23:21:40,191: Epoch 1/42 Batch 7500/7662 eta: 4 days, 12:31:11.669148	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.004)	
2021-08-13 23:21:40,192: ============================================================
2021-08-13 23:23:45,361: time cost, forward:0.6563025474987589, backward:0.49580727889202414, data cost:0.09556907370680648 
2021-08-13 23:23:45,362: ============================================================
2021-08-13 23:23:45,362: Epoch 1/42 Batch 7600/7662 eta: 4 days, 13:14:51.693224	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.004)	
2021-08-13 23:23:45,362: ============================================================
2021-08-13 23:25:04,908: Epoch: 1/42 eta: 4 days, 13:13:32.835865	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.004)
2021-08-13 23:25:04,908: ============================================================
2021-08-13 23:27:12,733: time cost, forward:0.6555176575978597, backward:0.49451652440157806, data cost:0.13829957355152478 
2021-08-13 23:27:12,734: ============================================================
2021-08-13 23:27:12,734: Epoch 2/42 Batch 100/7662 eta: 4 days, 15:24:23.804781	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.002)	
2021-08-13 23:27:12,734: ============================================================
2021-08-13 23:29:17,270: time cost, forward:0.6547745951456041, backward:0.4947857904673821, data cost:0.11682664808915488 
2021-08-13 23:29:17,270: ============================================================
2021-08-13 23:29:17,271: Epoch 2/42 Batch 200/7662 eta: 4 days, 12:36:14.007354	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.002)	
2021-08-13 23:29:17,271: ============================================================
2021-08-13 23:31:22,331: time cost, forward:0.6563893369209008, backward:0.4949865093996692, data cost:0.10950842111007027 
2021-08-13 23:31:22,331: ============================================================
2021-08-13 23:31:22,331: Epoch 2/42 Batch 300/7662 eta: 4 days, 13:01:34.763573	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.002)	
2021-08-13 23:31:22,331: ============================================================
2021-08-13 23:33:27,445: time cost, forward:0.6574206943798783, backward:0.4949569355575064, data cost:0.10587681803786964 
2021-08-13 23:33:27,445: ============================================================
2021-08-13 23:33:27,445: Epoch 2/42 Batch 400/7662 eta: 4 days, 13:02:16.369404	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2021-08-13 23:33:27,446: ============================================================
2021-08-13 23:35:32,741: time cost, forward:0.6583871005293362, backward:0.4949360381147427, data cost:0.10372771433216775 
2021-08-13 23:35:32,741: ============================================================
2021-08-13 23:35:32,741: Epoch 2/42 Batch 500/7662 eta: 4 days, 13:09:42.118336	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.004)	
2021-08-13 23:35:32,741: ============================================================
2021-08-13 23:37:38,314: time cost, forward:0.6594744484890284, backward:0.49493364579291493, data cost:0.10230778413941347 
2021-08-13 23:37:38,314: ============================================================
2021-08-13 23:37:38,315: Epoch 2/42 Batch 600/7662 eta: 4 days, 13:22:05.865557	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.004)	
2021-08-13 23:37:38,315: ============================================================
2021-08-13 23:39:43,974: time cost, forward:0.6603496313436178, backward:0.4950196204096804, data cost:0.10122184214503298 
2021-08-13 23:39:43,975: ============================================================
2021-08-13 23:39:43,975: Epoch 2/42 Batch 700/7662 eta: 4 days, 13:24:32.961313	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.004)	
2021-08-13 23:39:43,975: ============================================================
2021-08-13 23:41:49,756: time cost, forward:0.6611840519051677, backward:0.49503816143890494, data cost:0.10043619243015485 
2021-08-13 23:41:49,756: ============================================================
2021-08-13 23:41:49,756: Epoch 2/42 Batch 800/7662 eta: 4 days, 13:28:47.964391	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.004)	
2021-08-13 23:41:49,756: ============================================================
2021-08-13 23:43:55,097: time cost, forward:0.6613122992573909, backward:0.49503698046666234, data cost:0.09986506367684471 
2021-08-13 23:43:55,097: ============================================================
2021-08-13 23:43:55,097: Epoch 2/42 Batch 900/7662 eta: 4 days, 13:03:41.153089	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2021-08-13 23:43:55,097: ============================================================
2021-08-13 23:46:00,521: time cost, forward:0.6615428055848207, backward:0.49502198474185244, data cost:0.09937686676735634 
2021-08-13 23:46:00,521: ============================================================
2021-08-13 23:46:00,521: Epoch 2/42 Batch 1000/7662 eta: 4 days, 13:05:57.267784	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.004)	
2021-08-13 23:46:00,522: ============================================================
2021-08-13 23:48:07,044: time cost, forward:0.6627024680077759, backward:0.49507140918034004, data cost:0.09893695065061867 
2021-08-13 23:48:07,044: ============================================================
2021-08-13 23:48:07,044: Epoch 2/42 Batch 1100/7662 eta: 4 days, 14:01:10.350749	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.004)	
2021-08-13 23:48:07,044: ============================================================
2021-08-13 23:50:13,146: time cost, forward:0.6633428244714045, backward:0.4951131017731069, data cost:0.0985534612689046 
2021-08-13 23:50:13,147: ============================================================
2021-08-13 23:50:13,147: Epoch 2/42 Batch 1200/7662 eta: 4 days, 13:37:09.658249	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.004)	
2021-08-13 23:50:13,147: ============================================================
2021-08-13 23:52:20,602: time cost, forward:0.6649192818501438, backward:0.49531945455432214, data cost:0.09806988384285002 
2021-08-13 23:52:20,602: ============================================================
2021-08-13 23:52:20,603: Epoch 2/42 Batch 1300/7662 eta: 4 days, 14:45:36.591831	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.004)	
2021-08-13 23:52:20,603: ============================================================
2021-08-13 23:54:27,737: time cost, forward:0.6659984728368032, backward:0.49556628255182883, data cost:0.09761903199065661 
2021-08-13 23:54:27,738: ============================================================
2021-08-13 23:54:27,738: Epoch 2/42 Batch 1400/7662 eta: 4 days, 14:26:46.439356	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.004)	
2021-08-13 23:54:27,738: ============================================================
2021-08-13 23:56:34,508: time cost, forward:0.6667008584463413, backward:0.49573164561973404, data cost:0.09727223926261713 
2021-08-13 23:56:34,508: ============================================================
2021-08-13 23:56:34,509: Epoch 2/42 Batch 1500/7662 eta: 4 days, 14:05:39.665863	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.004)	
2021-08-13 23:56:34,509: ============================================================
2021-08-13 23:58:42,806: time cost, forward:0.6682710969649381, backward:0.4958892815406804, data cost:0.09695598168101738 
2021-08-13 23:58:42,806: ============================================================
2021-08-13 23:58:42,807: Epoch 2/42 Batch 1600/7662 eta: 4 days, 15:23:06.402050	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.004)	
2021-08-13 23:58:42,807: ============================================================
2021-08-14 00:00:52,642: time cost, forward:0.6705859837074571, backward:0.4960130818666185, data cost:0.09666161175122186 
2021-08-14 00:00:52,642: ============================================================
2021-08-14 00:00:52,642: Epoch 2/42 Batch 1700/7662 eta: 4 days, 16:41:02.393123	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.004)	
2021-08-14 00:00:52,642: ============================================================
2021-08-14 00:03:01,255: time cost, forward:0.671944843258309, backward:0.49609760193774405, data cost:0.09644481723609402 
2021-08-14 00:03:01,255: ============================================================
2021-08-14 00:03:01,255: Epoch 2/42 Batch 1800/7662 eta: 4 days, 15:35:14.025521	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.004)	
2021-08-14 00:03:01,255: ============================================================
2021-08-14 00:05:09,406: time cost, forward:0.6729206904541385, backward:0.4961606110316192, data cost:0.09626317714753435 
2021-08-14 00:05:09,406: ============================================================
2021-08-14 00:05:09,406: Epoch 2/42 Batch 1900/7662 eta: 4 days, 15:09:02.061723	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.004)	
2021-08-14 00:05:09,406: ============================================================
2021-08-14 00:07:18,475: time cost, forward:0.6742862078355156, backward:0.49626112043887394, data cost:0.09602329586672151 
2021-08-14 00:07:18,475: ============================================================
2021-08-14 00:07:18,475: Epoch 2/42 Batch 2000/7662 eta: 4 days, 15:54:40.085346	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.004)	
2021-08-14 00:07:18,475: ============================================================
2021-08-14 00:09:26,542: time cost, forward:0.6750141224899765, backward:0.4963045541646084, data cost:0.09589174090254131 
2021-08-14 00:09:26,542: ============================================================
2021-08-14 00:09:26,542: Epoch 2/42 Batch 2100/7662 eta: 4 days, 15:00:23.927008	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.004)	
2021-08-14 00:09:26,542: ============================================================
2021-08-14 00:11:36,121: time cost, forward:0.6763658360927525, backward:0.4964103317087268, data cost:0.09570232300717162 
2021-08-14 00:11:36,122: ============================================================
2021-08-14 00:11:36,122: Epoch 2/42 Batch 2200/7662 eta: 4 days, 16:16:55.742881	Training Loss1 nan (nan)	Training Loss2 nan (nan)	Training Total_Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.004)	
2021-08-14 00:11:36,122: ============================================================
