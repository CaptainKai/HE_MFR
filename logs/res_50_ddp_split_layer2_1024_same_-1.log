2021-04-12 15:26:04,477: [('name', 'resnet50_split_layer2_1024_same'), ('backbone_model_name', 'resnet50_split'), ('classify_model_name', 'MarginCosineProduct'), ('resume_net_model', None), ('resume_net_classifier', None), ('no_cuda', False), ('gpu_num', 1), ('log_interval', 100), ('log_path', './logs/res_50_ddp_split_layer2_1024_same.log'), ('log_pic_path', './logs/pic/res_50_ddp_split_layer2_1024_same/'), ('save_path', 'snapshot/res_50_ddp_split_layer2_1024_same/'), ('lmdb_path', '/home/ubuntu/data4/lk/data/lmdb_default'), ('batch_size', 512), ('datanum', 3923399), ('num_class', 86876), ('lmdb_workers', 4), ('num_workers', 4), ('start_epoch', 1), ('max_epoch', 38), ('lr', 0.1), ('base', 'epoch'), ('step_size', [10, 20, 30, 40, 50, 60]), ('momentum', 0.9), ('gama', 0.1), ('weight_decay', 0.0005), ('rank', -1), ('dist_url', 'env://'), ('world_size', -1), ('gpu', None), ('dist_backend', 'nccl'), ('distributed', False), ('master_port', 32345), ('multiprocessing_distributed', False), ('SEED', 1337), ('local_rank', -1)]
2021-04-12 15:26:11,003: Use DP
2021-04-12 15:27:07,604: time cost, forward:0.04787230732465031, backward:0.4743495396893434, data cost:0.046339316801591354 
2021-04-12 15:27:07,605: ============================================================
2021-04-12 15:27:07,605: Epoch 1/38 Batch 100/7662 eta: 1 day, 21:37:13.855697	Training Loss 22.6854 (22.6014)	Training Loss2 23.9914 (23.6751)	Training Loss3 24.0368 (23.6336)	Training Total_Loss 23.5712 (23.3034)	Training Prec@1 0.000 (0.006)	Training Prec@5 0.000 (0.028)	Training Prec@1_up 0.000 (0.006)	Training Prec@1_down 0.000 (0.006)	
2021-04-12 15:27:07,605: ============================================================
2021-04-12 15:27:59,638: time cost, forward:0.044712016330891516, backward:0.46452903987175254, data cost:0.03454620394874458 
2021-04-12 15:27:59,638: ============================================================
2021-04-12 15:27:59,639: Epoch 1/38 Batch 200/7662 eta: 1 day, 18:03:15.512811	Training Loss 22.7051 (22.6957)	Training Loss2 23.5915 (23.9047)	Training Loss3 23.8529 (23.8665)	Training Total_Loss 23.3831 (23.4890)	Training Prec@1 0.000 (0.006)	Training Prec@5 0.195 (0.034)	Training Prec@1_up 0.000 (0.008)	Training Prec@1_down 0.000 (0.008)	
2021-04-12 15:27:59,639: ============================================================
2021-04-12 15:28:52,080: time cost, forward:0.04470805978296592, backward:0.4614496183235908, data cost:0.03070133824810934 
2021-04-12 15:28:52,081: ============================================================
2021-04-12 15:28:52,081: Epoch 1/38 Batch 300/7662 eta: 1 day, 18:22:12.456491	Training Loss 22.5505 (22.6671)	Training Loss2 23.4280 (23.7939)	Training Loss3 23.2721 (23.7646)	Training Total_Loss 23.0835 (23.4085)	Training Prec@1 0.000 (0.007)	Training Prec@5 0.000 (0.039)	Training Prec@1_up 0.000 (0.006)	Training Prec@1_down 0.000 (0.007)	
2021-04-12 15:28:52,081: ============================================================
2021-04-12 15:29:44,722: time cost, forward:0.045146157568259945, backward:0.4600548929439153, data cost:0.028775834798215327 
2021-04-12 15:29:44,723: ============================================================
2021-04-12 15:29:44,723: Epoch 1/38 Batch 400/7662 eta: 1 day, 18:30:59.723066	Training Loss 22.3043 (22.6006)	Training Loss2 22.9796 (23.6364)	Training Loss3 23.0653 (23.6092)	Training Total_Loss 22.7831 (23.2821)	Training Prec@1 0.000 (0.011)	Training Prec@5 0.000 (0.050)	Training Prec@1_up 0.000 (0.007)	Training Prec@1_down 0.000 (0.009)	
2021-04-12 15:29:44,723: ============================================================
2021-04-12 15:30:37,482: time cost, forward:0.0449592231031888, backward:0.4598251478466577, data cost:0.027723102149122464 
2021-04-12 15:30:37,483: ============================================================
2021-04-12 15:30:37,483: Epoch 1/38 Batch 500/7662 eta: 1 day, 18:35:50.583971	Training Loss 22.0413 (22.5068)	Training Loss2 22.7051 (23.4626)	Training Loss3 22.6967 (23.4412)	Training Total_Loss 22.4811 (23.1368)	Training Prec@1 0.000 (0.020)	Training Prec@5 0.000 (0.076)	Training Prec@1_up 0.000 (0.013)	Training Prec@1_down 0.000 (0.013)	
2021-04-12 15:30:37,483: ============================================================
2021-04-12 15:31:30,279: time cost, forward:0.04503993796983824, backward:0.4594827948110132, data cost:0.02703117528224429 
2021-04-12 15:31:30,279: ============================================================
2021-04-12 15:31:30,279: Epoch 1/38 Batch 600/7662 eta: 1 day, 18:36:44.208357	Training Loss 21.6334 (22.4009)	Training Loss2 22.1548 (23.2906)	Training Loss3 22.1307 (23.2702)	Training Total_Loss 21.9730 (22.9872)	Training Prec@1 0.195 (0.032)	Training Prec@5 0.391 (0.121)	Training Prec@1_up 0.195 (0.022)	Training Prec@1_down 0.000 (0.022)	
2021-04-12 15:31:30,280: ============================================================
2021-04-12 15:32:23,112: time cost, forward:0.044773231419029835, backward:0.4595994724224565, data cost:0.02658438000385683 
2021-04-12 15:32:23,112: ============================================================
2021-04-12 15:32:23,112: Epoch 1/38 Batch 700/7662 eta: 1 day, 18:37:37.389718	Training Loss 21.2857 (22.2749)	Training Loss2 21.6939 (23.1106)	Training Loss3 21.7325 (23.0896)	Training Total_Loss 21.5707 (22.8250)	Training Prec@1 0.391 (0.060)	Training Prec@5 0.977 (0.205)	Training Prec@1_up 0.391 (0.038)	Training Prec@1_down 0.000 (0.039)	
2021-04-12 15:32:23,113: ============================================================
2021-04-12 15:33:16,011: time cost, forward:0.04454194648990942, backward:0.45975331251552615, data cost:0.02629659710002035 
2021-04-12 15:33:16,012: ============================================================
2021-04-12 15:33:16,012: Epoch 1/38 Batch 800/7662 eta: 1 day, 18:39:58.129168	Training Loss 20.9050 (22.1362)	Training Loss2 21.3096 (22.9277)	Training Loss3 21.3464 (22.9093)	Training Total_Loss 21.1870 (22.6577)	Training Prec@1 0.195 (0.108)	Training Prec@5 0.781 (0.317)	Training Prec@1_up 0.195 (0.069)	Training Prec@1_down 0.000 (0.071)	
2021-04-12 15:33:16,012: ============================================================
2021-04-12 15:34:08,925: time cost, forward:0.044177729507972455, backward:0.4600203066434425, data cost:0.02609300666444161 
2021-04-12 15:34:08,925: ============================================================
2021-04-12 15:34:08,925: Epoch 1/38 Batch 900/7662 eta: 1 day, 18:39:43.974603	Training Loss 20.5239 (21.9835)	Training Loss2 20.8868 (22.7372)	Training Loss3 20.9489 (22.7211)	Training Total_Loss 20.7866 (22.4806)	Training Prec@1 1.367 (0.187)	Training Prec@5 2.148 (0.514)	Training Prec@1_up 0.977 (0.123)	Training Prec@1_down 1.172 (0.123)	
2021-04-12 15:34:08,925: ============================================================
2021-04-12 15:35:01,857: time cost, forward:0.04419660257982897, backward:0.4599646591209435, data cost:0.02592701573032994 
2021-04-12 15:35:01,857: ============================================================
2021-04-12 15:35:01,857: Epoch 1/38 Batch 1000/7662 eta: 1 day, 18:39:45.839369	Training Loss 20.2099 (21.8245)	Training Loss2 20.6166 (22.5457)	Training Loss3 20.6913 (22.5328)	Training Total_Loss 20.5059 (22.3010)	Training Prec@1 2.148 (0.295)	Training Prec@5 4.492 (0.789)	Training Prec@1_up 1.172 (0.208)	Training Prec@1_down 0.781 (0.205)	
2021-04-12 15:35:01,857: ============================================================
2021-04-12 15:35:54,652: time cost, forward:0.04402439913172631, backward:0.4600041600332356, data cost:0.025765458923561992 
2021-04-12 15:35:54,652: ============================================================
2021-04-12 15:35:54,653: Epoch 1/38 Batch 1100/7662 eta: 1 day, 18:32:16.288379	Training Loss 19.4988 (21.6568)	Training Loss2 19.8438 (22.3512)	Training Loss3 20.0812 (22.3408)	Training Total_Loss 19.8079 (22.1163)	Training Prec@1 2.930 (0.471)	Training Prec@5 8.594 (1.176)	Training Prec@1_up 1.758 (0.345)	Training Prec@1_down 3.320 (0.336)	
2021-04-12 15:35:54,653: ============================================================
2021-04-12 15:36:47,517: time cost, forward:0.043970627025130195, backward:0.4600081233007894, data cost:0.02561979635841554 
2021-04-12 15:36:47,517: ============================================================
2021-04-12 15:36:47,517: Epoch 1/38 Batch 1200/7662 eta: 1 day, 18:34:44.577639	Training Loss 19.2925 (21.4785)	Training Loss2 19.6845 (22.1495)	Training Loss3 19.7358 (22.1422)	Training Total_Loss 19.5709 (21.9234)	Training Prec@1 4.102 (0.722)	Training Prec@5 8.789 (1.702)	Training Prec@1_up 2.930 (0.538)	Training Prec@1_down 2.344 (0.518)	
2021-04-12 15:36:47,517: ============================================================
2021-04-12 15:37:40,369: time cost, forward:0.04381969894602998, backward:0.4601073712913507, data cost:0.025495705006212524 
2021-04-12 15:37:40,370: ============================================================
2021-04-12 15:37:40,370: Epoch 1/38 Batch 1300/7662 eta: 1 day, 18:33:17.663886	Training Loss 18.9327 (21.2963)	Training Loss2 19.3671 (21.9466)	Training Loss3 19.4435 (21.9430)	Training Total_Loss 19.2478 (21.7286)	Training Prec@1 6.641 (1.044)	Training Prec@5 12.695 (2.338)	Training Prec@1_up 5.664 (0.790)	Training Prec@1_down 5.469 (0.760)	
2021-04-12 15:37:40,370: ============================================================
2021-04-12 15:38:33,256: time cost, forward:0.043734552350702074, backward:0.4601520525718945, data cost:0.02541455615836437 
2021-04-12 15:38:33,256: ============================================================
2021-04-12 15:38:33,257: Epoch 1/38 Batch 1400/7662 eta: 1 day, 18:34:02.405237	Training Loss 18.2534 (21.1108)	Training Loss2 18.6068 (21.7436)	Training Loss3 18.7245 (21.7434)	Training Total_Loss 18.5283 (21.5326)	Training Prec@1 8.789 (1.444)	Training Prec@5 14.453 (3.083)	Training Prec@1_up 7.031 (1.110)	Training Prec@1_down 6.641 (1.075)	
2021-04-12 15:38:33,257: ============================================================
2021-04-12 15:39:26,126: time cost, forward:0.043588047587450066, backward:0.4602429664795044, data cost:0.02535401701211452 
2021-04-12 15:39:26,127: ============================================================
2021-04-12 15:39:26,127: Epoch 1/38 Batch 1500/7662 eta: 1 day, 18:32:23.338551	Training Loss 17.8287 (20.9199)	Training Loss2 18.1869 (21.5378)	Training Loss3 18.2985 (21.5404)	Training Total_Loss 18.1047 (21.3327)	Training Prec@1 12.695 (1.951)	Training Prec@5 20.703 (3.966)	Training Prec@1_up 11.328 (1.521)	Training Prec@1_down 9.570 (1.475)	
2021-04-12 15:39:26,127: ============================================================
2021-04-12 15:40:18,976: time cost, forward:0.043582387832345776, backward:0.46022232477928865, data cost:0.025269300509721804 
2021-04-12 15:40:18,976: ============================================================
2021-04-12 15:40:18,977: Epoch 1/38 Batch 1600/7662 eta: 1 day, 18:30:29.496270	Training Loss 17.6094 (20.7257)	Training Loss2 18.0260 (21.3313)	Training Loss3 17.9904 (21.3354)	Training Total_Loss 17.8753 (21.1308)	Training Prec@1 13.672 (2.556)	Training Prec@5 21.289 (4.968)	Training Prec@1_up 10.156 (2.013)	Training Prec@1_down 9.961 (1.961)	
2021-04-12 15:40:18,977: ============================================================
2021-04-12 15:41:11,825: time cost, forward:0.04357222741740532, backward:0.46021448464306614, data cost:0.025189682902694802 
2021-04-12 15:41:11,825: ============================================================
2021-04-12 15:41:11,825: Epoch 1/38 Batch 1700/7662 eta: 1 day, 18:29:33.768949	Training Loss 17.0999 (20.5288)	Training Loss2 17.5474 (21.1243)	Training Loss3 17.5787 (21.1292)	Training Total_Loss 17.4087 (20.9275)	Training Prec@1 16.406 (3.253)	Training Prec@5 27.539 (6.079)	Training Prec@1_up 11.914 (2.583)	Training Prec@1_down 13.672 (2.525)	
2021-04-12 15:41:11,825: ============================================================
