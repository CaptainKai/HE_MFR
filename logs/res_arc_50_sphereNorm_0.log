2022-04-05 14:39:17,385: [('name', 'SResnet_split_layer4'), ('backbone_model_name', 'resnet50'), ('classify_model_name', 'ArcFace'), ('resume_net_model', None), ('resume_net_classifier', None), ('no_cuda', False), ('gpu_num', 1), ('log_interval', 100), ('log_path', './logs/res_arc_50_sphereNorm.log'), ('log_pic_path', './logs/pic/res_arc_50_sphereNorm/'), ('save_path', 'snapshot/res_arc_50_sphereNorm/'), ('lmdb_path', '/home/ubuntu/data4/lk/data/lmdb_default'), ('batch_size', 512), ('datanum', 3923399), ('num_class', 86876), ('lmdb_workers', 4), ('num_workers', 4), ('start_epoch', 1), ('max_epoch', 38), ('lr', 0.05), ('base', 'epoch'), ('step_size', [10, 20, 30, 40, 50, 60]), ('momentum', 0.9), ('gama', 0.1), ('weight_decay', 0.0005), ('rank', 0), ('dist_url', 'env://'), ('world_size', 2), ('gpu', 0), ('dist_backend', 'nccl'), ('distributed', True), ('master_port', 32323), ('multiprocessing_distributed', False), ('SEED', 1312), ('local_rank', 0)]
2022-04-05 14:39:17,385: ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (bn_o1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dropout): Dropout(p=0, inplace=False)
  (fc): Linear(in_features=32768, out_features=512, bias=True)
)
2022-04-05 14:39:18,325: data balance
2022-04-05 14:39:48,606: time cost, forward:0.07135286475672867, backward:0.1327470312214861, data cost:0.09793431108648126 
2022-04-05 14:39:48,607: ============================================================
2022-04-05 14:39:48,607: Epoch 1/38 Batch 100/7662 eta: 1 day, 0:20:48.182833	Training Loss 43.0387 (43.8556)	Training Prec@1 0.000 (0.002)	Training Prec@5 0.000 (0.016)	
2022-04-05 14:39:48,607: ============================================================
2022-04-05 14:40:16,508: time cost, forward:0.06827154950280885, backward:0.12912711665857976, data cost:0.09283390835901002 
2022-04-05 14:40:16,509: ============================================================
2022-04-05 14:40:16,509: Epoch 1/38 Batch 200/7662 eta: 22:33:02.513301	Training Loss 42.8142 (43.3779)	Training Prec@1 0.000 (0.003)	Training Prec@5 0.000 (0.014)	
2022-04-05 14:40:16,509: ============================================================
2022-04-05 14:40:44,637: time cost, forward:0.06720922543452336, backward:0.12796704984428892, data cost:0.09191752436966402 
2022-04-05 14:40:44,638: ============================================================
2022-04-05 14:40:44,638: Epoch 1/38 Batch 300/7662 eta: 22:43:35.648026	Training Loss 42.6760 (43.1626)	Training Prec@1 0.000 (0.005)	Training Prec@5 0.000 (0.014)	
2022-04-05 14:40:44,638: ============================================================
2022-04-05 14:41:12,954: time cost, forward:0.06673382159163778, backward:0.12759150180003995, data cost:0.09165573837165546 
2022-04-05 14:41:12,955: ============================================================
2022-04-05 14:41:12,955: Epoch 1/38 Batch 400/7662 eta: 22:52:13.509700	Training Loss 42.6444 (43.0390)	Training Prec@1 0.000 (0.004)	Training Prec@5 0.000 (0.015)	
2022-04-05 14:41:12,955: ============================================================
2022-04-05 14:41:41,466: time cost, forward:0.06642222213362883, backward:0.1273515472909014, data cost:0.09193840963329246 
2022-04-05 14:41:41,466: ============================================================
2022-04-05 14:41:41,466: Epoch 1/38 Batch 500/7662 eta: 23:01:10.264818	Training Loss 42.5966 (42.9583)	Training Prec@1 0.000 (0.004)	Training Prec@5 0.000 (0.014)	
2022-04-05 14:41:41,467: ============================================================
2022-04-05 14:42:10,503: time cost, forward:0.06627442721333449, backward:0.1271885428484374, data cost:0.09291385091803905 
2022-04-05 14:42:10,503: ============================================================
2022-04-05 14:42:10,503: Epoch 1/38 Batch 600/7662 eta: 23:26:08.572474	Training Loss 42.5182 (42.8993)	Training Prec@1 0.000 (0.004)	Training Prec@5 0.000 (0.013)	
2022-04-05 14:42:10,503: ============================================================
2022-04-05 14:42:39,868: time cost, forward:0.06617132686238432, backward:0.12699499628233465, data cost:0.09418978097612765 
2022-04-05 14:42:39,868: ============================================================
2022-04-05 14:42:39,868: Epoch 1/38 Batch 700/7662 eta: 23:41:32.909984	Training Loss 42.5338 (42.8536)	Training Prec@1 0.195 (0.004)	Training Prec@5 0.195 (0.013)	
2022-04-05 14:42:39,869: ============================================================
2022-04-05 14:43:09,482: time cost, forward:0.06611520119095327, backward:0.1269711899071074, data cost:0.09530926765279567 
2022-04-05 14:43:09,483: ============================================================
2022-04-05 14:43:09,483: Epoch 1/38 Batch 800/7662 eta: 23:53:08.321480	Training Loss 42.5690 (42.8153)	Training Prec@1 0.000 (0.003)	Training Prec@5 0.000 (0.013)	
2022-04-05 14:43:09,483: ============================================================
2022-04-05 14:43:39,439: time cost, forward:0.06604671557832746, backward:0.12688170656876782, data cost:0.09665607027005035 
2022-04-05 14:43:39,439: ============================================================
2022-04-05 14:43:39,439: Epoch 1/38 Batch 900/7662 eta: 1 day, 0:09:10.335257	Training Loss 42.4208 (42.7812)	Training Prec@1 0.000 (0.003)	Training Prec@5 0.000 (0.012)	
2022-04-05 14:43:39,440: ============================================================
2022-04-05 14:44:09,731: time cost, forward:0.06601652177843126, backward:0.12678845222289856, data cost:0.09807084463499449 
2022-04-05 14:44:09,732: ============================================================
2022-04-05 14:44:09,732: Epoch 1/38 Batch 1000/7662 eta: 1 day, 0:24:56.386923	Training Loss 42.4909 (42.7489)	Training Prec@1 0.000 (0.004)	Training Prec@5 0.000 (0.012)	
2022-04-05 14:44:09,732: ============================================================
2022-04-05 14:44:40,474: time cost, forward:0.06597847239986347, backward:0.12666296199628502, data cost:0.09968056322988106 
2022-04-05 14:44:40,475: ============================================================
2022-04-05 14:44:40,475: Epoch 1/38 Batch 1100/7662 eta: 1 day, 0:46:11.845321	Training Loss 42.4242 (42.7175)	Training Prec@1 0.000 (0.004)	Training Prec@5 0.000 (0.013)	
2022-04-05 14:44:40,475: ============================================================
2022-04-05 14:45:11,140: time cost, forward:0.0659755417662327, backward:0.12660552900567265, data cost:0.10090713783340517 
2022-04-05 14:45:11,140: ============================================================
2022-04-05 14:45:11,141: Epoch 1/38 Batch 1200/7662 eta: 1 day, 0:41:56.842429	Training Loss 42.2820 (42.6862)	Training Prec@1 0.000 (0.004)	Training Prec@5 0.195 (0.014)	
2022-04-05 14:45:11,141: ============================================================
2022-04-05 14:45:42,460: time cost, forward:0.06597606120795999, backward:0.12647270018362466, data cost:0.1025280737711706 
2022-04-05 14:45:42,461: ============================================================
2022-04-05 14:45:42,461: Epoch 1/38 Batch 1300/7662 eta: 1 day, 1:13:04.505204	Training Loss 42.2204 (42.6531)	Training Prec@1 0.000 (0.004)	Training Prec@5 0.195 (0.014)	
2022-04-05 14:45:42,461: ============================================================
2022-04-05 14:46:14,022: time cost, forward:0.0660021034115293, backward:0.12636919069324246, data cost:0.10402582542141307 
2022-04-05 14:46:14,022: ============================================================
2022-04-05 14:46:14,022: Epoch 1/38 Batch 1400/7662 eta: 1 day, 1:24:11.168727	Training Loss 42.1437 (42.6182)	Training Prec@1 0.000 (0.004)	Training Prec@5 0.000 (0.015)	
2022-04-05 14:46:14,022: ============================================================
2022-04-05 14:46:47,871: time cost, forward:0.06600707192831314, backward:0.12619977636763538, data cost:0.1069934757810342 
2022-04-05 14:46:47,872: ============================================================
2022-04-05 14:46:47,872: Epoch 1/38 Batch 1500/7662 eta: 1 day, 3:14:07.818966	Training Loss 41.9483 (42.5802)	Training Prec@1 0.000 (0.004)	Training Prec@5 0.000 (0.015)	
2022-04-05 14:46:47,872: ============================================================
2022-04-05 14:47:20,369: time cost, forward:0.06602599234637653, backward:0.12614809326114618, data cost:0.10855454456217219 
2022-04-05 14:47:20,369: ============================================================
2022-04-05 14:47:20,369: Epoch 1/38 Batch 1600/7662 eta: 1 day, 2:08:18.308872	Training Loss 41.8819 (42.5381)	Training Prec@1 0.000 (0.004)	Training Prec@5 0.000 (0.017)	
2022-04-05 14:47:20,369: ============================================================
2022-04-05 14:47:53,301: time cost, forward:0.06602761715983839, backward:0.12603875943813134, data cost:0.11034960084413345 
2022-04-05 14:47:53,301: ============================================================
2022-04-05 14:47:53,301: Epoch 1/38 Batch 1700/7662 eta: 1 day, 2:28:44.449675	Training Loss 41.6454 (42.4920)	Training Prec@1 0.000 (0.005)	Training Prec@5 0.000 (0.019)	
2022-04-05 14:47:53,302: ============================================================
2022-04-05 14:48:27,549: time cost, forward:0.06603518719803035, backward:0.1259125037084625, data cost:0.11261979959751911 
2022-04-05 14:48:27,549: ============================================================
2022-04-05 14:48:27,549: Epoch 1/38 Batch 1800/7662 eta: 1 day, 3:31:38.818828	Training Loss 41.5363 (42.4406)	Training Prec@1 0.000 (0.005)	Training Prec@5 0.000 (0.021)	
2022-04-05 14:48:27,550: ============================================================
2022-04-05 14:49:00,975: time cost, forward:0.06604145199703881, backward:0.12581330201950996, data cost:0.11430788115490857 
2022-04-05 14:49:00,975: ============================================================
2022-04-05 14:49:00,975: Epoch 1/38 Batch 1900/7662 eta: 1 day, 2:51:26.827925	Training Loss 41.2604 (42.3835)	Training Prec@1 0.000 (0.006)	Training Prec@5 0.000 (0.023)	
2022-04-05 14:49:00,975: ============================================================
2022-04-05 14:49:34,452: time cost, forward:0.06604039185997723, backward:0.12570195105029322, data cost:0.11581935090622704 
2022-04-05 14:49:34,453: ============================================================
2022-04-05 14:49:34,453: Epoch 1/38 Batch 2000/7662 eta: 1 day, 2:53:22.919942	Training Loss 41.0954 (42.3205)	Training Prec@1 0.000 (0.007)	Training Prec@5 0.000 (0.026)	
2022-04-05 14:49:34,453: ============================================================
2022-04-05 14:50:07,992: time cost, forward:0.06604878297017948, backward:0.1256463280060338, data cost:0.11717335458821827 
2022-04-05 14:50:07,993: ============================================================
2022-04-05 14:50:07,993: Epoch 1/38 Batch 2100/7662 eta: 1 day, 2:55:49.834159	Training Loss 40.7591 (42.2510)	Training Prec@1 0.000 (0.007)	Training Prec@5 0.000 (0.028)	
2022-04-05 14:50:07,993: ============================================================
2022-04-05 14:50:40,836: time cost, forward:0.06605300755867258, backward:0.12558276602331753, data cost:0.1181016031207145 
2022-04-05 14:50:40,837: ============================================================
2022-04-05 14:50:40,837: Epoch 1/38 Batch 2200/7662 eta: 1 day, 2:21:44.409071	Training Loss 40.4529 (42.1746)	Training Prec@1 0.000 (0.008)	Training Prec@5 0.000 (0.030)	
2022-04-05 14:50:40,837: ============================================================
2022-04-05 14:51:14,403: time cost, forward:0.06606733970094526, backward:0.12552708913679483, data cost:0.11924404909217705 
2022-04-05 14:51:14,404: ============================================================
2022-04-05 14:51:14,404: Epoch 1/38 Batch 2300/7662 eta: 1 day, 2:56:00.938158	Training Loss 39.9978 (42.0899)	Training Prec@1 0.000 (0.008)	Training Prec@5 0.000 (0.033)	
2022-04-05 14:51:14,404: ============================================================
2022-04-05 14:51:48,458: time cost, forward:0.06609065505851056, backward:0.12546759617731143, data cost:0.12050495499519072 
2022-04-05 14:51:48,459: ============================================================
2022-04-05 14:51:48,459: Epoch 1/38 Batch 2400/7662 eta: 1 day, 3:18:56.025807	Training Loss 39.7200 (41.9967)	Training Prec@1 0.000 (0.009)	Training Prec@5 0.000 (0.036)	
2022-04-05 14:51:48,459: ============================================================
2022-04-05 14:52:23,055: time cost, forward:0.06611553322271901, backward:0.12542010612991533, data cost:0.12186271832341335 
2022-04-05 14:52:23,055: ============================================================
2022-04-05 14:52:23,055: Epoch 1/38 Batch 2500/7662 eta: 1 day, 3:44:25.684693	Training Loss 39.1932 (41.8939)	Training Prec@1 0.000 (0.010)	Training Prec@5 0.000 (0.039)	
2022-04-05 14:52:23,056: ============================================================
2022-04-05 14:52:56,969: time cost, forward:0.06612499689496265, backward:0.12535435789224963, data cost:0.12288883641482592 
2022-04-05 14:52:56,970: ============================================================
2022-04-05 14:52:56,970: Epoch 1/38 Batch 2600/7662 eta: 1 day, 3:11:03.083423	Training Loss 38.5294 (41.7806)	Training Prec@1 0.586 (0.011)	Training Prec@5 0.781 (0.044)	
2022-04-05 14:52:56,970: ============================================================
2022-04-05 14:53:30,449: time cost, forward:0.06613831521847284, backward:0.12531617218673383, data cost:0.12366099461841336 
2022-04-05 14:53:30,449: ============================================================
2022-04-05 14:53:30,450: Epoch 1/38 Batch 2700/7662 eta: 1 day, 2:49:33.821177	Training Loss 38.1026 (41.6552)	Training Prec@1 0.000 (0.012)	Training Prec@5 0.000 (0.048)	
2022-04-05 14:53:30,450: ============================================================
2022-04-05 14:54:04,178: time cost, forward:0.06614990207117427, backward:0.1252840924748526, data cost:0.12445378005398133 
2022-04-05 14:54:04,178: ============================================================
2022-04-05 14:54:04,178: Epoch 1/38 Batch 2800/7662 eta: 1 day, 3:00:59.250797	Training Loss 37.3748 (41.5159)	Training Prec@1 0.000 (0.013)	Training Prec@5 0.195 (0.053)	
2022-04-05 14:54:04,179: ============================================================
2022-04-05 14:54:39,073: time cost, forward:0.06614799563660215, backward:0.12520956343393894, data cost:0.12565903369210102 
2022-04-05 14:54:39,073: ============================================================
2022-04-05 14:54:39,074: Epoch 1/38 Batch 2900/7662 eta: 1 day, 3:56:28.073191	Training Loss 36.6523 (41.3608)	Training Prec@1 0.195 (0.015)	Training Prec@5 0.586 (0.057)	
2022-04-05 14:54:39,074: ============================================================
2022-04-05 14:55:14,600: time cost, forward:0.06615232674031704, backward:0.12513651598211367, data cost:0.1269800287757408 
2022-04-05 14:55:14,600: ============================================================
2022-04-05 14:55:14,601: Epoch 1/38 Batch 3000/7662 eta: 1 day, 4:26:13.369614	Training Loss 35.7029 (41.1881)	Training Prec@1 0.195 (0.017)	Training Prec@5 0.195 (0.061)	
2022-04-05 14:55:14,601: ============================================================
2022-04-05 14:55:48,851: time cost, forward:0.06615821143510689, backward:0.12509305887200747, data cost:0.12778204754499822 
2022-04-05 14:55:48,852: ============================================================
2022-04-05 14:55:48,852: Epoch 1/38 Batch 3100/7662 eta: 1 day, 3:24:23.072473	Training Loss 34.7036 (40.9952)	Training Prec@1 0.195 (0.018)	Training Prec@5 0.195 (0.066)	
2022-04-05 14:55:48,852: ============================================================
2022-04-05 14:56:24,881: time cost, forward:0.06617250260653888, backward:0.12502636995938615, data cost:0.1291149079184787 
2022-04-05 14:56:24,882: ============================================================
2022-04-05 14:56:24,882: Epoch 1/38 Batch 3200/7662 eta: 1 day, 4:49:11.641619	Training Loss 33.4513 (40.7792)	Training Prec@1 0.000 (0.019)	Training Prec@5 0.000 (0.070)	
2022-04-05 14:56:24,882: ============================================================
2022-04-05 14:57:00,826: time cost, forward:0.0661834051624216, backward:0.12495398731006208, data cost:0.1303406864558252 
2022-04-05 14:57:00,827: ============================================================
2022-04-05 14:57:00,827: Epoch 1/38 Batch 3300/7662 eta: 1 day, 4:44:29.541664	Training Loss 32.1064 (40.5376)	Training Prec@1 0.195 (0.020)	Training Prec@5 0.195 (0.074)	
2022-04-05 14:57:00,827: ============================================================
2022-04-05 14:57:35,834: time cost, forward:0.06619249564403995, backward:0.12489628560334734, data cost:0.1312048283700699 
2022-04-05 14:57:35,835: ============================================================
2022-04-05 14:57:35,835: Epoch 1/38 Batch 3400/7662 eta: 1 day, 3:58:58.115710	Training Loss 30.5278 (40.2669)	Training Prec@1 0.000 (0.021)	Training Prec@5 0.000 (0.077)	
2022-04-05 14:57:35,835: ============================================================
2022-04-05 14:58:12,759: time cost, forward:0.06620366943328848, backward:0.12480703058430861, data cost:0.13257137534617697 
2022-04-05 14:58:12,760: ============================================================
2022-04-05 14:58:12,760: Epoch 1/38 Batch 3500/7662 eta: 1 day, 5:30:17.406094	Training Loss 28.7315 (39.9636)	Training Prec@1 0.000 (0.021)	Training Prec@5 0.000 (0.078)	
2022-04-05 14:58:12,760: ============================================================
2022-04-05 14:58:48,641: time cost, forward:0.06621143274023719, backward:0.12473455261077838, data cost:0.1336353296834782 
2022-04-05 14:58:48,642: ============================================================
2022-04-05 14:58:48,642: Epoch 1/38 Batch 3600/7662 eta: 1 day, 4:39:41.116268	Training Loss 26.7196 (39.6242)	Training Prec@1 0.000 (0.021)	Training Prec@5 0.000 (0.078)	
2022-04-05 14:58:48,642: ============================================================
2022-04-05 14:59:24,258: time cost, forward:0.0662322614798194, backward:0.12467513866893792, data cost:0.1345251630080265 
2022-04-05 14:59:24,258: ============================================================
2022-04-05 14:59:24,258: Epoch 1/38 Batch 3700/7662 eta: 1 day, 4:26:21.906920	Training Loss 24.4295 (39.2446)	Training Prec@1 0.000 (0.022)	Training Prec@5 0.000 (0.079)	
2022-04-05 14:59:24,258: ============================================================
2022-04-05 15:00:01,351: time cost, forward:0.06625054679001027, backward:0.12460077765992705, data cost:0.13576203987892754 
2022-04-05 15:00:01,351: ============================================================
2022-04-05 15:00:01,352: Epoch 1/38 Batch 3800/7662 eta: 1 day, 5:36:30.288926	Training Loss 21.8702 (38.8212)	Training Prec@1 0.000 (0.022)	Training Prec@5 0.195 (0.078)	
2022-04-05 15:00:01,352: ============================================================
2022-04-05 15:00:37,561: time cost, forward:0.06625843463906511, backward:0.12453592040655581, data cost:0.1367065766494254 
2022-04-05 15:00:37,561: ============================================================
2022-04-05 15:00:37,562: Epoch 1/38 Batch 3900/7662 eta: 1 day, 4:53:35.190679	Training Loss 24.0433 (38.3581)	Training Prec@1 0.000 (0.021)	Training Prec@5 0.000 (0.076)	
2022-04-05 15:00:37,562: ============================================================
2022-04-05 15:01:14,052: time cost, forward:0.06626961504885423, backward:0.12447292919783748, data cost:0.13769202132199995 
2022-04-05 15:01:14,053: ============================================================
2022-04-05 15:01:14,053: Epoch 1/38 Batch 4000/7662 eta: 1 day, 5:06:28.443394	Training Loss 24.9192 (38.0156)	Training Prec@1 0.000 (0.021)	Training Prec@5 0.000 (0.075)	
2022-04-05 15:01:14,053: ============================================================
2022-04-05 15:01:51,852: time cost, forward:0.06628195533929263, backward:0.1243772803355671, data cost:0.1389890391235091 
2022-04-05 15:01:51,853: ============================================================
2022-04-05 15:01:51,853: Epoch 1/38 Batch 4100/7662 eta: 1 day, 6:08:26.263987	Training Loss 24.7900 (37.6953)	Training Prec@1 0.000 (0.021)	Training Prec@5 0.000 (0.074)	
2022-04-05 15:01:51,853: ============================================================
2022-04-05 15:02:28,494: time cost, forward:0.06629278228861515, backward:0.12431630539763511, data cost:0.1398785621332367 
2022-04-05 15:02:28,494: ============================================================
2022-04-05 15:02:28,495: Epoch 1/38 Batch 4200/7662 eta: 1 day, 5:12:26.293283	Training Loss 24.9851 (37.3931)	Training Prec@1 0.000 (0.021)	Training Prec@5 0.000 (0.074)	
2022-04-05 15:02:28,495: ============================================================
2022-04-05 15:03:05,384: time cost, forward:0.06630509552442077, backward:0.1242574158034288, data cost:0.1408327717813233 
2022-04-05 15:03:05,384: ============================================================
2022-04-05 15:03:05,384: Epoch 1/38 Batch 4300/7662 eta: 1 day, 5:23:40.513513	Training Loss 25.0451 (37.1067)	Training Prec@1 0.000 (0.022)	Training Prec@5 0.391 (0.075)	
2022-04-05 15:03:05,384: ============================================================
2022-04-05 15:03:41,653: time cost, forward:0.06631224636165466, backward:0.12419589532183148, data cost:0.14157969655164618 
2022-04-05 15:03:41,653: ============================================================
2022-04-05 15:03:41,653: Epoch 1/38 Batch 4400/7662 eta: 1 day, 4:53:23.679072	Training Loss 24.9017 (36.8335)	Training Prec@1 0.195 (0.023)	Training Prec@5 0.195 (0.077)	
2022-04-05 15:03:41,653: ============================================================
2022-04-05 15:04:17,383: time cost, forward:0.06632550495628464, backward:0.1241588620086012, data cost:0.14212674144957696 
2022-04-05 15:04:17,384: ============================================================
2022-04-05 15:04:17,384: Epoch 1/38 Batch 4500/7662 eta: 1 day, 4:27:04.409222	Training Loss 25.3463 (36.5726)	Training Prec@1 0.000 (0.023)	Training Prec@5 0.000 (0.081)	
2022-04-05 15:04:17,384: ============================================================
2022-04-05 15:04:54,871: time cost, forward:0.06633103648121653, backward:0.1240952193775911, data cost:0.14311316781107253 
2022-04-05 15:04:54,872: ============================================================
2022-04-05 15:04:54,872: Epoch 1/38 Batch 4600/7662 eta: 1 day, 5:50:24.486553	Training Loss 24.7313 (36.3216)	Training Prec@1 0.000 (0.025)	Training Prec@5 0.000 (0.087)	
2022-04-05 15:04:54,872: ============================================================
2022-04-05 15:05:31,389: time cost, forward:0.06634205872770521, backward:0.12404526652465807, data cost:0.14381883483816904 
2022-04-05 15:05:31,389: ============================================================
2022-04-05 15:05:31,389: Epoch 1/38 Batch 4700/7662 eta: 1 day, 5:03:27.097985	Training Loss 24.8011 (36.0802)	Training Prec@1 0.000 (0.027)	Training Prec@5 0.781 (0.093)	
2022-04-05 15:05:31,390: ============================================================
2022-04-05 15:06:08,771: time cost, forward:0.06634990854495812, backward:0.12399880800129945, data cost:0.14467734454894815 
2022-04-05 15:06:08,771: ============================================================
2022-04-05 15:06:08,771: Epoch 1/38 Batch 4800/7662 eta: 1 day, 5:44:05.736063	Training Loss 24.7510 (35.8471)	Training Prec@1 0.391 (0.030)	Training Prec@5 0.781 (0.101)	
2022-04-05 15:06:08,772: ============================================================
2022-04-05 15:06:47,505: time cost, forward:0.06635734844655498, backward:0.12393002102243143, data cost:0.14579198943957866 
2022-04-05 15:06:47,505: ============================================================
2022-04-05 15:06:47,506: Epoch 1/38 Batch 4900/7662 eta: 1 day, 6:47:59.776580	Training Loss 24.7845 (35.6217)	Training Prec@1 0.000 (0.032)	Training Prec@5 1.172 (0.111)	
2022-04-05 15:06:47,506: ============================================================
2022-04-05 15:07:24,197: time cost, forward:0.06636495155247481, backward:0.1239057853952077, data cost:0.14640686893062513 
2022-04-05 15:07:24,198: ============================================================
2022-04-05 15:07:24,198: Epoch 1/38 Batch 5000/7662 eta: 1 day, 5:09:57.980849	Training Loss 24.7567 (35.4038)	Training Prec@1 0.391 (0.035)	Training Prec@5 1.172 (0.123)	
2022-04-05 15:07:24,198: ============================================================
2022-04-05 15:08:00,554: time cost, forward:0.0663745687577883, backward:0.1238693283407986, data cost:0.1469769759794431 
2022-04-05 15:08:00,554: ============================================================
2022-04-05 15:08:00,555: Epoch 1/38 Batch 5100/7662 eta: 1 day, 4:53:20.163588	Training Loss 24.5163 (35.1926)	Training Prec@1 0.000 (0.039)	Training Prec@5 0.781 (0.137)	
2022-04-05 15:08:00,555: ============================================================
2022-04-05 15:08:37,546: time cost, forward:0.06638055619058024, backward:0.12382417574458958, data cost:0.14761454514710576 
2022-04-05 15:08:37,547: ============================================================
2022-04-05 15:08:37,547: Epoch 1/38 Batch 5200/7662 eta: 1 day, 5:23:03.732210	Training Loss 24.4584 (34.9880)	Training Prec@1 0.391 (0.044)	Training Prec@5 1.172 (0.155)	
2022-04-05 15:08:37,548: ============================================================
2022-04-05 15:09:14,041: time cost, forward:0.06638984789059148, backward:0.12378469348561204, data cost:0.14815646667574053 
2022-04-05 15:09:14,041: ============================================================
2022-04-05 15:09:14,041: Epoch 1/38 Batch 5300/7662 eta: 1 day, 4:58:40.023721	Training Loss 24.1321 (34.7896)	Training Prec@1 0.781 (0.049)	Training Prec@5 2.344 (0.176)	
2022-04-05 15:09:14,041: ============================================================
2022-04-05 15:09:51,618: time cost, forward:0.06639652195672764, backward:0.123730171696083, data cost:0.14887165798039587 
2022-04-05 15:09:51,618: ============================================================
2022-04-05 15:09:51,619: Epoch 1/38 Batch 5400/7662 eta: 1 day, 5:49:40.025622	Training Loss 24.2099 (34.5968)	Training Prec@1 0.781 (0.056)	Training Prec@5 1.367 (0.199)	
2022-04-05 15:09:51,619: ============================================================
2022-04-05 15:10:29,300: time cost, forward:0.06640678337431535, backward:0.12368347454469927, data cost:0.14959600275701818 
2022-04-05 15:10:29,300: ============================================================
2022-04-05 15:10:29,300: Epoch 1/38 Batch 5500/7662 eta: 1 day, 5:54:00.300688	Training Loss 24.0477 (34.4093)	Training Prec@1 0.195 (0.063)	Training Prec@5 1.758 (0.226)	
2022-04-05 15:10:29,300: ============================================================
2022-04-05 15:11:05,149: time cost, forward:0.06641370003767195, backward:0.12365946315786502, data cost:0.14994763250669468 
2022-04-05 15:11:05,149: ============================================================
2022-04-05 15:11:05,150: Epoch 1/38 Batch 5600/7662 eta: 1 day, 4:26:10.300396	Training Loss 23.9748 (34.2276)	Training Prec@1 0.781 (0.073)	Training Prec@5 1.172 (0.256)	
2022-04-05 15:11:05,150: ============================================================
2022-04-05 15:11:41,302: time cost, forward:0.0664173181192104, backward:0.12363720404555827, data cost:0.15033401960154377 
2022-04-05 15:11:41,303: ============================================================
2022-04-05 15:11:41,303: Epoch 1/38 Batch 5700/7662 eta: 1 day, 4:40:03.049265	Training Loss 23.8363 (34.0511)	Training Prec@1 0.391 (0.084)	Training Prec@5 2.539 (0.289)	
2022-04-05 15:11:41,303: ============================================================
2022-04-05 15:12:18,051: time cost, forward:0.06642357084869947, backward:0.12361907921982009, data cost:0.15079472603150781 
2022-04-05 15:12:18,052: ============================================================
2022-04-05 15:12:18,052: Epoch 1/38 Batch 5800/7662 eta: 1 day, 5:07:44.840629	Training Loss 24.0014 (33.8797)	Training Prec@1 0.977 (0.094)	Training Prec@5 2.539 (0.324)	
2022-04-05 15:12:18,052: ============================================================
2022-04-05 15:12:54,782: time cost, forward:0.0664288256810588, backward:0.12358823912207162, data cost:0.15126849696037628 
2022-04-05 15:12:54,782: ============================================================
2022-04-05 15:12:54,783: Epoch 1/38 Batch 5900/7662 eta: 1 day, 5:06:16.816093	Training Loss 23.6778 (33.7130)	Training Prec@1 0.781 (0.106)	Training Prec@5 2.539 (0.362)	
2022-04-05 15:12:54,783: ============================================================
2022-04-05 15:13:30,850: time cost, forward:0.06643569201186292, backward:0.12357120260355334, data cost:0.15159691920139767 
2022-04-05 15:13:30,851: ============================================================
2022-04-05 15:13:30,851: Epoch 1/38 Batch 6000/7662 eta: 1 day, 4:34:11.304012	Training Loss 23.6751 (33.5512)	Training Prec@1 0.586 (0.119)	Training Prec@5 3.516 (0.403)	
2022-04-05 15:13:30,851: ============================================================
2022-04-05 15:14:07,703: time cost, forward:0.06644225050179875, backward:0.12353639896628622, data cost:0.1520586833150138 
2022-04-05 15:14:07,703: ============================================================
2022-04-05 15:14:07,703: Epoch 1/38 Batch 6100/7662 eta: 1 day, 5:10:50.408578	Training Loss 24.0845 (33.3940)	Training Prec@1 1.172 (0.134)	Training Prec@5 2.148 (0.445)	
2022-04-05 15:14:07,704: ============================================================
2022-04-05 15:14:45,011: time cost, forward:0.06644440574172158, backward:0.12349000898632893, data cost:0.15259469799657738 
2022-04-05 15:14:45,011: ============================================================
2022-04-05 15:14:45,011: Epoch 1/38 Batch 6200/7662 eta: 1 day, 5:31:51.255740	Training Loss 23.8358 (33.2414)	Training Prec@1 1.758 (0.150)	Training Prec@5 5.078 (0.488)	
2022-04-05 15:14:45,011: ============================================================
2022-04-05 15:15:21,806: time cost, forward:0.06644926878890833, backward:0.12347092994112195, data cost:0.1530076142889599 
2022-04-05 15:15:21,806: ============================================================
2022-04-05 15:15:21,807: Epoch 1/38 Batch 6300/7662 eta: 1 day, 5:06:55.117170	Training Loss 23.8353 (33.0930)	Training Prec@1 1.953 (0.167)	Training Prec@5 5.078 (0.534)	
2022-04-05 15:15:21,807: ============================================================
2022-04-05 15:15:58,687: time cost, forward:0.06645350009878272, backward:0.12343502927709508, data cost:0.15342661298277005 
2022-04-05 15:15:58,687: ============================================================
2022-04-05 15:15:58,687: Epoch 1/38 Batch 6400/7662 eta: 1 day, 5:10:19.352772	Training Loss 24.1436 (32.9489)	Training Prec@1 1.172 (0.183)	Training Prec@5 3.516 (0.582)	
2022-04-05 15:15:58,687: ============================================================
2022-04-05 15:16:35,287: time cost, forward:0.06646079998013422, backward:0.12341077057356686, data cost:0.15378836335209925 
2022-04-05 15:16:35,287: ============================================================
2022-04-05 15:16:35,287: Epoch 1/38 Batch 6500/7662 eta: 1 day, 4:56:24.365357	Training Loss 23.4759 (32.8089)	Training Prec@1 1.562 (0.201)	Training Prec@5 3.906 (0.630)	
2022-04-05 15:16:35,287: ============================================================
2022-04-05 15:17:11,899: time cost, forward:0.06646806367474409, backward:0.12339444062189182, data cost:0.1541321298934814 
2022-04-05 15:17:11,899: ============================================================
2022-04-05 15:17:11,900: Epoch 1/38 Batch 6600/7662 eta: 1 day, 4:56:22.881956	Training Loss 24.2309 (32.6730)	Training Prec@1 1.562 (0.219)	Training Prec@5 5.273 (0.678)	
2022-04-05 15:17:11,900: ============================================================
2022-04-05 15:17:50,079: time cost, forward:0.06647022753833817, backward:0.12334873004001652, data cost:0.15473355074607895 
2022-04-05 15:17:50,079: ============================================================
2022-04-05 15:17:50,079: Epoch 1/38 Batch 6700/7662 eta: 1 day, 6:10:05.142266	Training Loss 23.9456 (32.5406)	Training Prec@1 1.758 (0.236)	Training Prec@5 3.320 (0.725)	
2022-04-05 15:17:50,079: ============================================================
2022-04-05 15:18:27,804: time cost, forward:0.06647547831832004, backward:0.12332613490262195, data cost:0.15522505458899677 
2022-04-05 15:18:27,805: ============================================================
2022-04-05 15:18:27,805: Epoch 1/38 Batch 6800/7662 eta: 1 day, 5:47:55.624402	Training Loss 23.6063 (32.4118)	Training Prec@1 1.172 (0.257)	Training Prec@5 4.492 (0.777)	
2022-04-05 15:18:27,805: ============================================================
2022-04-05 15:19:04,014: time cost, forward:0.06648087636303739, backward:0.12331027223642606, data cost:0.15547657341244497 
2022-04-05 15:19:04,014: ============================================================
2022-04-05 15:19:04,014: Epoch 1/38 Batch 6900/7662 eta: 1 day, 4:35:27.713320	Training Loss 23.7917 (32.2864)	Training Prec@1 2.344 (0.278)	Training Prec@5 5.273 (0.828)	
2022-04-05 15:19:04,015: ============================================================
2022-04-05 15:19:40,356: time cost, forward:0.06648315244239472, backward:0.1232889276314572, data cost:0.15574648009315356 
2022-04-05 15:19:40,357: ============================================================
2022-04-05 15:19:40,357: Epoch 1/38 Batch 7000/7662 eta: 1 day, 4:41:10.431062	Training Loss 23.9949 (32.1644)	Training Prec@1 1.367 (0.299)	Training Prec@5 4.102 (0.879)	
2022-04-05 15:19:40,357: ============================================================
2022-04-05 15:20:19,013: time cost, forward:0.06648653808622768, backward:0.12325541891502921, data cost:0.1563334972157514 
2022-04-05 15:20:19,013: ============================================================
2022-04-05 15:20:19,014: Epoch 1/38 Batch 7100/7662 eta: 1 day, 6:30:06.329244	Training Loss 23.8688 (32.0459)	Training Prec@1 1.562 (0.318)	Training Prec@5 4.883 (0.931)	
2022-04-05 15:20:19,014: ============================================================
2022-04-05 15:20:56,609: time cost, forward:0.06649023007147675, backward:0.12322539143271538, data cost:0.1567356048615248 
2022-04-05 15:20:56,610: ============================================================
2022-04-05 15:20:56,610: Epoch 1/38 Batch 7200/7662 eta: 1 day, 5:39:17.306990	Training Loss 23.7093 (31.9303)	Training Prec@1 2.148 (0.339)	Training Prec@5 4.883 (0.983)	
2022-04-05 15:20:56,610: ============================================================
2022-04-05 15:21:34,089: time cost, forward:0.06649726148271645, backward:0.12320125728523033, data cost:0.15718006114303812 
2022-04-05 15:21:34,089: ============================================================
2022-04-05 15:21:34,089: Epoch 1/38 Batch 7300/7662 eta: 1 day, 5:33:08.030169	Training Loss 23.7053 (31.8178)	Training Prec@1 2.930 (0.362)	Training Prec@5 5.078 (1.037)	
2022-04-05 15:21:34,089: ============================================================
2022-04-05 15:22:11,548: time cost, forward:0.0665003885077631, backward:0.12317692935426487, data cost:0.15756926744076702 
2022-04-05 15:22:11,548: ============================================================
2022-04-05 15:22:11,549: Epoch 1/38 Batch 7400/7662 eta: 1 day, 5:31:33.356927	Training Loss 23.7756 (31.7080)	Training Prec@1 2.344 (0.384)	Training Prec@5 5.273 (1.089)	
2022-04-05 15:22:11,549: ============================================================
2022-04-05 15:22:50,060: time cost, forward:0.06650032782843947, backward:0.12314811498741418, data cost:0.15809568593876888 
2022-04-05 15:22:50,060: ============================================================
2022-04-05 15:22:50,061: Epoch 1/38 Batch 7500/7662 eta: 1 day, 6:20:42.039534	Training Loss 23.7085 (31.6008)	Training Prec@1 1.758 (0.407)	Training Prec@5 4.883 (1.143)	
2022-04-05 15:22:50,061: ============================================================
2022-04-05 15:23:25,647: time cost, forward:0.0665027920800395, backward:0.12314049189522762, data cost:0.15820472816806638 
2022-04-05 15:23:25,647: ============================================================
2022-04-05 15:23:25,647: Epoch 1/38 Batch 7600/7662 eta: 1 day, 4:01:48.847206	Training Loss 23.9769 (31.4965)	Training Prec@1 1.758 (0.430)	Training Prec@5 5.273 (1.194)	
2022-04-05 15:23:25,648: ============================================================
2022-04-05 15:23:50,798: Epoch: 1/38 eta: 1 day, 4:01:26.427525	Training Loss 23.5633 (31.4322)	Training Prec@1 1.953 (0.444)	Training Prec@5 5.664 (1.228)
2022-04-05 15:23:50,806: ============================================================
2022-04-05 15:24:28,512: time cost, forward:0.06702717386110864, backward:0.12190708006271209, data cost:0.18450855727147575 
2022-04-05 15:24:28,513: ============================================================
2022-04-05 15:24:28,513: Epoch 2/38 Batch 100/7662 eta: 1 day, 5:15:53.782983	Training Loss 23.4742 (23.5774)	Training Prec@1 1.758 (2.289)	Training Prec@5 4.297 (5.313)	
2022-04-05 15:24:28,513: ============================================================
2022-04-05 15:25:06,093: time cost, forward:0.06705655883904078, backward:0.12137947849292852, data cost:0.18617864230170322 
2022-04-05 15:25:06,094: ============================================================
2022-04-05 15:25:06,094: Epoch 2/38 Batch 200/7662 eta: 1 day, 5:34:24.880769	Training Loss 23.5734 (23.5895)	Training Prec@1 3.516 (2.346)	Training Prec@5 7.617 (5.376)	
2022-04-05 15:25:06,094: ============================================================
2022-04-05 15:25:43,778: time cost, forward:0.06702197674524824, backward:0.12097923093814913, data cost:0.18703150509990576 
2022-04-05 15:25:43,779: ============================================================
2022-04-05 15:25:43,779: Epoch 2/38 Batch 300/7662 eta: 1 day, 5:38:41.028880	Training Loss 23.6585 (23.5825)	Training Prec@1 3.320 (2.390)	Training Prec@5 5.273 (5.448)	
2022-04-05 15:25:43,779: ============================================================
2022-04-05 15:26:22,231: time cost, forward:0.06701656750270299, backward:0.12077448123080987, data cost:0.18952506407161704 
2022-04-05 15:26:22,231: ============================================================
2022-04-05 15:26:22,232: Epoch 2/38 Batch 400/7662 eta: 1 day, 6:14:18.059553	Training Loss 23.7201 (23.5841)	Training Prec@1 1.953 (2.420)	Training Prec@5 4.883 (5.468)	
2022-04-05 15:26:22,232: ============================================================
2022-04-05 15:26:58,392: time cost, forward:0.06697730979842986, backward:0.12081330142661421, data cost:0.1862186826541572 
2022-04-05 15:26:58,392: ============================================================
2022-04-05 15:26:58,393: Epoch 2/38 Batch 500/7662 eta: 1 day, 4:25:33.666804	Training Loss 23.5215 (23.5882)	Training Prec@1 2.344 (2.417)	Training Prec@5 7.812 (5.457)	
2022-04-05 15:26:58,393: ============================================================
2022-04-05 15:27:35,328: time cost, forward:0.06700040103995143, backward:0.12086401678285934, data cost:0.18537782747080808 
2022-04-05 15:27:35,328: ============================================================
2022-04-05 15:27:35,329: Epoch 2/38 Batch 600/7662 eta: 1 day, 5:01:29.989592	Training Loss 23.2850 (23.5876)	Training Prec@1 2.930 (2.446)	Training Prec@5 6.055 (5.475)	
2022-04-05 15:27:35,329: ============================================================
2022-04-05 15:28:13,256: time cost, forward:0.06702373297258848, backward:0.1207487695718528, data cost:0.1862172118583974 
2022-04-05 15:28:13,257: ============================================================
2022-04-05 15:28:13,257: Epoch 2/38 Batch 700/7662 eta: 1 day, 5:47:39.861617	Training Loss 23.4498 (23.5840)	Training Prec@1 2.344 (2.455)	Training Prec@5 5.664 (5.512)	
2022-04-05 15:28:13,257: ============================================================
2022-04-05 15:28:50,869: time cost, forward:0.06701519104357208, backward:0.1206199048606863, data cost:0.18655677014805647 
2022-04-05 15:28:50,869: ============================================================
2022-04-05 15:28:50,870: Epoch 2/38 Batch 800/7662 eta: 1 day, 5:32:09.114384	Training Loss 23.4882 (23.5832)	Training Prec@1 2.539 (2.471)	Training Prec@5 5.859 (5.531)	
2022-04-05 15:28:50,870: ============================================================
2022-04-05 15:29:29,146: time cost, forward:0.0670226248803738, backward:0.12058799868828728, data cost:0.18746435098573813 
2022-04-05 15:29:29,147: ============================================================
2022-04-05 15:29:29,147: Epoch 2/38 Batch 900/7662 eta: 1 day, 6:02:49.950528	Training Loss 23.7258 (23.5830)	Training Prec@1 1.953 (2.495)	Training Prec@5 5.078 (5.574)	
2022-04-05 15:29:29,147: ============================================================
2022-04-05 15:30:06,689: time cost, forward:0.06703319730940047, backward:0.12058492060060855, data cost:0.18741188106594142 
2022-04-05 15:30:06,689: ============================================================
2022-04-05 15:30:06,689: Epoch 2/38 Batch 1000/7662 eta: 1 day, 5:27:35.314319	Training Loss 23.6901 (23.5808)	Training Prec@1 3.711 (2.517)	Training Prec@5 5.664 (5.602)	
2022-04-05 15:30:06,690: ============================================================
2022-04-05 15:30:44,623: time cost, forward:0.06702027199374642, backward:0.120570710618676, data cost:0.1878499077926233 
2022-04-05 15:30:44,624: ============================================================
2022-04-05 15:30:44,624: Epoch 2/38 Batch 1100/7662 eta: 1 day, 5:45:25.067254	Training Loss 23.6192 (23.5798)	Training Prec@1 4.688 (2.531)	Training Prec@5 7.812 (5.612)	
2022-04-05 15:30:44,624: ============================================================
2022-04-05 15:31:21,874: time cost, forward:0.06702970563619708, backward:0.1206466115644517, data cost:0.18744184992728977 
2022-04-05 15:31:21,874: ============================================================
2022-04-05 15:31:21,874: Epoch 2/38 Batch 1200/7662 eta: 1 day, 5:12:35.697845	Training Loss 23.2238 (23.5763)	Training Prec@1 2.930 (2.544)	Training Prec@5 6.250 (5.632)	
2022-04-05 15:31:21,874: ============================================================
2022-04-05 15:31:57,904: time cost, forward:0.06702478194438649, backward:0.12070532428750265, data cost:0.18625447399529976 
2022-04-05 15:31:57,904: ============================================================
2022-04-05 15:31:57,904: Epoch 2/38 Batch 1300/7662 eta: 1 day, 4:14:34.625629	Training Loss 23.8726 (23.5733)	Training Prec@1 1.367 (2.550)	Training Prec@5 4.297 (5.645)	
2022-04-05 15:31:57,904: ============================================================
2022-04-05 15:32:35,234: time cost, forward:0.06702039819516993, backward:0.1206871278802355, data cost:0.18615980775463659 
2022-04-05 15:32:35,234: ============================================================
2022-04-05 15:32:35,235: Epoch 2/38 Batch 1400/7662 eta: 1 day, 5:15:07.168985	Training Loss 23.5708 (23.5711)	Training Prec@1 2.344 (2.563)	Training Prec@5 5.273 (5.674)	
2022-04-05 15:32:35,235: ============================================================
2022-04-05 15:33:12,952: time cost, forward:0.06701650295041257, backward:0.12067019390694056, data cost:0.18638156985663987 
2022-04-05 15:33:12,952: ============================================================
2022-04-05 15:33:12,952: Epoch 2/38 Batch 1500/7662 eta: 1 day, 5:32:42.346812	Training Loss 23.6679 (23.5673)	Training Prec@1 3.711 (2.580)	Training Prec@5 6.836 (5.702)	
2022-04-05 15:33:12,953: ============================================================
2022-04-05 15:33:50,122: time cost, forward:0.0669993927808312, backward:0.12070136088144041, data cost:0.18615766836002964 
2022-04-05 15:33:50,122: ============================================================
2022-04-05 15:33:50,122: Epoch 2/38 Batch 1600/7662 eta: 1 day, 5:06:20.632486	Training Loss 23.0218 (23.5647)	Training Prec@1 2.539 (2.587)	Training Prec@5 7.617 (5.711)	
2022-04-05 15:33:50,123: ============================================================
2022-04-05 15:34:27,367: time cost, forward:0.06699640965588027, backward:0.12072732969197615, data cost:0.18605352219867313 
2022-04-05 15:34:27,368: ============================================================
2022-04-05 15:34:27,368: Epoch 2/38 Batch 1700/7662 eta: 1 day, 5:09:16.153153	Training Loss 23.0646 (23.5608)	Training Prec@1 4.102 (2.603)	Training Prec@5 7.422 (5.731)	
2022-04-05 15:34:27,368: ============================================================
2022-04-05 15:35:04,055: time cost, forward:0.06699638210315714, backward:0.12076565131271727, data cost:0.18560992153967135 
2022-04-05 15:35:04,055: ============================================================
2022-04-05 15:35:04,055: Epoch 2/38 Batch 1800/7662 eta: 1 day, 4:42:25.857123	Training Loss 23.4404 (23.5563)	Training Prec@1 3.125 (2.618)	Training Prec@5 6.055 (5.750)	
2022-04-05 15:35:04,055: ============================================================
2022-04-05 15:35:42,495: time cost, forward:0.06698891387103795, backward:0.12069407596658442, data cost:0.18623694713144567 
2022-04-05 15:35:42,495: ============================================================
2022-04-05 15:35:42,495: Epoch 2/38 Batch 1900/7662 eta: 1 day, 6:04:05.263178	Training Loss 23.8045 (23.5532)	Training Prec@1 3.125 (2.629)	Training Prec@5 5.078 (5.766)	
2022-04-05 15:35:42,495: ============================================================
2022-04-05 15:36:21,178: time cost, forward:0.06699256899358035, backward:0.12066915215343878, data cost:0.1868682843914862 
2022-04-05 15:36:21,179: ============================================================
2022-04-05 15:36:21,179: Epoch 2/38 Batch 2000/7662 eta: 1 day, 6:14:53.075510	Training Loss 23.8310 (23.5500)	Training Prec@1 1.172 (2.636)	Training Prec@5 5.469 (5.775)	
2022-04-05 15:36:21,179: ============================================================
2022-04-05 15:36:58,511: time cost, forward:0.06698143499246263, backward:0.12065038344813506, data cost:0.18683641828089 
2022-04-05 15:36:58,512: ============================================================
2022-04-05 15:36:58,512: Epoch 2/38 Batch 2100/7662 eta: 1 day, 5:10:52.519600	Training Loss 23.4419 (23.5469)	Training Prec@1 2.734 (2.645)	Training Prec@5 5.859 (5.796)	
2022-04-05 15:36:58,512: ============================================================
2022-04-05 15:37:37,801: time cost, forward:0.06698546815102835, backward:0.12061104887233316, data cost:0.18767062671187792 
2022-04-05 15:37:37,802: ============================================================
2022-04-05 15:37:37,802: Epoch 2/38 Batch 2200/7662 eta: 1 day, 6:42:01.478064	Training Loss 23.2298 (23.5419)	Training Prec@1 4.297 (2.654)	Training Prec@5 8.203 (5.814)	
2022-04-05 15:37:37,802: ============================================================
2022-04-05 15:38:14,460: time cost, forward:0.06697877452082093, backward:0.12064206408334328, data cost:0.1872706726459173 
2022-04-05 15:38:14,461: ============================================================
2022-04-05 15:38:14,461: Epoch 2/38 Batch 2300/7662 eta: 1 day, 4:38:02.275227	Training Loss 23.7324 (23.5385)	Training Prec@1 1.758 (2.660)	Training Prec@5 3.711 (5.826)	
2022-04-05 15:38:14,461: ============================================================
2022-04-05 15:38:51,292: time cost, forward:0.0669650165872705, backward:0.12064577192502898, data cost:0.1869363082155877 
2022-04-05 15:38:51,292: ============================================================
2022-04-05 15:38:51,292: Epoch 2/38 Batch 2400/7662 eta: 1 day, 4:45:31.956108	Training Loss 24.4175 (23.5633)	Training Prec@1 0.000 (2.609)	Training Prec@5 0.000 (5.726)	
2022-04-05 15:38:51,293: ============================================================
2022-04-05 15:39:30,324: time cost, forward:0.06696135981553265, backward:0.1205930790933622, data cost:0.1876407577877953 
2022-04-05 15:39:30,324: ============================================================
2022-04-05 15:39:30,324: Epoch 2/38 Batch 2500/7662 eta: 1 day, 6:27:57.738121	Training Loss 25.6042 (23.5972)	Training Prec@1 0.000 (2.504)	Training Prec@5 0.000 (5.496)	
2022-04-05 15:39:30,325: ============================================================
2022-04-05 15:40:07,569: time cost, forward:0.0669651411275948, backward:0.12059104658173066, data cost:0.1875138015827797 
2022-04-05 15:40:07,569: ============================================================
2022-04-05 15:40:07,570: Epoch 2/38 Batch 2600/7662 eta: 1 day, 5:03:39.913688	Training Loss 27.0259 (23.7141)	Training Prec@1 0.000 (2.408)	Training Prec@5 0.000 (5.285)	
2022-04-05 15:40:07,570: ============================================================
2022-04-05 15:40:46,043: time cost, forward:0.06695624668097311, backward:0.1205541685626612, data cost:0.18788535208912327 
2022-04-05 15:40:46,043: ============================================================
2022-04-05 15:40:46,044: Epoch 2/38 Batch 2700/7662 eta: 1 day, 6:00:33.103718	Training Loss 25.5508 (23.7984)	Training Prec@1 0.000 (2.319)	Training Prec@5 0.000 (5.089)	
2022-04-05 15:40:46,044: ============================================================
2022-04-05 15:41:23,318: time cost, forward:0.06695065433615316, backward:0.12054957258313756, data cost:0.1877760714230771 
2022-04-05 15:41:23,319: ============================================================
2022-04-05 15:41:23,319: Epoch 2/38 Batch 2800/7662 eta: 1 day, 5:03:49.794323	Training Loss 24.4297 (23.8447)	Training Prec@1 0.000 (2.236)	Training Prec@5 0.000 (4.908)	
2022-04-05 15:41:23,319: ============================================================
2022-04-05 15:42:01,312: time cost, forward:0.06694806341715376, backward:0.12054757785698429, data cost:0.1879454692012896 
2022-04-05 15:42:01,313: ============================================================
2022-04-05 15:42:01,313: Epoch 2/38 Batch 2900/7662 eta: 1 day, 5:36:49.019332	Training Loss 23.8210 (23.8802)	Training Prec@1 0.000 (2.159)	Training Prec@5 0.000 (4.739)	
2022-04-05 15:42:01,313: ============================================================
2022-04-05 15:42:37,330: time cost, forward:0.06694195977287636, backward:0.12057580976495746, data cost:0.18739222462632807 
2022-04-05 15:42:37,331: ============================================================
2022-04-05 15:42:37,331: Epoch 2/38 Batch 3000/7662 eta: 1 day, 4:03:48.747635	Training Loss 25.0678 (23.9067)	Training Prec@1 0.000 (2.087)	Training Prec@5 0.000 (4.581)	
2022-04-05 15:42:37,331: ============================================================
2022-04-05 15:43:13,118: time cost, forward:0.06693613963882629, backward:0.1206269318075017, data cost:0.18679181011540924 
2022-04-05 15:43:13,118: ============================================================
2022-04-05 15:43:13,118: Epoch 2/38 Batch 3100/7662 eta: 1 day, 3:52:26.880461	Training Loss 22.9416 (23.9250)	Training Prec@1 0.000 (2.019)	Training Prec@5 0.195 (4.434)	
2022-04-05 15:43:13,119: ============================================================
2022-04-05 15:43:49,324: time cost, forward:0.06692344973183453, backward:0.12062482037891556, data cost:0.1863904432342663 
2022-04-05 15:43:49,324: ============================================================
2022-04-05 15:43:49,325: Epoch 2/38 Batch 3200/7662 eta: 1 day, 4:11:24.004239	Training Loss 21.8359 (23.9163)	Training Prec@1 0.000 (1.956)	Training Prec@5 0.000 (4.295)	
2022-04-05 15:43:49,325: ============================================================
2022-04-05 15:44:27,142: time cost, forward:0.06691957141602173, backward:0.12062146519848563, data cost:0.18651441127179283 
2022-04-05 15:44:27,142: ============================================================
2022-04-05 15:44:27,143: Epoch 2/38 Batch 3300/7662 eta: 1 day, 5:26:03.786583	Training Loss 22.4107 (23.9006)	Training Prec@1 0.000 (1.897)	Training Prec@5 0.000 (4.165)	
2022-04-05 15:44:27,143: ============================================================
2022-04-05 15:45:05,770: time cost, forward:0.06691796003141344, backward:0.12059233173898964, data cost:0.18688088306506687 
2022-04-05 15:45:05,770: ============================================================
2022-04-05 15:45:05,770: Epoch 2/38 Batch 3400/7662 eta: 1 day, 6:03:14.486959	Training Loss 22.7183 (23.8982)	Training Prec@1 0.000 (1.841)	Training Prec@5 0.000 (4.043)	
2022-04-05 15:45:05,770: ============================================================
2022-04-05 15:45:43,636: time cost, forward:0.06691255512221196, backward:0.12057412730247234, data cost:0.18700604964815298 
2022-04-05 15:45:43,637: ============================================================
2022-04-05 15:45:43,637: Epoch 2/38 Batch 3500/7662 eta: 1 day, 5:27:04.840445	Training Loss 21.7564 (23.9091)	Training Prec@1 0.000 (1.789)	Training Prec@5 0.000 (3.927)	
2022-04-05 15:45:43,637: ============================================================
2022-04-05 15:46:20,492: time cost, forward:0.06691249975663949, backward:0.12059403287268043, data cost:0.1868251404122334 
2022-04-05 15:46:20,493: ============================================================
2022-04-05 15:46:20,493: Epoch 2/38 Batch 3600/7662 eta: 1 day, 4:39:18.789059	Training Loss 23.8789 (23.9133)	Training Prec@1 0.000 (1.739)	Training Prec@5 0.000 (3.818)	
2022-04-05 15:46:20,493: ============================================================
2022-04-05 15:46:57,939: time cost, forward:0.06690885750594479, backward:0.12059543815359099, data cost:0.1868055262543698 
2022-04-05 15:46:57,940: ============================================================
2022-04-05 15:46:57,940: Epoch 2/38 Batch 3700/7662 eta: 1 day, 5:06:14.167309	Training Loss 27.8840 (23.9388)	Training Prec@1 0.000 (1.692)	Training Prec@5 0.000 (3.715)	
2022-04-05 15:46:57,940: ============================================================
2022-04-05 15:47:35,239: time cost, forward:0.06690594502956373, backward:0.12060507362407896, data cost:0.18674776176930605 
2022-04-05 15:47:35,240: ============================================================
2022-04-05 15:47:35,240: Epoch 2/38 Batch 3800/7662 eta: 1 day, 4:58:46.583626	Training Loss 23.2119 (23.9406)	Training Prec@1 0.000 (1.647)	Training Prec@5 0.000 (3.617)	
2022-04-05 15:47:35,240: ============================================================
2022-04-05 15:48:14,726: time cost, forward:0.06690385929282795, backward:0.12056153827705393, data cost:0.18728091246533254 
2022-04-05 15:48:14,726: ============================================================
2022-04-05 15:48:14,726: Epoch 2/38 Batch 3900/7662 eta: 1 day, 6:40:01.016525	Training Loss 23.9498 (23.9350)	Training Prec@1 0.000 (1.605)	Training Prec@5 0.000 (3.525)	
2022-04-05 15:48:14,726: ============================================================
2022-04-05 15:48:53,345: time cost, forward:0.06689573896321752, backward:0.12053059989078309, data cost:0.18761945778383377 
2022-04-05 15:48:53,345: ============================================================
2022-04-05 15:48:53,346: Epoch 2/38 Batch 4000/7662 eta: 1 day, 5:58:59.088856	Training Loss 27.7071 (23.9732)	Training Prec@1 0.000 (1.565)	Training Prec@5 0.000 (3.436)	
2022-04-05 15:48:53,346: ============================================================
2022-04-05 15:49:33,251: time cost, forward:0.06690923200348116, backward:0.1204928196648558, data cost:0.18821602909993648 
2022-04-05 15:49:33,251: ============================================================
2022-04-05 15:49:33,252: Epoch 2/38 Batch 4100/7662 eta: 1 day, 6:58:15.931963	Training Loss 26.6677 (24.0152)	Training Prec@1 0.000 (1.527)	Training Prec@5 0.000 (3.353)	
2022-04-05 15:49:33,252: ============================================================
2022-04-05 15:50:11,964: time cost, forward:0.0669025274991251, backward:0.12046485618342385, data cost:0.18852944912357425 
2022-04-05 15:50:11,965: ============================================================
2022-04-05 15:50:11,965: Epoch 2/38 Batch 4200/7662 eta: 1 day, 6:02:04.626173	Training Loss 24.1084 (24.0823)	Training Prec@1 0.000 (1.490)	Training Prec@5 0.000 (3.273)	
2022-04-05 15:50:11,965: ============================================================
2022-04-05 15:50:50,472: time cost, forward:0.06689985454512297, backward:0.12044658159538602, data cost:0.18874386472185148 
2022-04-05 15:50:50,472: ============================================================
2022-04-05 15:50:50,473: Epoch 2/38 Batch 4300/7662 eta: 1 day, 5:51:51.115618	Training Loss 26.0974 (24.1143)	Training Prec@1 0.000 (1.456)	Training Prec@5 0.000 (3.197)	
2022-04-05 15:50:50,473: ============================================================
2022-04-05 15:51:27,392: time cost, forward:0.0669018747481901, backward:0.12047021056771631, data cost:0.18853699963156215 
2022-04-05 15:51:27,392: ============================================================
2022-04-05 15:51:27,392: Epoch 2/38 Batch 4400/7662 eta: 1 day, 4:37:21.101667	Training Loss 33.8024 (24.1358)	Training Prec@1 0.000 (1.423)	Training Prec@5 0.000 (3.124)	
2022-04-05 15:51:27,393: ============================================================
2022-04-05 15:52:04,195: time cost, forward:0.06690203271778088, backward:0.12047599961848279, data cost:0.18835701346794853 
2022-04-05 15:52:04,195: ============================================================
2022-04-05 15:52:04,196: Epoch 2/38 Batch 4500/7662 eta: 1 day, 4:31:19.372553	Training Loss 23.6972 (24.1852)	Training Prec@1 0.000 (1.391)	Training Prec@5 0.000 (3.055)	
2022-04-05 15:52:04,196: ============================================================
2022-04-05 15:52:41,590: time cost, forward:0.06689845040767808, backward:0.12046504238424781, data cost:0.18832071575555473 
2022-04-05 15:52:41,591: ============================================================
2022-04-05 15:52:41,591: Epoch 2/38 Batch 4600/7662 eta: 1 day, 4:58:13.827154	Training Loss 22.2901 (24.2047)	Training Prec@1 0.000 (1.361)	Training Prec@5 0.000 (2.988)	
2022-04-05 15:52:41,591: ============================================================
2022-04-05 15:53:19,346: time cost, forward:0.06690067477773215, backward:0.12047542727077279, data cost:0.18834557820442105 
2022-04-05 15:53:19,346: ============================================================
2022-04-05 15:53:19,347: Epoch 2/38 Batch 4700/7662 eta: 1 day, 5:14:20.667123	Training Loss 23.3824 (24.2028)	Training Prec@1 0.000 (1.332)	Training Prec@5 0.000 (2.925)	
2022-04-05 15:53:19,347: ============================================================
2022-04-05 15:53:56,225: time cost, forward:0.06689601139268123, backward:0.12047975027056927, data cost:0.18818964315320233 
2022-04-05 15:53:56,225: ============================================================
2022-04-05 15:53:56,226: Epoch 2/38 Batch 4800/7662 eta: 1 day, 4:32:59.787376	Training Loss 25.2869 (24.1983)	Training Prec@1 0.000 (1.304)	Training Prec@5 0.000 (2.864)	
2022-04-05 15:53:56,226: ============================================================
2022-04-05 15:54:32,946: time cost, forward:0.06689917693261736, backward:0.12050273316712934, data cost:0.18797270374216335 
2022-04-05 15:54:32,947: ============================================================
2022-04-05 15:54:32,947: Epoch 2/38 Batch 4900/7662 eta: 1 day, 4:25:04.265359	Training Loss 26.7597 (24.2302)	Training Prec@1 0.000 (1.278)	Training Prec@5 0.000 (2.805)	
2022-04-05 15:54:32,947: ============================================================
2022-04-05 15:55:09,169: time cost, forward:0.06690260581336849, backward:0.12052992230106482, data cost:0.187668860161345 
2022-04-05 15:55:09,169: ============================================================
2022-04-05 15:55:09,170: Epoch 2/38 Batch 5000/7662 eta: 1 day, 4:01:18.285195	Training Loss 24.3479 (24.2885)	Training Prec@1 0.000 (1.252)	Training Prec@5 0.000 (2.749)	
2022-04-05 15:55:09,170: ============================================================
2022-04-05 15:55:46,488: time cost, forward:0.06689965629091355, backward:0.1205210912038261, data cost:0.1876385348477675 
2022-04-05 15:55:46,489: ============================================================
2022-04-05 15:55:46,489: Epoch 2/38 Batch 5100/7662 eta: 1 day, 4:51:35.063066	Training Loss 33.9840 (24.3070)	Training Prec@1 0.000 (1.227)	Training Prec@5 0.000 (2.695)	
2022-04-05 15:55:46,489: ============================================================
2022-04-05 15:56:23,996: time cost, forward:0.06689996649655362, backward:0.12051946989823635, data cost:0.18762900802258645 
2022-04-05 15:56:23,997: ============================================================
2022-04-05 15:56:23,997: Epoch 2/38 Batch 5200/7662 eta: 1 day, 4:59:42.032389	Training Loss 23.0844 (24.3327)	Training Prec@1 0.000 (1.204)	Training Prec@5 0.000 (2.643)	
2022-04-05 15:56:23,997: ============================================================
2022-04-05 15:57:01,708: time cost, forward:0.06690166954455185, backward:0.12052494509081994, data cost:0.18765110919151154 
2022-04-05 15:57:01,708: ============================================================
2022-04-05 15:57:01,709: Epoch 2/38 Batch 5300/7662 eta: 1 day, 5:08:32.409937	Training Loss 26.4063 (24.3648)	Training Prec@1 0.000 (1.181)	Training Prec@5 0.000 (2.594)	
2022-04-05 15:57:01,709: ============================================================
2022-04-05 15:57:40,041: time cost, forward:0.06689992147941327, backward:0.12051566721532363, data cost:0.1878038546535169 
2022-04-05 15:57:40,041: ============================================================
2022-04-05 15:57:40,041: Epoch 2/38 Batch 5400/7662 eta: 1 day, 5:36:41.715281	Training Loss 24.0821 (24.3992)	Training Prec@1 0.000 (1.159)	Training Prec@5 0.000 (2.546)	
2022-04-05 15:57:40,042: ============================================================
2022-04-05 15:58:17,986: time cost, forward:0.06689919829693766, backward:0.12051575728862324, data cost:0.18787134805794822 
2022-04-05 15:58:17,986: ============================================================
2022-04-05 15:58:17,986: Epoch 2/38 Batch 5500/7662 eta: 1 day, 5:18:04.409203	Training Loss 24.3054 (24.4093)	Training Prec@1 0.000 (1.138)	Training Prec@5 0.000 (2.500)	
2022-04-05 15:58:17,986: ============================================================
2022-04-05 15:58:55,127: time cost, forward:0.06689589639756696, backward:0.12051230120773847, data cost:0.1878045138386152 
2022-04-05 15:58:55,128: ============================================================
2022-04-05 15:58:55,128: Epoch 2/38 Batch 5600/7662 eta: 1 day, 4:40:15.585875	Training Loss 24.3463 (24.4308)	Training Prec@1 0.000 (1.118)	Training Prec@5 0.000 (2.455)	
2022-04-05 15:58:55,128: ============================================================
2022-04-05 15:59:32,593: time cost, forward:0.06689718594193814, backward:0.12052569867937497, data cost:0.18777233794145906 
2022-04-05 15:59:32,594: ============================================================
2022-04-05 15:59:32,594: Epoch 2/38 Batch 5700/7662 eta: 1 day, 4:54:38.196731	Training Loss 24.0372 (24.4405)	Training Prec@1 0.000 (1.098)	Training Prec@5 0.000 (2.412)	
2022-04-05 15:59:32,594: ============================================================
2022-04-05 16:00:08,782: time cost, forward:0.06690148950383712, backward:0.12053739282463641, data cost:0.18750071768144796 
2022-04-05 16:00:08,783: ============================================================
2022-04-05 16:00:08,889: Epoch 2/38 Batch 5800/7662 eta: 1 day, 3:59:48.927077	Training Loss 25.7703 (24.4496)	Training Prec@1 0.000 (1.079)	Training Prec@5 0.000 (2.370)	
2022-04-05 16:00:08,889: ============================================================
2022-04-05 16:00:46,043: time cost, forward:0.06690038153833809, backward:0.12053575413088452, data cost:0.18746360921722324 
2022-04-05 16:00:46,043: ============================================================
2022-04-05 16:00:46,044: Epoch 2/38 Batch 5900/7662 eta: 1 day, 4:39:00.238841	Training Loss 23.9795 (24.4529)	Training Prec@1 0.000 (1.061)	Training Prec@5 0.000 (2.330)	
2022-04-05 16:00:46,044: ============================================================
2022-04-05 16:01:24,469: time cost, forward:0.06690086331202956, backward:0.12052222632789517, data cost:0.18761787503575222 
2022-04-05 16:01:24,470: ============================================================
2022-04-05 16:01:24,470: Epoch 2/38 Batch 6000/7662 eta: 1 day, 5:37:11.181070	Training Loss 25.1489 (24.4703)	Training Prec@1 0.000 (1.043)	Training Prec@5 0.000 (2.291)	
2022-04-05 16:01:24,470: ============================================================
2022-04-05 16:02:02,185: time cost, forward:0.06690026181782831, backward:0.12052212368955298, data cost:0.18765150333822975 
2022-04-05 16:02:02,186: ============================================================
2022-04-05 16:02:02,186: Epoch 2/38 Batch 6100/7662 eta: 1 day, 5:03:43.043115	Training Loss 24.9287 (24.4734)	Training Prec@1 0.000 (1.026)	Training Prec@5 0.000 (2.254)	
2022-04-05 16:02:02,187: ============================================================
2022-04-05 16:02:40,150: time cost, forward:0.06690184068902882, backward:0.12051096057138014, data cost:0.18771755754957278 
2022-04-05 16:02:40,150: ============================================================
2022-04-05 16:02:40,150: Epoch 2/38 Batch 6200/7662 eta: 1 day, 5:14:32.688287	Training Loss 24.6144 (24.4868)	Training Prec@1 0.000 (1.010)	Training Prec@5 0.000 (2.218)	
2022-04-05 16:02:40,151: ============================================================
2022-04-05 16:03:20,211: time cost, forward:0.06690436155952373, backward:0.12048699027262977, data cost:0.1881508934704117 
2022-04-05 16:03:20,212: ============================================================
2022-04-05 16:03:20,212: Epoch 2/38 Batch 6300/7662 eta: 1 day, 6:50:48.590120	Training Loss 23.9264 (24.4886)	Training Prec@1 0.000 (0.994)	Training Prec@5 0.000 (2.182)	
2022-04-05 16:03:20,212: ============================================================
2022-04-05 16:03:58,699: time cost, forward:0.06690431464592131, backward:0.12046934344057256, data cost:0.18829884274264092 
2022-04-05 16:03:58,700: ============================================================
2022-04-05 16:03:58,700: Epoch 2/38 Batch 6400/7662 eta: 1 day, 5:37:28.947017	Training Loss 23.8794 (24.4876)	Training Prec@1 0.000 (0.978)	Training Prec@5 0.000 (2.148)	
2022-04-05 16:03:58,700: ============================================================
2022-04-05 16:04:37,257: time cost, forward:0.06690644762996234, backward:0.1204623102609846, data cost:0.18844631195508585 
2022-04-05 16:04:37,258: ============================================================
2022-04-05 16:04:37,258: Epoch 2/38 Batch 6500/7662 eta: 1 day, 5:40:02.492098	Training Loss 24.4959 (24.4855)	Training Prec@1 0.000 (0.963)	Training Prec@5 0.000 (2.115)	
2022-04-05 16:04:37,258: ============================================================
2022-04-05 16:05:16,524: time cost, forward:0.06690462182228954, backward:0.12044852819383496, data cost:0.18868603123229713 
2022-04-05 16:05:16,525: ============================================================
2022-04-05 16:05:16,525: Epoch 2/38 Batch 6600/7662 eta: 1 day, 6:12:08.733195	Training Loss 23.6429 (24.4926)	Training Prec@1 0.000 (0.949)	Training Prec@5 0.000 (2.083)	
2022-04-05 16:05:16,525: ============================================================
2022-04-05 16:05:54,813: time cost, forward:0.06690371422682935, backward:0.1204266864197488, data cost:0.18882870428561097 
2022-04-05 16:05:54,814: ============================================================
2022-04-05 16:05:54,814: Epoch 2/38 Batch 6700/7662 eta: 1 day, 5:26:21.281364	Training Loss 24.3730 (24.4908)	Training Prec@1 0.000 (0.934)	Training Prec@5 0.000 (2.052)	
2022-04-05 16:05:54,814: ============================================================
2022-04-05 16:06:32,977: time cost, forward:0.06690679600807231, backward:0.12042666284595102, data cost:0.18889416812325 
2022-04-05 16:06:32,977: ============================================================
2022-04-05 16:06:32,978: Epoch 2/38 Batch 6800/7662 eta: 1 day, 5:19:57.689904	Training Loss 24.2950 (24.4927)	Training Prec@1 0.000 (0.921)	Training Prec@5 0.000 (2.022)	
2022-04-05 16:06:32,978: ============================================================
2022-04-05 16:07:11,623: time cost, forward:0.06691358507258388, backward:0.1204210221033059, data cost:0.18903944768461908 
2022-04-05 16:07:11,624: ============================================================
2022-04-05 16:07:11,624: Epoch 2/38 Batch 6900/7662 eta: 1 day, 5:41:34.376667	Training Loss 24.2267 (24.4944)	Training Prec@1 0.000 (0.907)	Training Prec@5 0.000 (1.993)	
2022-04-05 16:07:11,625: ============================================================
2022-04-05 16:07:48,812: time cost, forward:0.06691712719284648, backward:0.12041586118862448, data cost:0.1889665496402135 
2022-04-05 16:07:48,813: ============================================================
2022-04-05 16:07:48,813: Epoch 2/38 Batch 7000/7662 eta: 1 day, 4:33:44.744996	Training Loss 26.4940 (24.4979)	Training Prec@1 0.000 (0.894)	Training Prec@5 0.000 (1.964)	
2022-04-05 16:07:48,813: ============================================================
2022-04-05 16:08:25,814: time cost, forward:0.06692107866676547, backward:0.12042968477022649, data cost:0.18885830862956376 
2022-04-05 16:08:25,815: ============================================================
2022-04-05 16:08:25,815: Epoch 2/38 Batch 7100/7662 eta: 1 day, 4:24:32.370743	Training Loss 24.8543 (24.4939)	Training Prec@1 0.000 (0.882)	Training Prec@5 0.000 (1.937)	
2022-04-05 16:08:25,815: ============================================================
2022-04-05 16:09:02,345: time cost, forward:0.06692339533119504, backward:0.12043816772728004, data cost:0.1886866042673265 
2022-04-05 16:09:02,346: ============================================================
2022-04-05 16:09:02,346: Epoch 2/38 Batch 7200/7662 eta: 1 day, 4:02:12.648987	Training Loss 23.7971 (24.4925)	Training Prec@1 0.000 (0.869)	Training Prec@5 0.000 (1.910)	
2022-04-05 16:09:02,346: ============================================================
2022-04-05 16:09:41,507: time cost, forward:0.06691958708605679, backward:0.12042275221809359, data cost:0.18890650214553123 
2022-04-05 16:09:41,508: ============================================================
2022-04-05 16:09:41,508: Epoch 2/38 Batch 7300/7662 eta: 1 day, 6:02:43.450836	Training Loss 24.2297 (24.4887)	Training Prec@1 0.000 (0.858)	Training Prec@5 0.000 (1.884)	
2022-04-05 16:09:41,508: ============================================================
2022-04-05 16:10:20,998: time cost, forward:0.06692153238770059, backward:0.12040754765622566, data cost:0.1891657794097063 
2022-04-05 16:10:20,999: ============================================================
2022-04-05 16:10:20,999: Epoch 2/38 Batch 7400/7662 eta: 1 day, 6:17:12.771724	Training Loss 25.9534 (24.4893)	Training Prec@1 0.000 (0.846)	Training Prec@5 0.000 (1.858)	
2022-04-05 16:10:20,999: ============================================================
2022-04-05 16:10:58,132: time cost, forward:0.0669260025342348, backward:0.12040540904645237, data cost:0.18909163003858684 
2022-04-05 16:10:58,132: ============================================================
2022-04-05 16:10:58,133: Epoch 2/38 Batch 7500/7662 eta: 1 day, 4:28:06.872730	Training Loss 24.5496 (24.4916)	Training Prec@1 0.000 (0.835)	Training Prec@5 0.195 (1.834)	
2022-04-05 16:10:58,133: ============================================================
2022-04-05 16:11:36,231: time cost, forward:0.06692651871521703, backward:0.12040489104911362, data cost:0.18914741995397688 
2022-04-05 16:11:36,231: ============================================================
2022-04-05 16:11:36,231: Epoch 2/38 Batch 7600/7662 eta: 1 day, 5:11:52.636087	Training Loss 24.0338 (24.4880)	Training Prec@1 0.000 (0.824)	Training Prec@5 0.000 (1.810)	
2022-04-05 16:11:36,232: ============================================================
2022-04-05 16:12:00,917: Epoch: 2/38 eta: 1 day, 5:11:28.633854	Training Loss 24.2232 (24.4860)	Training Prec@1 0.000 (0.817)	Training Prec@5 0.000 (1.795)
2022-04-05 16:12:00,917: ============================================================
2022-04-05 16:12:41,074: time cost, forward:0.06698012351989746, backward:0.12107554589859162, data cost:0.21217924175840436 
2022-04-05 16:12:41,075: ============================================================
2022-04-05 16:12:41,075: Epoch 3/38 Batch 100/7662 eta: 1 day, 6:30:53.845328	Training Loss 24.4856 (24.1907)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.087)	
2022-04-05 16:12:41,076: ============================================================
2022-04-05 16:13:18,988: time cost, forward:0.06696003645508733, backward:0.12107661980480405, data cost:0.20118476398027124 
2022-04-05 16:13:18,988: ============================================================
2022-04-05 16:13:18,989: Epoch 3/38 Batch 200/7662 eta: 1 day, 5:01:41.101680	Training Loss 23.9096 (24.1694)	Training Prec@1 0.000 (0.006)	Training Prec@5 0.195 (0.072)	
2022-04-05 16:13:18,989: ============================================================
2022-04-05 16:13:56,802: time cost, forward:0.0669680989306906, backward:0.1210481753716102, data cost:0.19717807514611693 
2022-04-05 16:13:56,802: ============================================================
2022-04-05 16:13:56,803: Epoch 3/38 Batch 300/7662 eta: 1 day, 4:56:30.154085	Training Loss 23.9190 (24.1741)	Training Prec@1 0.000 (0.005)	Training Prec@5 0.195 (0.078)	
2022-04-05 16:13:56,803: ============================================================
2022-04-05 16:14:34,109: time cost, forward:0.06695690489651863, backward:0.12118780702576601, data cost:0.19404748089630203 
2022-04-05 16:14:34,109: ============================================================
2022-04-05 16:14:34,110: Epoch 3/38 Batch 400/7662 eta: 1 day, 4:32:36.816804	Training Loss 25.1565 (24.1527)	Training Prec@1 0.000 (0.005)	Training Prec@5 0.000 (0.085)	
2022-04-05 16:14:34,110: ============================================================
2022-04-05 16:15:10,605: time cost, forward:0.0669902680154315, backward:0.12133572717945657, data cost:0.19030015693160002 
2022-04-05 16:15:10,605: ============================================================
2022-04-05 16:15:10,606: Epoch 3/38 Batch 500/7662 eta: 1 day, 3:54:45.009948	Training Loss 23.9982 (24.1357)	Training Prec@1 0.000 (0.005)	Training Prec@5 0.000 (0.085)	
2022-04-05 16:15:10,606: ============================================================
2022-04-05 16:15:48,803: time cost, forward:0.06695306122004488, backward:0.12126519723806238, data cost:0.19089193336155658 
2022-04-05 16:15:48,803: ============================================================
2022-04-05 16:15:48,803: Epoch 3/38 Batch 600/7662 eta: 1 day, 5:12:12.312168	Training Loss 24.1218 (24.1108)	Training Prec@1 0.000 (0.006)	Training Prec@5 0.000 (0.085)	
2022-04-05 16:15:48,804: ============================================================
2022-04-05 16:16:25,914: time cost, forward:0.06696113155294045, backward:0.12125903205980729, data cost:0.18951773404733988 
2022-04-05 16:16:25,915: ============================================================
2022-04-05 16:16:25,915: Epoch 3/38 Batch 700/7662 eta: 1 day, 4:21:47.120156	Training Loss 23.4845 (24.0848)	Training Prec@1 0.000 (0.005)	Training Prec@5 0.195 (0.082)	
2022-04-05 16:16:25,915: ============================================================
2022-04-05 16:17:03,305: time cost, forward:0.06694467225868501, backward:0.12129976871762616, data cost:0.18897363718818216 
2022-04-05 16:17:03,305: ============================================================
2022-04-05 16:17:03,306: Epoch 3/38 Batch 800/7662 eta: 1 day, 4:33:56.170776	Training Loss 22.9748 (24.0371)	Training Prec@1 0.000 (0.004)	Training Prec@5 0.000 (0.077)	
2022-04-05 16:17:03,306: ============================================================
2022-04-05 16:17:38,972: time cost, forward:0.06692871713267019, backward:0.12142112496432261, data cost:0.1865330357705393 
2022-04-05 16:17:38,972: ============================================================
2022-04-05 16:17:38,972: Epoch 3/38 Batch 900/7662 eta: 1 day, 3:14:19.680072	Training Loss 24.3261 (24.0131)	Training Prec@1 0.000 (0.004)	Training Prec@5 0.000 (0.074)	
2022-04-05 16:17:38,973: ============================================================
2022-04-05 16:18:15,273: time cost, forward:0.06691891772372348, backward:0.12154709875165999, data cost:0.18522683683935706 
2022-04-05 16:18:15,274: ============================================================
2022-04-05 16:18:15,274: Epoch 3/38 Batch 1000/7662 eta: 1 day, 3:42:48.986608	Training Loss 24.0475 (23.9982)	Training Prec@1 0.000 (0.004)	Training Prec@5 0.000 (0.070)	
2022-04-05 16:18:15,274: ============================================================
2022-04-05 16:18:52,777: time cost, forward:0.06689994220195628, backward:0.12145123013157103, data cost:0.18541752088059069 
2022-04-05 16:18:52,778: ============================================================
2022-04-05 16:18:52,778: Epoch 3/38 Batch 1100/7662 eta: 1 day, 4:37:15.737761	Training Loss 23.8276 (23.9766)	Training Prec@1 0.000 (0.004)	Training Prec@5 0.000 (0.066)	
2022-04-05 16:18:52,778: ============================================================
2022-04-05 16:19:30,461: time cost, forward:0.06690487292928433, backward:0.12147056787982397, data cost:0.1856230160313114 
2022-04-05 16:19:30,461: ============================================================
2022-04-05 16:19:30,462: Epoch 3/38 Batch 1200/7662 eta: 1 day, 4:44:51.602357	Training Loss 23.4266 (23.9417)	Training Prec@1 0.000 (0.003)	Training Prec@5 0.000 (0.062)	
2022-04-05 16:19:30,462: ============================================================
2022-04-05 16:20:06,403: time cost, forward:0.06691701142396993, backward:0.12155453180514271, data cost:0.18436818750570882 
2022-04-05 16:20:06,403: ============================================================
2022-04-05 16:20:06,403: Epoch 3/38 Batch 1300/7662 eta: 1 day, 3:24:31.910992	Training Loss 24.1459 (23.9451)	Training Prec@1 0.000 (0.003)	Training Prec@5 0.000 (0.058)	
2022-04-05 16:20:06,404: ============================================================
2022-04-05 16:20:43,358: time cost, forward:0.06689681880724609, backward:0.1215326090724746, data cost:0.18411396383131462 
2022-04-05 16:20:43,358: ============================================================
2022-04-05 16:20:43,358: Epoch 3/38 Batch 1400/7662 eta: 1 day, 4:10:16.757470	Training Loss 23.8929 (23.9220)	Training Prec@1 0.000 (0.003)	Training Prec@5 0.000 (0.056)	
2022-04-05 16:20:43,359: ============================================================
2022-04-05 16:21:20,851: time cost, forward:0.06689776127301827, backward:0.12149982360142561, data cost:0.18425123002864743 
2022-04-05 16:21:20,851: ============================================================
2022-04-05 16:21:20,852: Epoch 3/38 Batch 1500/7662 eta: 1 day, 4:34:15.990955	Training Loss 23.8107 (23.9048)	Training Prec@1 0.000 (0.003)	Training Prec@5 0.195 (0.053)	
2022-04-05 16:21:20,852: ============================================================
2022-04-05 16:21:59,689: time cost, forward:0.06687252964356157, backward:0.12139188102664912, data cost:0.18534338094056435 
2022-04-05 16:21:59,690: ============================================================
2022-04-05 16:21:59,690: Epoch 3/38 Batch 1600/7662 eta: 1 day, 5:35:07.392058	Training Loss 23.9598 (23.8972)	Training Prec@1 0.000 (0.003)	Training Prec@5 0.195 (0.052)	
2022-04-05 16:21:59,690: ============================================================
2022-04-05 16:22:37,202: time cost, forward:0.06687669349881464, backward:0.12142500546204475, data cost:0.1853596315164998 
2022-04-05 16:22:37,203: ============================================================
2022-04-05 16:22:37,203: Epoch 3/38 Batch 1700/7662 eta: 1 day, 4:33:55.524086	Training Loss 23.3895 (23.8835)	Training Prec@1 0.000 (0.003)	Training Prec@5 0.000 (0.050)	
2022-04-05 16:22:37,203: ============================================================
2022-04-05 16:23:13,913: time cost, forward:0.06689074676920799, backward:0.12141868298155788, data cost:0.18494375829500515 
2022-04-05 16:23:13,914: ============================================================
2022-04-05 16:23:13,914: Epoch 3/38 Batch 1800/7662 eta: 1 day, 3:56:40.927482	Training Loss 23.6849 (23.8563)	Training Prec@1 0.000 (0.002)	Training Prec@5 0.000 (0.048)	
2022-04-05 16:23:13,914: ============================================================
2022-04-05 16:23:49,977: time cost, forward:0.06688161772637069, backward:0.12142803193143069, data cost:0.1842647778981607 
2022-04-05 16:23:49,977: ============================================================
2022-04-05 16:23:49,977: Epoch 3/38 Batch 1900/7662 eta: 1 day, 3:26:28.873720	Training Loss 23.6515 (23.8386)	Training Prec@1 0.000 (0.002)	Training Prec@5 0.000 (0.046)	
2022-04-05 16:23:49,978: ============================================================
2022-04-05 16:24:26,378: time cost, forward:0.06688657255396956, backward:0.12147612378500652, data cost:0.18377920888315863 
2022-04-05 16:24:26,378: ============================================================
2022-04-05 16:24:26,379: Epoch 3/38 Batch 2000/7662 eta: 1 day, 3:41:19.028932	Training Loss 23.2246 (23.8469)	Training Prec@1 0.000 (0.002)	Training Prec@5 0.000 (0.044)	
2022-04-05 16:24:26,379: ============================================================
2022-04-05 16:25:02,478: time cost, forward:0.06689559396759222, backward:0.12152854643190629, data cost:0.1831377610074389 
2022-04-05 16:25:02,479: ============================================================
2022-04-05 16:25:02,479: Epoch 3/38 Batch 2100/7662 eta: 1 day, 3:26:58.123096	Training Loss 23.5591 (23.8338)	Training Prec@1 0.000 (0.002)	Training Prec@5 0.000 (0.042)	
2022-04-05 16:25:02,479: ============================================================
2022-04-05 16:25:39,382: time cost, forward:0.06688692549132606, backward:0.12152471798232817, data cost:0.18295206824125296 
2022-04-05 16:25:39,383: ============================================================
2022-04-05 16:25:39,383: Epoch 3/38 Batch 2200/7662 eta: 1 day, 4:03:01.555060	Training Loss 23.0942 (23.8334)	Training Prec@1 0.000 (0.002)	Training Prec@5 0.000 (0.041)	
2022-04-05 16:25:39,383: ============================================================
2022-04-05 16:26:16,965: time cost, forward:0.06689205009971509, backward:0.12152523940519024, data cost:0.18318474360163597 
2022-04-05 16:26:16,966: ============================================================
2022-04-05 16:26:16,966: Epoch 3/38 Batch 2300/7662 eta: 1 day, 4:33:21.820502	Training Loss 23.3232 (23.8159)	Training Prec@1 0.000 (0.002)	Training Prec@5 0.000 (0.040)	
2022-04-05 16:26:16,966: ============================================================
2022-04-05 16:26:54,952: time cost, forward:0.06689113157398753, backward:0.12148752576264306, data cost:0.18350399549229038 
2022-04-05 16:26:54,953: ============================================================
2022-04-05 16:26:54,953: Epoch 3/38 Batch 2400/7662 eta: 1 day, 4:51:09.682753	Training Loss 23.2656 (23.8031)	Training Prec@1 0.000 (0.002)	Training Prec@5 0.000 (0.038)	
2022-04-05 16:26:54,953: ============================================================
2022-04-05 16:27:32,288: time cost, forward:0.06691049985668096, backward:0.1214905719177014, data cost:0.18354490891892036 
2022-04-05 16:27:32,288: ============================================================
2022-04-05 16:27:32,288: Epoch 3/38 Batch 2500/7662 eta: 1 day, 4:20:49.263402	Training Loss 22.5124 (23.7921)	Training Prec@1 0.000 (0.002)	Training Prec@5 0.000 (0.038)	
2022-04-05 16:27:32,288: ============================================================
2022-04-05 16:28:11,230: time cost, forward:0.0669238992084123, backward:0.12143570333776588, data cost:0.18424020569431823 
2022-04-05 16:28:11,231: ============================================================
2022-04-05 16:28:11,231: Epoch 3/38 Batch 2600/7662 eta: 1 day, 5:33:24.531989	Training Loss 24.6353 (23.8063)	Training Prec@1 0.000 (0.002)	Training Prec@5 0.000 (0.037)	
2022-04-05 16:28:11,231: ============================================================
2022-04-05 16:28:49,079: time cost, forward:0.06691677820333598, backward:0.12140659616363628, data cost:0.1844783308065216 
2022-04-05 16:28:49,080: ============================================================
2022-04-05 16:28:49,080: Epoch 3/38 Batch 2700/7662 eta: 1 day, 4:42:57.641546	Training Loss 24.0724 (23.8129)	Training Prec@1 0.000 (0.002)	Training Prec@5 0.000 (0.037)	
2022-04-05 16:28:49,080: ============================================================
2022-04-05 16:29:25,191: time cost, forward:0.0669216975947711, backward:0.12144573887316999, data cost:0.18400693450837785 
2022-04-05 16:29:25,192: ============================================================
2022-04-05 16:29:25,192: Epoch 3/38 Batch 2800/7662 eta: 1 day, 3:23:18.310314	Training Loss 23.3493 (23.8082)	Training Prec@1 0.000 (0.002)	Training Prec@5 0.000 (0.036)	
2022-04-05 16:29:25,192: ============================================================
2022-04-05 16:30:01,336: time cost, forward:0.0669262029253725, backward:0.12147581441602283, data cost:0.18357040109861386 
2022-04-05 16:30:01,337: ============================================================
2022-04-05 16:30:01,337: Epoch 3/38 Batch 2900/7662 eta: 1 day, 3:24:11.702425	Training Loss 23.8161 (23.7968)	Training Prec@1 0.000 (0.002)	Training Prec@5 0.000 (0.035)	
2022-04-05 16:30:01,337: ============================================================
2022-04-05 16:30:38,703: time cost, forward:0.06692171581747851, backward:0.12145396398281645, data cost:0.18358686694545562 
2022-04-05 16:30:38,703: ============================================================
2022-04-05 16:30:38,703: Epoch 3/38 Batch 3000/7662 eta: 1 day, 4:19:07.585209	Training Loss 23.3078 (23.8027)	Training Prec@1 0.000 (0.002)	Training Prec@5 0.000 (0.035)	
2022-04-05 16:30:38,704: ============================================================
2022-04-05 16:31:16,376: time cost, forward:0.06691909321049176, backward:0.12143155250598554, data cost:0.1837844174690653 
2022-04-05 16:31:16,377: ============================================================
2022-04-05 16:31:16,377: Epoch 3/38 Batch 3100/7662 eta: 1 day, 4:32:28.928498	Training Loss 24.8772 (23.7944)	Training Prec@1 0.000 (0.002)	Training Prec@5 0.000 (0.034)	
2022-04-05 16:31:16,377: ============================================================
2022-04-05 16:31:53,525: time cost, forward:0.06691578396710428, backward:0.1214167874244721, data cost:0.1837761157674691 
2022-04-05 16:31:53,526: ============================================================
2022-04-05 16:31:53,526: Epoch 3/38 Batch 3200/7662 eta: 1 day, 4:07:59.446916	Training Loss 23.5328 (23.7908)	Training Prec@1 0.000 (0.002)	Training Prec@5 0.000 (0.033)	
2022-04-05 16:31:53,526: ============================================================
2022-04-05 16:32:32,687: time cost, forward:0.06691657481463832, backward:0.12137636295699032, data cost:0.18437693718166992 
2022-04-05 16:32:32,688: ============================================================
2022-04-05 16:32:32,688: Epoch 3/38 Batch 3300/7662 eta: 1 day, 5:38:50.049059	Training Loss 23.5974 (23.7883)	Training Prec@1 0.000 (0.002)	Training Prec@5 0.000 (0.033)	
2022-04-05 16:32:32,688: ============================================================
2022-04-05 16:33:11,323: time cost, forward:0.06691755368871596, backward:0.1213544087748066, data cost:0.18479145621860613 
2022-04-05 16:33:11,323: ============================================================
2022-04-05 16:33:11,324: Epoch 3/38 Batch 3400/7662 eta: 1 day, 5:14:16.168561	Training Loss 23.5889 (23.7870)	Training Prec@1 0.000 (0.002)	Training Prec@5 0.000 (0.032)	
2022-04-05 16:33:11,324: ============================================================
2022-04-05 16:33:48,039: time cost, forward:0.06691461428467564, backward:0.12136105184181652, data cost:0.18461032914038758 
2022-04-05 16:33:48,039: ============================================================
2022-04-05 16:33:48,040: Epoch 3/38 Batch 3500/7662 eta: 1 day, 3:46:29.456605	Training Loss 23.8985 (23.7983)	Training Prec@1 0.000 (0.003)	Training Prec@5 0.000 (0.033)	
2022-04-05 16:33:48,040: ============================================================
2022-04-05 16:34:24,908: time cost, forward:0.06691846108231223, backward:0.12135325130007936, data cost:0.18447807054448107 
2022-04-05 16:34:24,908: ============================================================
2022-04-05 16:34:24,908: Epoch 3/38 Batch 3600/7662 eta: 1 day, 3:52:48.910289	Training Loss 25.3266 (23.7997)	Training Prec@1 0.000 (0.003)	Training Prec@5 0.000 (0.033)	
2022-04-05 16:34:24,909: ============================================================
2022-04-05 16:35:01,588: time cost, forward:0.06691921379283236, backward:0.12136347392469717, data cost:0.18428610685420185 
2022-04-05 16:35:01,588: ============================================================
2022-04-05 16:35:01,588: Epoch 3/38 Batch 3700/7662 eta: 1 day, 3:43:38.098454	Training Loss 23.6170 (23.7946)	Training Prec@1 0.000 (0.003)	Training Prec@5 0.000 (0.033)	
2022-04-05 16:35:01,589: ============================================================
2022-04-05 16:35:38,788: time cost, forward:0.06691917679001702, backward:0.12135777902716115, data cost:0.18428020911581988 
2022-04-05 16:35:38,789: ============================================================
2022-04-05 16:35:38,789: Epoch 3/38 Batch 3800/7662 eta: 1 day, 4:06:37.292450	Training Loss 23.4617 (23.7887)	Training Prec@1 0.000 (0.003)	Training Prec@5 0.000 (0.033)	
2022-04-05 16:35:38,789: ============================================================
2022-04-05 16:36:15,225: time cost, forward:0.06692101735890171, backward:0.12139263285157252, data cost:0.1840091157674973 
2022-04-05 16:36:15,226: ============================================================
2022-04-05 16:36:15,226: Epoch 3/38 Batch 3900/7662 eta: 1 day, 3:31:25.346990	Training Loss 23.5073 (23.7845)	Training Prec@1 0.000 (0.003)	Training Prec@5 0.000 (0.034)	
2022-04-05 16:36:15,226: ============================================================
2022-04-05 16:36:52,302: time cost, forward:0.06692159703505818, backward:0.12139351101451053, data cost:0.1839436920740271 
2022-04-05 16:36:52,303: ============================================================
2022-04-05 16:36:52,303: Epoch 3/38 Batch 4000/7662 eta: 1 day, 3:59:46.906098	Training Loss 23.1357 (23.7814)	Training Prec@1 0.000 (0.003)	Training Prec@5 0.000 (0.034)	
2022-04-05 16:36:52,303: ============================================================
2022-04-05 16:37:29,688: time cost, forward:0.0669254431290637, backward:0.12140283552022293, data cost:0.18388029068497572 
2022-04-05 16:37:29,688: ============================================================
2022-04-05 16:37:29,689: Epoch 3/38 Batch 4100/7662 eta: 1 day, 4:13:09.498589	Training Loss 23.2866 (23.7770)	Training Prec@1 0.000 (0.003)	Training Prec@5 0.000 (0.034)	
2022-04-05 16:37:29,689: ============================================================
2022-04-05 16:38:08,261: time cost, forward:0.06693896312718166, backward:0.12138747129874106, data cost:0.1842741688594105 
2022-04-05 16:38:08,261: ============================================================
2022-04-05 16:38:08,261: Epoch 3/38 Batch 4200/7662 eta: 1 day, 5:06:15.893415	Training Loss 23.6103 (23.7739)	Training Prec@1 0.000 (0.003)	Training Prec@5 0.000 (0.034)	
2022-04-05 16:38:08,261: ============================================================
2022-04-05 16:38:46,626: time cost, forward:0.06693496995816316, backward:0.12136043329853378, data cost:0.18453397465794827 
2022-04-05 16:38:46,627: ============================================================
2022-04-05 16:38:46,627: Epoch 3/38 Batch 4300/7662 eta: 1 day, 4:56:16.035980	Training Loss 23.5257 (23.7704)	Training Prec@1 0.000 (0.003)	Training Prec@5 0.000 (0.034)	
2022-04-05 16:38:46,627: ============================================================
2022-04-05 16:39:23,946: time cost, forward:0.06693687718628373, backward:0.12134687140140243, data cost:0.18456766892086512 
2022-04-05 16:39:23,946: ============================================================
2022-04-05 16:39:23,946: Epoch 3/38 Batch 4400/7662 eta: 1 day, 4:08:16.621006	Training Loss 24.3583 (23.7852)	Training Prec@1 0.000 (0.003)	Training Prec@5 0.000 (0.035)	
2022-04-05 16:39:23,947: ============================================================
2022-04-05 16:40:03,966: time cost, forward:0.06694187697211115, backward:0.12132133698829096, data cost:0.18517448224340924 
2022-04-05 16:40:03,967: ============================================================
2022-04-05 16:40:03,967: Epoch 3/38 Batch 4500/7662 eta: 1 day, 6:09:49.002955	Training Loss 24.2848 (23.7918)	Training Prec@1 0.000 (0.003)	Training Prec@5 0.000 (0.036)	
2022-04-05 16:40:03,967: ============================================================
2022-04-05 16:40:41,736: time cost, forward:0.06693912366339112, backward:0.12130765470117817, data cost:0.1852751491017434 
2022-04-05 16:40:41,737: ============================================================
2022-04-05 16:40:41,737: Epoch 3/38 Batch 4600/7662 eta: 1 day, 4:27:24.977590	Training Loss 23.5864 (23.7921)	Training Prec@1 0.000 (0.003)	Training Prec@5 0.195 (0.037)	
2022-04-05 16:40:41,737: ============================================================
2022-04-05 16:41:19,154: time cost, forward:0.06694284305341144, backward:0.12132026759024046, data cost:0.18525923062040595 
2022-04-05 16:41:19,154: ============================================================
2022-04-05 16:41:19,154: Epoch 3/38 Batch 4700/7662 eta: 1 day, 4:10:50.928522	Training Loss 23.8569 (23.7913)	Training Prec@1 0.000 (0.003)	Training Prec@5 0.000 (0.037)	
2022-04-05 16:41:19,155: ============================================================
2022-04-05 16:41:57,162: time cost, forward:0.06694486285776216, backward:0.12129659085354226, data cost:0.1854046192435479 
2022-04-05 16:41:57,163: ============================================================
2022-04-05 16:41:57,163: Epoch 3/38 Batch 4800/7662 eta: 1 day, 4:36:55.978067	Training Loss 24.6877 (23.7975)	Training Prec@1 0.000 (0.003)	Training Prec@5 0.000 (0.038)	
2022-04-05 16:41:57,163: ============================================================
2022-04-05 16:42:35,820: time cost, forward:0.066943727578161, backward:0.1212698005856142, data cost:0.18568359639941978 
2022-04-05 16:42:35,821: ============================================================
2022-04-05 16:42:35,821: Epoch 3/38 Batch 4900/7662 eta: 1 day, 5:05:37.736455	Training Loss 23.9444 (23.8056)	Training Prec@1 0.000 (0.004)	Training Prec@5 0.000 (0.038)	
2022-04-05 16:42:35,822: ============================================================
2022-04-05 16:43:15,117: time cost, forward:0.0669448863126965, backward:0.12123602942291034, data cost:0.18608538773947417 
2022-04-05 16:43:15,118: ============================================================
2022-04-05 16:43:15,118: Epoch 3/38 Batch 5000/7662 eta: 1 day, 5:33:47.834226	Training Loss 23.5033 (23.8064)	Training Prec@1 0.000 (0.004)	Training Prec@5 0.000 (0.038)	
2022-04-05 16:43:15,118: ============================================================
2022-04-05 16:43:55,252: time cost, forward:0.06694120284411738, backward:0.12119157320379065, data cost:0.18665207065912198 
2022-04-05 16:43:55,252: ============================================================
2022-04-05 16:43:55,253: Epoch 3/38 Batch 5100/7662 eta: 1 day, 6:10:58.053083	Training Loss 23.3508 (23.8047)	Training Prec@1 0.195 (0.004)	Training Prec@5 0.195 (0.038)	
2022-04-05 16:43:55,253: ============================================================
2022-04-05 16:44:33,388: time cost, forward:0.06693892956788917, backward:0.12118345825780834, data cost:0.18678616266751386 
2022-04-05 16:44:33,388: ============================================================
2022-04-05 16:44:33,388: Epoch 3/38 Batch 5200/7662 eta: 1 day, 4:40:07.697323	Training Loss 24.1491 (23.8052)	Training Prec@1 0.000 (0.004)	Training Prec@5 0.000 (0.038)	
2022-04-05 16:44:33,388: ============================================================
2022-04-05 16:45:10,693: time cost, forward:0.06694535215658205, backward:0.12118391591122654, data cost:0.1867380207451948 
2022-04-05 16:45:10,693: ============================================================
2022-04-05 16:45:10,693: Epoch 3/38 Batch 5300/7662 eta: 1 day, 4:02:02.296486	Training Loss 23.5000 (23.8061)	Training Prec@1 0.000 (0.004)	Training Prec@5 0.000 (0.039)	
2022-04-05 16:45:10,693: ============================================================
2022-04-05 16:45:48,729: time cost, forward:0.06698686490568502, backward:0.12116488436235942, data cost:0.1868100330595132 
2022-04-05 16:45:48,730: ============================================================
2022-04-05 16:45:48,730: Epoch 3/38 Batch 5400/7662 eta: 1 day, 4:34:24.531061	Training Loss 23.6052 (23.8047)	Training Prec@1 0.000 (0.004)	Training Prec@5 0.000 (0.040)	
2022-04-05 16:45:48,730: ============================================================
2022-04-05 16:46:27,508: time cost, forward:0.06698872618598058, backward:0.1211418860564342, data cost:0.18704181490084934 
2022-04-05 16:46:27,509: ============================================================
2022-04-05 16:46:27,509: Epoch 3/38 Batch 5500/7662 eta: 1 day, 5:07:11.306271	Training Loss 24.1677 (23.8050)	Training Prec@1 0.000 (0.004)	Training Prec@5 0.000 (0.040)	
2022-04-05 16:46:27,509: ============================================================
2022-04-05 16:47:06,131: time cost, forward:0.06698744508661869, backward:0.12113021743959733, data cost:0.18725022091825513 
2022-04-05 16:47:06,132: ============================================================
2022-04-05 16:47:06,132: Epoch 3/38 Batch 5600/7662 eta: 1 day, 4:59:33.001713	Training Loss 23.7702 (23.8029)	Training Prec@1 0.000 (0.004)	Training Prec@5 0.000 (0.039)	
2022-04-05 16:47:06,132: ============================================================
2022-04-05 16:47:45,985: time cost, forward:0.06698491561201543, backward:0.1211018268215717, data cost:0.18766922979192538 
2022-04-05 16:47:45,985: ============================================================
2022-04-05 16:47:45,986: Epoch 3/38 Batch 5700/7662 eta: 1 day, 5:54:17.166967	Training Loss 23.6759 (23.8010)	Training Prec@1 0.000 (0.004)	Training Prec@5 0.195 (0.039)	
2022-04-05 16:47:45,986: ============================================================
2022-04-05 16:48:23,186: time cost, forward:0.06698459842160234, backward:0.12109441883832468, data cost:0.18759786254230254 
2022-04-05 16:48:23,186: ============================================================
2022-04-05 16:48:23,187: Epoch 3/38 Batch 5800/7662 eta: 1 day, 3:54:15.645824	Training Loss 23.5608 (23.7972)	Training Prec@1 0.000 (0.004)	Training Prec@5 0.000 (0.039)	
2022-04-05 16:48:23,187: ============================================================
2022-04-05 16:49:01,893: time cost, forward:0.0669868764765769, backward:0.12108441202655487, data cost:0.18779960828911432 
2022-04-05 16:49:01,893: ============================================================
2022-04-05 16:49:01,894: Epoch 3/38 Batch 5900/7662 eta: 1 day, 5:01:23.107298	Training Loss 23.7899 (23.7950)	Training Prec@1 0.000 (0.004)	Training Prec@5 0.000 (0.039)	
2022-04-05 16:49:01,894: ============================================================
2022-04-05 16:49:41,314: time cost, forward:0.06698383528265402, backward:0.12105702590815205, data cost:0.18812788961569177 
2022-04-05 16:49:41,315: ============================================================
2022-04-05 16:49:41,315: Epoch 3/38 Batch 6000/7662 eta: 1 day, 5:32:50.735057	Training Loss 23.8562 (23.7987)	Training Prec@1 0.000 (0.004)	Training Prec@5 0.000 (0.039)	
2022-04-05 16:49:41,315: ============================================================
2022-04-05 16:50:18,955: time cost, forward:0.06698536165309042, backward:0.12104767552163292, data cost:0.18813180106451208 
2022-04-05 16:50:18,956: ============================================================
2022-04-05 16:50:18,956: Epoch 3/38 Batch 6100/7662 eta: 1 day, 4:12:10.104726	Training Loss 23.5994 (23.7962)	Training Prec@1 0.000 (0.004)	Training Prec@5 0.000 (0.038)	
2022-04-05 16:50:18,956: ============================================================
2022-04-05 16:50:54,734: time cost, forward:0.06698029020906052, backward:0.12106191606209304, data cost:0.18782145366647163 
2022-04-05 16:50:54,734: ============================================================
2022-04-05 16:50:54,735: Epoch 3/38 Batch 6200/7662 eta: 1 day, 2:47:51.258333	Training Loss 23.4752 (23.7934)	Training Prec@1 0.000 (0.004)	Training Prec@5 0.000 (0.038)	
2022-04-05 16:50:54,735: ============================================================
2022-04-05 16:51:31,543: time cost, forward:0.06698526982599487, backward:0.12106679053245262, data cost:0.18768210046952594 
2022-04-05 16:51:31,543: ============================================================
2022-04-05 16:51:31,543: Epoch 3/38 Batch 6300/7662 eta: 1 day, 3:33:32.043431	Training Loss 23.3689 (23.7920)	Training Prec@1 0.000 (0.004)	Training Prec@5 0.000 (0.038)	
2022-04-05 16:51:31,544: ============================================================
2022-04-05 16:52:07,643: time cost, forward:0.06698296695672865, backward:0.12107836672953841, data cost:0.18743233480273158 
2022-04-05 16:52:07,644: ============================================================
2022-04-05 16:52:07,644: Epoch 3/38 Batch 6400/7662 eta: 1 day, 3:01:07.027893	Training Loss 23.3942 (23.7909)	Training Prec@1 0.000 (0.004)	Training Prec@5 0.195 (0.037)	
2022-04-05 16:52:07,644: ============================================================
2022-04-05 16:52:44,547: time cost, forward:0.06697960882338033, backward:0.12107861575795643, data cost:0.18732688173475145 
2022-04-05 16:52:44,548: ============================================================
2022-04-05 16:52:44,548: Epoch 3/38 Batch 6500/7662 eta: 1 day, 3:36:34.460365	Training Loss 27.3896 (23.7900)	Training Prec@1 0.000 (0.004)	Training Prec@5 0.000 (0.037)	
2022-04-05 16:52:44,548: ============================================================
2022-04-05 16:53:24,639: time cost, forward:0.06697805485304133, backward:0.12105499970080148, data cost:0.1877335049669243 
2022-04-05 16:53:24,640: ============================================================
2022-04-05 16:53:24,640: Epoch 3/38 Batch 6600/7662 eta: 1 day, 5:59:00.695391	Training Loss 23.5939 (23.7896)	Training Prec@1 0.000 (0.004)	Training Prec@5 0.000 (0.037)	
2022-04-05 16:53:24,640: ============================================================
2022-04-05 16:54:03,055: time cost, forward:0.06697676515984952, backward:0.12103763472270496, data cost:0.1878691870590736 
2022-04-05 16:54:03,056: ============================================================
2022-04-05 16:54:03,057: Epoch 3/38 Batch 6700/7662 eta: 1 day, 4:43:11.514483	Training Loss 23.4785 (23.7895)	Training Prec@1 0.000 (0.004)	Training Prec@5 0.195 (0.037)	
2022-04-05 16:54:03,057: ============================================================
2022-04-05 16:54:41,895: time cost, forward:0.06697190286552474, backward:0.12101782429023672, data cost:0.18807037056570702 
2022-04-05 16:54:41,896: ============================================================
2022-04-05 16:54:41,896: Epoch 3/38 Batch 6800/7662 eta: 1 day, 5:01:31.911526	Training Loss 23.8236 (23.7912)	Training Prec@1 0.000 (0.004)	Training Prec@5 0.000 (0.037)	
2022-04-05 16:54:41,897: ============================================================
2022-04-05 16:55:21,606: time cost, forward:0.06697124079494031, backward:0.12099327825432084, data cost:0.1883910171950031 
2022-04-05 16:55:21,607: ============================================================
2022-04-05 16:55:21,607: Epoch 3/38 Batch 6900/7662 eta: 1 day, 5:39:55.679615	Training Loss 24.3867 (23.7939)	Training Prec@1 0.000 (0.004)	Training Prec@5 0.000 (0.037)	
2022-04-05 16:55:21,607: ============================================================
2022-04-05 16:55:59,960: time cost, forward:0.06696695636929674, backward:0.12097029782036473, data cost:0.1885095842396605 
2022-04-05 16:55:59,961: ============================================================
2022-04-05 16:55:59,961: Epoch 3/38 Batch 7000/7662 eta: 1 day, 4:38:27.467489	Training Loss 24.0962 (23.7941)	Training Prec@1 0.000 (0.004)	Training Prec@5 0.000 (0.037)	
2022-04-05 16:55:59,961: ============================================================
2022-04-05 16:56:38,413: time cost, forward:0.06696263124009458, backward:0.12095564260400841, data cost:0.18863931066940529 
2022-04-05 16:56:38,414: ============================================================
2022-04-05 16:56:38,414: Epoch 3/38 Batch 7100/7662 eta: 1 day, 4:42:16.390817	Training Loss 24.3146 (23.7948)	Training Prec@1 0.000 (0.004)	Training Prec@5 0.000 (0.037)	
2022-04-05 16:56:38,414: ============================================================
2022-04-05 16:57:15,813: time cost, forward:0.06696184066653765, backward:0.1209544110486931, data cost:0.18858630737673493 
2022-04-05 16:57:15,813: ============================================================
2022-04-05 16:57:15,814: Epoch 3/38 Batch 7200/7662 eta: 1 day, 3:54:27.613746	Training Loss 24.0043 (23.7957)	Training Prec@1 0.000 (0.004)	Training Prec@5 0.195 (0.037)	
2022-04-05 16:57:15,814: ============================================================
2022-04-05 16:57:53,491: time cost, forward:0.06695948772061963, backward:0.12095081507302193, data cost:0.1886021452319836 
2022-04-05 16:57:53,492: ============================================================
2022-04-05 16:57:53,492: Epoch 3/38 Batch 7300/7662 eta: 1 day, 4:06:19.795890	Training Loss 23.4765 (23.8004)	Training Prec@1 0.000 (0.004)	Training Prec@5 0.000 (0.037)	
2022-04-05 16:57:53,493: ============================================================
2022-04-05 16:58:31,345: time cost, forward:0.06695408140554478, backward:0.1209415088297306, data cost:0.18863623428061163 
2022-04-05 16:58:31,345: ============================================================
2022-04-05 16:58:31,345: Epoch 3/38 Batch 7400/7662 eta: 1 day, 4:13:29.563774	Training Loss 24.0247 (23.8029)	Training Prec@1 0.000 (0.004)	Training Prec@5 0.000 (0.037)	
2022-04-05 16:58:31,345: ============================================================
2022-04-05 16:59:08,894: time cost, forward:0.06695023468771463, backward:0.12094205330969382, data cost:0.1886252359575169 
2022-04-05 16:59:08,894: ============================================================
2022-04-05 16:59:08,895: Epoch 3/38 Batch 7500/7662 eta: 1 day, 3:59:17.449020	Training Loss 24.0699 (23.8064)	Training Prec@1 0.000 (0.004)	Training Prec@5 0.000 (0.037)	
2022-04-05 16:59:08,895: ============================================================
2022-04-05 16:59:45,428: time cost, forward:0.06694365818416748, backward:0.12094094747304132, data cost:0.1884725866921404 
2022-04-05 16:59:45,429: ============================================================
2022-04-05 16:59:45,429: Epoch 3/38 Batch 7600/7662 eta: 1 day, 3:13:18.025700	Training Loss 24.6433 (23.8105)	Training Prec@1 0.000 (0.004)	Training Prec@5 0.000 (0.037)	
2022-04-05 16:59:45,430: ============================================================
2022-04-05 17:00:10,940: Epoch: 3/38 eta: 1 day, 3:12:55.008862	Training Loss 23.9448 (23.8119)	Training Prec@1 0.000 (0.004)	Training Prec@5 0.000 (0.037)
2022-04-05 17:00:10,940: ============================================================
2022-04-05 17:00:49,120: time cost, forward:0.0668099071040298, backward:0.12251996753191707, data cost:0.1917536619937781 
2022-04-05 17:00:49,121: ============================================================
2022-04-05 17:00:49,121: Epoch 4/38 Batch 100/7662 eta: 1 day, 4:17:07.518130	Training Loss 23.7990 (23.9879)	Training Prec@1 0.000 (0.004)	Training Prec@5 0.000 (0.014)	
2022-04-05 17:00:49,122: ============================================================
2022-04-05 17:01:26,851: time cost, forward:0.06693810913430985, backward:0.12195057844995853, data cost:0.18999421416814602 
2022-04-05 17:01:26,852: ============================================================
2022-04-05 17:01:26,852: Epoch 4/38 Batch 200/7662 eta: 1 day, 4:05:08.058005	Training Loss 23.5042 (23.9533)	Training Prec@1 0.000 (0.005)	Training Prec@5 0.000 (0.029)	
2022-04-05 17:01:26,852: ============================================================
2022-04-05 17:02:06,762: time cost, forward:0.06692198686376463, backward:0.12120307568323652, data cost:0.19730641132214397 
2022-04-05 17:02:06,763: ============================================================
2022-04-05 17:02:06,763: Epoch 4/38 Batch 300/7662 eta: 1 day, 5:41:49.268057	Training Loss 24.1595 (23.9761)	Training Prec@1 0.000 (0.005)	Training Prec@5 0.000 (0.029)	
2022-04-05 17:02:06,763: ============================================================
2022-04-05 17:02:46,541: time cost, forward:0.06690106296300291, backward:0.12088357117540556, data cost:0.2004910083044143 
2022-04-05 17:02:46,541: ============================================================
2022-04-05 17:02:46,542: Epoch 4/38 Batch 400/7662 eta: 1 day, 5:35:15.635386	Training Loss 24.0041 (23.9772)	Training Prec@1 0.000 (0.004)	Training Prec@5 0.000 (0.027)	
2022-04-05 17:02:46,542: ============================================================
2022-04-05 17:03:25,883: time cost, forward:0.06688780966168176, backward:0.12073062273686778, data cost:0.20178075400526393 
2022-04-05 17:03:25,884: ============================================================
2022-04-05 17:03:25,884: Epoch 4/38 Batch 500/7662 eta: 1 day, 5:15:08.219953	Training Loss 23.8476 (23.9999)	Training Prec@1 0.000 (0.006)	Training Prec@5 0.000 (0.029)	
2022-04-05 17:03:25,884: ============================================================
2022-04-05 17:04:04,078: time cost, forward:0.06687401371925622, backward:0.12087579004354589, data cost:0.20005235369496036 
2022-04-05 17:04:04,078: ============================================================
2022-04-05 17:04:04,078: Epoch 4/38 Batch 600/7662 eta: 1 day, 4:23:17.260063	Training Loss 24.3884 (24.0176)	Training Prec@1 0.000 (0.006)	Training Prec@5 0.000 (0.030)	
2022-04-05 17:04:04,079: ============================================================
2022-04-05 17:04:41,183: time cost, forward:0.06684576322421837, backward:0.12099515351444184, data cost:0.1977273640202862 
2022-04-05 17:04:41,184: ============================================================
2022-04-05 17:04:41,184: Epoch 4/38 Batch 700/7662 eta: 1 day, 3:34:06.708852	Training Loss 24.3375 (24.0349)	Training Prec@1 0.000 (0.005)	Training Prec@5 0.000 (0.030)	
2022-04-05 17:04:41,184: ============================================================
2022-04-05 17:05:18,384: time cost, forward:0.06684880501337732, backward:0.12115386549910258, data cost:0.19578422085662955 
2022-04-05 17:05:18,384: ============================================================
2022-04-05 17:05:18,385: Epoch 4/38 Batch 800/7662 eta: 1 day, 3:37:43.757499	Training Loss 24.3014 (24.0522)	Training Prec@1 0.000 (0.005)	Training Prec@5 0.000 (0.031)	
2022-04-05 17:05:18,385: ============================================================
2022-04-05 17:05:58,473: time cost, forward:0.06685837967377219, backward:0.12104534943721716, data cost:0.19761767748067854 
2022-04-05 17:05:58,474: ============================================================
2022-04-05 17:05:58,474: Epoch 4/38 Batch 900/7662 eta: 1 day, 5:45:47.272233	Training Loss 24.1837 (24.0620)	Training Prec@1 0.000 (0.005)	Training Prec@5 0.000 (0.031)	
2022-04-05 17:05:58,474: ============================================================
2022-04-05 17:06:38,079: time cost, forward:0.06685260323074844, backward:0.1210533782645866, data cost:0.19867958917512787 
2022-04-05 17:06:38,079: ============================================================
2022-04-05 17:06:38,080: Epoch 4/38 Batch 1000/7662 eta: 1 day, 5:23:34.547311	Training Loss 25.1336 (24.0868)	Training Prec@1 0.000 (0.006)	Training Prec@5 0.000 (0.034)	
2022-04-05 17:06:38,080: ============================================================
2022-04-05 17:07:16,341: time cost, forward:0.06687356560093582, backward:0.12112967679455022, data cost:0.19817537519474046 
2022-04-05 17:07:16,342: ============================================================
2022-04-05 17:07:16,342: Epoch 4/38 Batch 1100/7662 eta: 1 day, 4:23:08.400628	Training Loss 24.0149 (24.1306)	Training Prec@1 0.000 (0.007)	Training Prec@5 0.000 (0.034)	
2022-04-05 17:07:16,342: ============================================================
2022-04-05 17:07:53,780: time cost, forward:0.06687822850969456, backward:0.12126064380076251, data cost:0.1969496291115246 
2022-04-05 17:07:53,781: ============================================================
2022-04-05 17:07:53,781: Epoch 4/38 Batch 1200/7662 eta: 1 day, 3:45:50.339278	Training Loss 24.2991 (24.1501)	Training Prec@1 0.000 (0.006)	Training Prec@5 0.000 (0.034)	
2022-04-05 17:07:53,781: ============================================================
2022-04-05 17:08:31,221: time cost, forward:0.06690490383474161, backward:0.12132544752448407, data cost:0.19608002501877572 
2022-04-05 17:08:31,222: ============================================================
2022-04-05 17:08:31,223: Epoch 4/38 Batch 1300/7662 eta: 1 day, 3:45:20.731203	Training Loss 24.1875 (24.1444)	Training Prec@1 0.000 (0.006)	Training Prec@5 0.000 (0.032)	
2022-04-05 17:08:31,223: ============================================================
2022-04-05 17:09:09,109: time cost, forward:0.06690394955076773, backward:0.12132361652682389, data cost:0.1956072128015045 
2022-04-05 17:09:09,110: ============================================================
2022-04-05 17:09:09,110: Epoch 4/38 Batch 1400/7662 eta: 1 day, 4:04:32.719645	Training Loss 23.5041 (24.1401)	Training Prec@1 0.000 (0.006)	Training Prec@5 0.000 (0.032)	
2022-04-05 17:09:09,110: ============================================================
2022-04-05 17:09:47,131: time cost, forward:0.06691035403658184, backward:0.12133414018464295, data cost:0.19536679247524677 
2022-04-05 17:09:47,131: ============================================================
2022-04-05 17:09:47,132: Epoch 4/38 Batch 1500/7662 eta: 1 day, 4:09:53.522567	Training Loss 24.1537 (24.1327)	Training Prec@1 0.000 (0.005)	Training Prec@5 0.000 (0.030)	
2022-04-05 17:09:47,132: ============================================================
2022-04-05 17:10:25,254: time cost, forward:0.06692653525390052, backward:0.1213471299040832, data cost:0.19515697534118615 
2022-04-05 17:10:25,255: ============================================================
2022-04-05 17:10:25,255: Epoch 4/38 Batch 1600/7662 eta: 1 day, 4:13:45.519245	Training Loss 24.3286 (24.1262)	Training Prec@1 0.000 (0.005)	Training Prec@5 0.000 (0.029)	
2022-04-05 17:10:25,255: ============================================================
2022-04-05 17:11:05,326: time cost, forward:0.066942713413609, backward:0.12130829641017724, data cost:0.19615686045035396 
2022-04-05 17:11:05,327: ============================================================
2022-04-05 17:11:05,328: Epoch 4/38 Batch 1700/7662 eta: 1 day, 5:39:41.233131	Training Loss 24.1879 (24.1337)	Training Prec@1 0.195 (0.006)	Training Prec@5 0.195 (0.030)	
2022-04-05 17:11:05,328: ============================================================
2022-04-05 17:11:45,961: time cost, forward:0.06695189470712049, backward:0.12123331180740025, data cost:0.19738102688134676 
2022-04-05 17:11:45,962: ============================================================
2022-04-05 17:11:45,962: Epoch 4/38 Batch 1800/7662 eta: 1 day, 6:03:59.065788	Training Loss 24.3053 (24.1411)	Training Prec@1 0.000 (0.005)	Training Prec@5 0.000 (0.030)	
2022-04-05 17:11:45,962: ============================================================
2022-04-05 17:12:25,989: time cost, forward:0.06696020558735394, backward:0.12120655626293984, data cost:0.1981639313409051 
2022-04-05 17:12:25,989: ============================================================
2022-04-05 17:12:25,989: Epoch 4/38 Batch 1900/7662 eta: 1 day, 5:36:20.466984	Training Loss 23.8644 (24.1424)	Training Prec@1 0.000 (0.005)	Training Prec@5 0.000 (0.031)	
2022-04-05 17:12:25,989: ============================================================
2022-04-05 17:13:03,853: time cost, forward:0.06697194847003886, backward:0.12122366379474986, data cost:0.1977135164252277 
2022-04-05 17:13:03,854: ============================================================
2022-04-05 17:13:03,855: Epoch 4/38 Batch 2000/7662 eta: 1 day, 3:59:46.378332	Training Loss 23.9795 (24.1435)	Training Prec@1 0.000 (0.005)	Training Prec@5 0.000 (0.031)	
2022-04-05 17:13:03,855: ============================================================
2022-04-05 17:13:42,704: time cost, forward:0.06696686328962452, backward:0.121223286823638, data cost:0.19780522349222665 
2022-04-05 17:13:42,704: ============================================================
2022-04-05 17:13:42,704: Epoch 4/38 Batch 2100/7662 eta: 1 day, 4:42:47.981870	Training Loss 24.9365 (24.1711)	Training Prec@1 0.000 (0.005)	Training Prec@5 0.000 (0.031)	
2022-04-05 17:13:42,704: ============================================================
2022-04-05 17:14:19,466: time cost, forward:0.06696497759747473, backward:0.1212710103646036, data cost:0.1969306664122078 
2022-04-05 17:14:19,467: ============================================================
2022-04-05 17:14:19,467: Epoch 4/38 Batch 2200/7662 eta: 1 day, 3:09:37.713290	Training Loss 24.3018 (24.1906)	Training Prec@1 0.000 (0.005)	Training Prec@5 0.000 (0.030)	
2022-04-05 17:14:19,467: ============================================================
2022-04-05 17:14:57,331: time cost, forward:0.0669640028357454, backward:0.12129262884371485, data cost:0.1966150629359673 
2022-04-05 17:14:57,332: ============================================================
2022-04-05 17:14:57,332: Epoch 4/38 Batch 2300/7662 eta: 1 day, 3:57:52.677919	Training Loss 25.3346 (24.2020)	Training Prec@1 0.000 (0.006)	Training Prec@5 0.195 (0.030)	
2022-04-05 17:14:57,332: ============================================================
2022-04-05 17:15:34,560: time cost, forward:0.06697214072125313, backward:0.12134963286424885, data cost:0.19598297309557464 
2022-04-05 17:15:34,561: ============================================================
2022-04-05 17:15:34,561: Epoch 4/38 Batch 2400/7662 eta: 1 day, 3:29:03.802826	Training Loss 23.9812 (24.2023)	Training Prec@1 0.000 (0.005)	Training Prec@5 0.000 (0.030)	
2022-04-05 17:15:34,561: ============================================================
2022-04-05 17:16:10,903: time cost, forward:0.06698625342470972, backward:0.12141337791601627, data cost:0.19504458630452304 
2022-04-05 17:16:10,904: ============================================================
2022-04-05 17:16:10,904: Epoch 4/38 Batch 2500/7662 eta: 1 day, 2:49:12.999480	Training Loss 24.0318 (24.2275)	Training Prec@1 0.000 (0.005)	Training Prec@5 0.000 (0.029)	
2022-04-05 17:16:10,904: ============================================================
2022-04-05 17:16:48,537: time cost, forward:0.06698783960375432, backward:0.12143695184752408, data cost:0.19473012597251738 
2022-04-05 17:16:48,538: ============================================================
2022-04-05 17:16:48,538: Epoch 4/38 Batch 2600/7662 eta: 1 day, 3:45:43.915183	Training Loss 24.1461 (24.2244)	Training Prec@1 0.000 (0.005)	Training Prec@5 0.000 (0.029)	
2022-04-05 17:16:48,538: ============================================================
2022-04-05 17:17:26,615: time cost, forward:0.06699712349777533, backward:0.12143215827475481, data cost:0.1946162057391093 
2022-04-05 17:17:26,616: ============================================================
2022-04-05 17:17:26,616: Epoch 4/38 Batch 2700/7662 eta: 1 day, 4:04:46.364028	Training Loss 24.1974 (24.2237)	Training Prec@1 0.000 (0.005)	Training Prec@5 0.000 (0.028)	
2022-04-05 17:17:26,616: ============================================================
2022-04-05 17:18:04,364: time cost, forward:0.06700733995045795, backward:0.1214516795759075, data cost:0.19437009387886836 
2022-04-05 17:18:04,364: ============================================================
2022-04-05 17:18:04,365: Epoch 4/38 Batch 2800/7662 eta: 1 day, 3:49:33.944880	Training Loss 23.9132 (24.2291)	Training Prec@1 0.000 (0.005)	Training Prec@5 0.000 (0.028)	
2022-04-05 17:18:04,365: ============================================================
2022-04-05 17:18:43,778: time cost, forward:0.06700552648904201, backward:0.12142071529847666, data cost:0.19474199106875023 
2022-04-05 17:18:43,778: ============================================================
2022-04-05 17:18:43,778: Epoch 4/38 Batch 2900/7662 eta: 1 day, 5:02:33.447809	Training Loss 24.4354 (24.2431)	Training Prec@1 0.000 (0.005)	Training Prec@5 0.000 (0.028)	
2022-04-05 17:18:43,779: ============================================================
2022-04-05 17:19:22,322: time cost, forward:0.0670124840521741, backward:0.12140094140801043, data cost:0.19484707266936982 
2022-04-05 17:19:22,323: ============================================================
2022-04-05 17:19:22,323: Epoch 4/38 Batch 3000/7662 eta: 1 day, 4:23:29.689254	Training Loss 24.4232 (24.2932)	Training Prec@1 0.000 (0.005)	Training Prec@5 0.000 (0.027)	
2022-04-05 17:19:22,323: ============================================================
2022-04-05 17:20:01,440: time cost, forward:0.0670182719543004, backward:0.12138390348741569, data cost:0.19509168415925088 
2022-04-05 17:20:01,440: ============================================================
2022-04-05 17:20:01,440: Epoch 4/38 Batch 3100/7662 eta: 1 day, 4:48:08.321974	Training Loss 24.4129 (24.2992)	Training Prec@1 0.000 (0.005)	Training Prec@5 0.000 (0.027)	
2022-04-05 17:20:01,441: ============================================================
2022-04-05 17:20:39,516: time cost, forward:0.06701678967393909, backward:0.12139775276780315, data cost:0.19497739027797822 
2022-04-05 17:20:39,517: ============================================================
2022-04-05 17:20:39,517: Epoch 4/38 Batch 3200/7662 eta: 1 day, 4:01:31.661806	Training Loss 25.2283 (24.3046)	Training Prec@1 0.000 (0.005)	Training Prec@5 0.000 (0.027)	
2022-04-05 17:20:39,517: ============================================================
2022-04-05 17:21:17,525: time cost, forward:0.06702288847036093, backward:0.12141197433830139, data cost:0.19483151302875334 
2022-04-05 17:21:17,526: ============================================================
2022-04-05 17:21:17,526: Epoch 4/38 Batch 3300/7662 eta: 1 day, 3:57:55.383751	Training Loss 24.5441 (24.3367)	Training Prec@1 0.000 (0.005)	Training Prec@5 0.000 (0.027)	
2022-04-05 17:21:17,526: ============================================================
2022-04-05 17:21:57,973: time cost, forward:0.06702897828549068, backward:0.12135980331957358, data cost:0.19549750538214616 
2022-04-05 17:21:57,973: ============================================================
2022-04-05 17:21:57,974: Epoch 4/38 Batch 3400/7662 eta: 1 day, 5:44:53.491342	Training Loss 24.3455 (24.3704)	Training Prec@1 0.000 (0.005)	Training Prec@5 0.000 (0.027)	
2022-04-05 17:21:57,974: ============================================================
2022-04-05 17:22:37,250: time cost, forward:0.067040835696992, backward:0.12133033700110607, data cost:0.19574392635708504 
2022-04-05 17:22:37,251: ============================================================
2022-04-05 17:22:37,251: Epoch 4/38 Batch 3500/7662 eta: 1 day, 4:52:35.197057	Training Loss 24.1539 (24.3898)	Training Prec@1 0.000 (0.005)	Training Prec@5 0.000 (0.027)	
2022-04-05 17:22:37,251: ============================================================
2022-04-05 17:23:15,161: time cost, forward:0.06704273340469534, backward:0.12133784405421336, data cost:0.19560572086820738 
2022-04-05 17:23:15,162: ============================================================
2022-04-05 17:23:15,162: Epoch 4/38 Batch 3600/7662 eta: 1 day, 3:51:41.645572	Training Loss 23.8263 (24.3951)	Training Prec@1 0.000 (0.005)	Training Prec@5 0.195 (0.027)	
2022-04-05 17:23:15,162: ============================================================
2022-04-05 17:23:51,618: time cost, forward:0.06706025665532772, backward:0.12136264896676424, data cost:0.19500960643693027 
2022-04-05 17:23:51,618: ============================================================
2022-04-05 17:23:51,619: Epoch 4/38 Batch 3700/7662 eta: 1 day, 2:46:57.515937	Training Loss 24.3671 (24.3927)	Training Prec@1 0.000 (0.005)	Training Prec@5 0.000 (0.027)	
2022-04-05 17:23:51,619: ============================================================
2022-04-05 17:24:27,575: time cost, forward:0.06706595289045085, backward:0.12141417980068575, data cost:0.1943215856931184 
2022-04-05 17:24:27,576: ============================================================
2022-04-05 17:24:27,576: Epoch 4/38 Batch 3800/7662 eta: 1 day, 2:24:21.697698	Training Loss 24.1845 (24.3929)	Training Prec@1 0.000 (0.005)	Training Prec@5 0.000 (0.027)	
2022-04-05 17:24:27,577: ============================================================
2022-04-05 17:25:05,871: time cost, forward:0.0670648977431801, backward:0.12140958374602638, data cost:0.19430735741679134 
2022-04-05 17:25:05,872: ============================================================
2022-04-05 17:25:05,872: Epoch 4/38 Batch 3900/7662 eta: 1 day, 4:06:44.639984	Training Loss 24.2454 (24.3942)	Training Prec@1 0.000 (0.005)	Training Prec@5 0.000 (0.027)	
2022-04-05 17:25:05,872: ============================================================
2022-04-05 17:25:43,360: time cost, forward:0.06706075764918631, backward:0.12141043128118303, data cost:0.19409953525406803 
2022-04-05 17:25:43,360: ============================================================
2022-04-05 17:25:43,361: Epoch 4/38 Batch 4000/7662 eta: 1 day, 3:30:33.472300	Training Loss 24.0131 (24.4021)	Training Prec@1 0.000 (0.005)	Training Prec@5 0.000 (0.027)	
2022-04-05 17:25:43,361: ============================================================
2022-04-05 17:26:20,599: time cost, forward:0.06705891385140085, backward:0.12141257107156055, data cost:0.19383314296250578 
2022-04-05 17:26:20,599: ============================================================
2022-04-05 17:26:20,599: Epoch 4/38 Batch 4100/7662 eta: 1 day, 3:18:57.183956	Training Loss 23.9833 (24.3993)	Training Prec@1 0.000 (0.005)	Training Prec@5 0.586 (0.028)	
2022-04-05 17:26:20,600: ============================================================
2022-04-05 17:26:58,040: time cost, forward:0.06705641808978828, backward:0.1214181562638561, data cost:0.19363148117610515 
2022-04-05 17:26:58,041: ============================================================
2022-04-05 17:26:58,041: Epoch 4/38 Batch 4200/7662 eta: 1 day, 3:27:15.060562	Training Loss 24.3846 (24.3975)	Training Prec@1 0.000 (0.005)	Training Prec@5 0.195 (0.030)	
2022-04-05 17:26:58,042: ============================================================
2022-04-05 17:27:36,293: time cost, forward:0.06705819692520741, backward:0.12142000061934924, data cost:0.19362670151292793 
2022-04-05 17:27:36,295: ============================================================
2022-04-05 17:27:36,295: Epoch 4/38 Batch 4300/7662 eta: 1 day, 4:02:21.829368	Training Loss 23.9049 (24.3938)	Training Prec@1 0.000 (0.005)	Training Prec@5 0.000 (0.030)	
2022-04-05 17:27:36,296: ============================================================
2022-04-05 17:28:13,248: time cost, forward:0.06705943365805961, backward:0.12145240747486903, data cost:0.19328891843469284 
2022-04-05 17:28:13,249: ============================================================
2022-04-05 17:28:13,249: Epoch 4/38 Batch 4400/7662 eta: 1 day, 3:04:33.175015	Training Loss 24.1360 (24.3920)	Training Prec@1 0.000 (0.005)	Training Prec@5 0.195 (0.030)	
2022-04-05 17:28:13,249: ============================================================
2022-04-05 17:28:50,174: time cost, forward:0.06705639987130725, backward:0.12146026104072592, data cost:0.19300218687292786 
2022-04-05 17:28:50,175: ============================================================
2022-04-05 17:28:50,175: Epoch 4/38 Batch 4500/7662 eta: 1 day, 3:02:42.437750	Training Loss 24.4039 (24.3945)	Training Prec@1 0.000 (0.005)	Training Prec@5 0.000 (0.031)	
2022-04-05 17:28:50,175: ============================================================
2022-04-05 17:29:28,515: time cost, forward:0.06706214713179358, backward:0.12145506358452532, data cost:0.19293322379652847 
2022-04-05 17:29:28,516: ============================================================
2022-04-05 17:29:28,516: Epoch 4/38 Batch 4600/7662 eta: 1 day, 4:04:16.221710	Training Loss 23.8554 (24.3952)	Training Prec@1 0.000 (0.005)	Training Prec@5 0.195 (0.032)	
2022-04-05 17:29:28,516: ============================================================
2022-04-05 17:30:07,755: time cost, forward:0.06706512905379818, backward:0.1214497345613657, data cost:0.19325042760329034 
2022-04-05 17:30:07,756: ============================================================
2022-04-05 17:30:07,756: Epoch 4/38 Batch 4700/7662 eta: 1 day, 4:43:07.229640	Training Loss 23.7030 (24.3913)	Training Prec@1 0.000 (0.005)	Training Prec@5 0.000 (0.032)	
2022-04-05 17:30:07,757: ============================================================
2022-04-05 17:30:46,447: time cost, forward:0.06706126204727342, backward:0.12143090983981215, data cost:0.1933778108321371 
2022-04-05 17:30:46,447: ============================================================
2022-04-05 17:30:46,447: Epoch 4/38 Batch 4800/7662 eta: 1 day, 4:18:20.989108	Training Loss 23.9327 (24.3833)	Training Prec@1 0.000 (0.005)	Training Prec@5 0.000 (0.032)	
2022-04-05 17:30:46,448: ============================================================
2022-04-05 17:31:24,678: time cost, forward:0.06706112047729991, backward:0.1214392923972976, data cost:0.19336768018150602 
2022-04-05 17:31:24,679: ============================================================
2022-04-05 17:31:24,679: Epoch 4/38 Batch 4900/7662 eta: 1 day, 3:57:32.209214	Training Loss 23.9664 (24.3740)	Training Prec@1 0.000 (0.005)	Training Prec@5 0.000 (0.032)	
2022-04-05 17:31:24,679: ============================================================
2022-04-05 17:32:01,415: time cost, forward:0.06706068605917839, backward:0.12146216121810177, data cost:0.19303447524412415 
2022-04-05 17:32:01,416: ============================================================
2022-04-05 17:32:01,416: Epoch 4/38 Batch 5000/7662 eta: 1 day, 2:51:21.720669	Training Loss 24.1674 (24.3770)	Training Prec@1 0.000 (0.005)	Training Prec@5 0.000 (0.033)	
2022-04-05 17:32:01,416: ============================================================
2022-04-05 17:32:37,979: time cost, forward:0.06705799732144661, backward:0.12148103916918584, data cost:0.19270757914384362 
2022-04-05 17:32:37,979: ============================================================
2022-04-05 17:32:37,980: Epoch 4/38 Batch 5100/7662 eta: 1 day, 2:43:07.781749	Training Loss 24.0647 (24.3713)	Training Prec@1 0.000 (0.005)	Training Prec@5 0.000 (0.033)	
2022-04-05 17:32:37,980: ============================================================
2022-04-05 17:33:14,597: time cost, forward:0.06705537221137779, backward:0.12149384774297768, data cost:0.19239356137624405 
2022-04-05 17:33:14,598: ============================================================
2022-04-05 17:33:14,598: Epoch 4/38 Batch 5200/7662 eta: 1 day, 2:44:56.112737	Training Loss 24.1369 (24.3619)	Training Prec@1 0.000 (0.005)	Training Prec@5 0.195 (0.033)	
2022-04-05 17:33:14,598: ============================================================
2022-04-05 17:33:50,992: time cost, forward:0.06705200804879383, backward:0.12150367900411416, data cost:0.19205673277704283 
2022-04-05 17:33:50,993: ============================================================
2022-04-05 17:33:50,993: Epoch 4/38 Batch 5300/7662 eta: 1 day, 2:34:31.987516	Training Loss 23.7894 (24.3523)	Training Prec@1 0.000 (0.005)	Training Prec@5 0.000 (0.032)	
2022-04-05 17:33:50,993: ============================================================
2022-04-05 17:34:27,442: time cost, forward:0.06705269303050697, backward:0.12152188419081322, data cost:0.19172763316625577 
2022-04-05 17:34:27,442: ============================================================
2022-04-05 17:34:27,443: Epoch 4/38 Batch 5400/7662 eta: 1 day, 2:36:19.164436	Training Loss 24.2604 (24.3427)	Training Prec@1 0.000 (0.005)	Training Prec@5 0.000 (0.032)	
2022-04-05 17:34:27,443: ============================================================
2022-04-05 17:35:06,085: time cost, forward:0.06705636114483639, backward:0.12152398272110432, data cost:0.1918243735806468 
2022-04-05 17:35:06,085: ============================================================
2022-04-05 17:35:06,086: Epoch 4/38 Batch 5500/7662 eta: 1 day, 4:11:43.347392	Training Loss 23.9636 (24.3339)	Training Prec@1 0.000 (0.005)	Training Prec@5 0.000 (0.032)	
2022-04-05 17:35:06,086: ============================================================
2022-04-05 17:35:43,003: time cost, forward:0.06705465485398568, backward:0.12152774858994236, data cost:0.1916044686261746 
2022-04-05 17:35:43,003: ============================================================
2022-04-05 17:35:43,004: Epoch 4/38 Batch 5600/7662 eta: 1 day, 2:55:36.121996	Training Loss 24.4002 (24.3244)	Training Prec@1 0.000 (0.005)	Training Prec@5 0.000 (0.033)	
2022-04-05 17:35:43,004: ============================================================
2022-04-05 17:36:22,007: time cost, forward:0.06705481898556635, backward:0.12152745602820751, data cost:0.19178307263511046 
2022-04-05 17:36:22,008: ============================================================
2022-04-05 17:36:22,008: Epoch 4/38 Batch 5700/7662 eta: 1 day, 4:26:15.675557	Training Loss 24.2130 (24.3232)	Training Prec@1 0.000 (0.005)	Training Prec@5 0.000 (0.033)	
2022-04-05 17:36:22,008: ============================================================
2022-04-05 17:37:02,018: time cost, forward:0.06705550272560547, backward:0.12149757659729235, data cost:0.19212650488031016 
2022-04-05 17:37:02,018: ============================================================
2022-04-05 17:37:02,019: Epoch 4/38 Batch 5800/7662 eta: 1 day, 5:09:35.416499	Training Loss 24.2849 (24.3136)	Training Prec@1 0.000 (0.005)	Training Prec@5 0.000 (0.033)	
2022-04-05 17:37:02,019: ============================================================
2022-04-05 17:37:38,243: time cost, forward:0.06705610957018782, backward:0.1215091178449135, data cost:0.19179608850564808 
2022-04-05 17:37:38,243: ============================================================
2022-04-05 17:37:38,244: Epoch 4/38 Batch 5900/7662 eta: 1 day, 2:23:27.657619	Training Loss 24.4215 (24.3034)	Training Prec@1 0.000 (0.005)	Training Prec@5 0.000 (0.033)	
2022-04-05 17:37:38,244: ============================================================
2022-04-05 17:38:16,645: time cost, forward:0.06705675106840266, backward:0.12150826126679835, data cost:0.19184977186781343 
2022-04-05 17:38:16,645: ============================================================
2022-04-05 17:38:16,645: Epoch 4/38 Batch 6000/7662 eta: 1 day, 3:57:58.143461	Training Loss 23.9230 (24.3024)	Training Prec@1 0.000 (0.005)	Training Prec@5 0.195 (0.032)	
2022-04-05 17:38:16,645: ============================================================
2022-04-05 17:38:55,598: time cost, forward:0.06705417490763867, backward:0.1214961646755986, data cost:0.19201581629402698 
2022-04-05 17:38:55,599: ============================================================
2022-04-05 17:38:55,599: Epoch 4/38 Batch 6100/7662 eta: 1 day, 4:21:26.817419	Training Loss 23.7894 (24.2925)	Training Prec@1 0.000 (0.005)	Training Prec@5 0.000 (0.032)	
2022-04-05 17:38:55,599: ============================================================
2022-04-05 17:39:35,176: time cost, forward:0.06705257284543191, backward:0.12148865201162089, data cost:0.19225820565842913 
2022-04-05 17:39:35,176: ============================================================
2022-04-05 17:39:35,177: Epoch 4/38 Batch 6200/7662 eta: 1 day, 4:48:01.269875	Training Loss 23.6988 (24.2837)	Training Prec@1 0.000 (0.005)	Training Prec@5 0.391 (0.032)	
2022-04-05 17:39:35,177: ============================================================
2022-04-05 17:40:12,703: time cost, forward:0.06705679759503849, backward:0.12149313283848524, data cost:0.19215406283705475 
2022-04-05 17:40:12,703: ============================================================
2022-04-05 17:40:12,704: Epoch 4/38 Batch 6300/7662 eta: 1 day, 3:17:53.096741	Training Loss 23.8323 (24.2755)	Training Prec@1 0.000 (0.005)	Training Prec@5 0.195 (0.033)	
2022-04-05 17:40:12,704: ============================================================
2022-04-05 17:40:51,658: time cost, forward:0.06705976817957739, backward:0.12148258734129429, data cost:0.19228538141937662 
2022-04-05 17:40:51,659: ============================================================
2022-04-05 17:40:51,659: Epoch 4/38 Batch 6400/7662 eta: 1 day, 4:19:32.911574	Training Loss 24.1239 (24.2689)	Training Prec@1 0.000 (0.005)	Training Prec@5 0.000 (0.034)	
2022-04-05 17:40:51,659: ============================================================
2022-04-05 17:41:30,882: time cost, forward:0.0670638653769422, backward:0.12147086168146258, data cost:0.1924617868292275 
2022-04-05 17:41:30,883: ============================================================
2022-04-05 17:41:30,883: Epoch 4/38 Batch 6500/7662 eta: 1 day, 4:30:38.038234	Training Loss 23.6693 (24.2628)	Training Prec@1 0.000 (0.005)	Training Prec@5 0.195 (0.036)	
2022-04-05 17:41:30,883: ============================================================
2022-04-05 17:42:08,454: time cost, forward:0.06706548857136123, backward:0.12147030380209714, data cost:0.19235385896798643 
2022-04-05 17:42:08,455: ============================================================
2022-04-05 17:42:08,455: Epoch 4/38 Batch 6600/7662 eta: 1 day, 3:17:58.631740	Training Loss 23.7265 (24.2569)	Training Prec@1 0.000 (0.005)	Training Prec@5 0.195 (0.038)	
2022-04-05 17:42:08,456: ============================================================
2022-04-05 17:42:45,713: time cost, forward:0.06706398429221441, backward:0.12147807242141088, data cost:0.19222057088130728 
2022-04-05 17:42:45,713: ============================================================
2022-04-05 17:42:45,713: Epoch 4/38 Batch 6700/7662 eta: 1 day, 3:03:38.973382	Training Loss 23.9680 (24.2561)	Training Prec@1 0.000 (0.006)	Training Prec@5 0.391 (0.040)	
2022-04-05 17:42:45,714: ============================================================
2022-04-05 17:43:23,095: time cost, forward:0.06706406639190014, backward:0.12148473679730078, data cost:0.1921115159672803 
2022-04-05 17:43:23,095: ============================================================
2022-04-05 17:43:23,095: Epoch 4/38 Batch 6800/7662 eta: 1 day, 3:08:25.907797	Training Loss 24.2220 (24.2523)	Training Prec@1 0.000 (0.007)	Training Prec@5 0.391 (0.044)	
2022-04-05 17:43:23,096: ============================================================
2022-04-05 17:44:00,901: time cost, forward:0.0670671968948048, backward:0.12148437287120098, data cost:0.19207292930823094 
2022-04-05 17:44:00,902: ============================================================
2022-04-05 17:44:00,902: Epoch 4/38 Batch 6900/7662 eta: 1 day, 3:26:17.142061	Training Loss 24.0890 (24.2485)	Training Prec@1 0.000 (0.007)	Training Prec@5 0.000 (0.046)	
2022-04-05 17:44:00,902: ============================================================
2022-04-05 17:44:39,130: time cost, forward:0.06706575159175615, backward:0.12147893171886799, data cost:0.19209925473323974 
2022-04-05 17:44:39,130: ============================================================
2022-04-05 17:44:39,130: Epoch 4/38 Batch 7000/7662 eta: 1 day, 3:44:01.896172	Training Loss 23.3256 (24.2398)	Training Prec@1 0.000 (0.007)	Training Prec@5 0.586 (0.047)	
2022-04-05 17:44:39,131: ============================================================
2022-04-05 17:45:17,300: time cost, forward:0.06706800120265438, backward:0.12147150732863367, data cost:0.19210295281555975 
2022-04-05 17:45:17,300: ============================================================
2022-04-05 17:45:17,301: Epoch 4/38 Batch 7100/7662 eta: 1 day, 3:40:51.471269	Training Loss 24.0298 (24.2321)	Training Prec@1 0.000 (0.007)	Training Prec@5 0.000 (0.049)	
2022-04-05 17:45:17,301: ============================================================
2022-04-05 17:45:55,871: time cost, forward:0.06706793753036709, backward:0.12146965495942946, data cost:0.19217446208381042 
2022-04-05 17:45:55,871: ============================================================
2022-04-05 17:45:55,871: Epoch 4/38 Batch 7200/7662 eta: 1 day, 3:57:38.619440	Training Loss 24.0296 (24.2240)	Training Prec@1 0.000 (0.007)	Training Prec@5 0.000 (0.049)	
2022-04-05 17:45:55,872: ============================================================
2022-04-05 17:46:32,952: time cost, forward:0.06706834649497441, backward:0.12147794130453102, data cost:0.1920146217638866 
2022-04-05 17:46:32,953: ============================================================
2022-04-05 17:46:32,953: Epoch 4/38 Batch 7300/7662 eta: 1 day, 2:52:14.915456	Training Loss 23.4146 (24.2143)	Training Prec@1 0.391 (0.007)	Training Prec@5 0.391 (0.050)	
2022-04-05 17:46:32,953: ============================================================
2022-04-05 17:47:10,692: time cost, forward:0.0670671141748574, backward:0.12148541414281745, data cost:0.19196016019317713 
2022-04-05 17:47:10,693: ============================================================
2022-04-05 17:47:10,693: Epoch 4/38 Batch 7400/7662 eta: 1 day, 3:20:15.914854	Training Loss 24.4973 (24.2054)	Training Prec@1 0.000 (0.007)	Training Prec@5 0.000 (0.051)	
2022-04-05 17:47:10,694: ============================================================
2022-04-05 17:47:49,396: time cost, forward:0.06707369805908407, backward:0.12147305806265908, data cost:0.19205111025937288 
2022-04-05 17:47:49,396: ============================================================
2022-04-05 17:47:49,397: Epoch 4/38 Batch 7500/7662 eta: 1 day, 4:01:28.141792	Training Loss 23.5789 (24.1972)	Training Prec@1 0.000 (0.007)	Training Prec@5 0.000 (0.052)	
2022-04-05 17:47:49,397: ============================================================
2022-04-05 17:48:28,328: time cost, forward:0.06707549455966239, backward:0.12146694265803093, data cost:0.19214903214900553 
2022-04-05 17:48:28,328: ============================================================
2022-04-05 17:48:28,329: Epoch 4/38 Batch 7600/7662 eta: 1 day, 4:10:45.692861	Training Loss 23.2628 (24.1878)	Training Prec@1 0.000 (0.007)	Training Prec@5 0.195 (0.053)	
2022-04-05 17:48:28,329: ============================================================
2022-04-05 17:48:54,776: Epoch: 4/38 eta: 1 day, 4:10:21.165655	Training Loss 22.8604 (24.1822)	Training Prec@1 0.000 (0.007)	Training Prec@5 0.195 (0.054)
2022-04-05 17:48:54,777: ============================================================
2022-04-05 17:49:35,566: time cost, forward:0.06684122181902027, backward:0.11986100071608419, data cost:0.22184073804604887 
2022-04-05 17:49:35,566: ============================================================
2022-04-05 17:49:35,567: Epoch 5/38 Batch 100/7662 eta: 1 day, 5:26:54.207028	Training Loss 23.2520 (23.4212)	Training Prec@1 0.000 (0.002)	Training Prec@5 0.195 (0.105)	
2022-04-05 17:49:35,567: ============================================================
2022-04-05 17:50:15,600: time cost, forward:0.06706166986245007, backward:0.11967886392794662, data cost:0.2173115847697809 
2022-04-05 17:50:15,601: ============================================================
2022-04-05 17:50:15,601: Epoch 5/38 Batch 200/7662 eta: 1 day, 4:56:52.868841	Training Loss 23.9638 (23.4412)	Training Prec@1 0.195 (0.013)	Training Prec@5 0.391 (0.136)	
2022-04-05 17:50:15,601: ============================================================
2022-04-05 17:50:56,103: time cost, forward:0.06713021399584104, backward:0.11948882058312661, data cost:0.21758172185125957 
2022-04-05 17:50:56,104: ============================================================
2022-04-05 17:50:56,104: Epoch 5/38 Batch 300/7662 eta: 1 day, 5:16:32.904827	Training Loss 23.6028 (23.4945)	Training Prec@1 0.000 (0.018)	Training Prec@5 0.195 (0.188)	
2022-04-05 17:50:56,104: ============================================================
2022-04-05 17:51:35,926: time cost, forward:0.06718475119511884, backward:0.11964234254115208, data cost:0.21577536551874682 
2022-04-05 17:51:35,926: ============================================================
2022-04-05 17:51:35,927: Epoch 5/38 Batch 400/7662 eta: 1 day, 4:46:22.231458	Training Loss 23.6173 (23.5314)	Training Prec@1 0.000 (0.021)	Training Prec@5 0.000 (0.222)	
2022-04-05 17:51:35,927: ============================================================
2022-04-05 17:52:14,716: time cost, forward:0.06718825147242728, backward:0.1197900055405611, data cost:0.2127125631114524 
2022-04-05 17:52:14,716: ============================================================
2022-04-05 17:52:14,717: Epoch 5/38 Batch 500/7662 eta: 1 day, 4:00:57.194064	Training Loss 22.8689 (23.5165)	Training Prec@1 0.000 (0.019)	Training Prec@5 0.781 (0.229)	
2022-04-05 17:52:14,717: ============================================================
2022-04-05 17:52:53,661: time cost, forward:0.06719824666769954, backward:0.11982477567032701, data cost:0.21082611036221052 
2022-04-05 17:52:53,661: ============================================================
2022-04-05 17:52:53,662: Epoch 5/38 Batch 600/7662 eta: 1 day, 4:07:01.592099	Training Loss 23.7177 (23.5452)	Training Prec@1 0.000 (0.028)	Training Prec@5 0.391 (0.251)	
2022-04-05 17:52:53,662: ============================================================
2022-04-05 17:53:31,304: time cost, forward:0.06722051937010497, backward:0.11998668521941135, data cost:0.20741292949398188 
2022-04-05 17:53:31,305: ============================================================
2022-04-05 17:53:31,305: Epoch 5/38 Batch 700/7662 eta: 1 day, 3:10:00.654808	Training Loss 23.5653 (23.5446)	Training Prec@1 0.195 (0.030)	Training Prec@5 1.172 (0.284)	
2022-04-05 17:53:31,305: ============================================================
2022-04-05 17:54:11,592: time cost, forward:0.06723033113682524, backward:0.11991778661372217, data cost:0.2085133717862775 
2022-04-05 17:54:11,592: ============================================================
2022-04-05 17:54:11,593: Epoch 5/38 Batch 800/7662 eta: 1 day, 5:03:51.137430	Training Loss 23.5252 (23.5651)	Training Prec@1 0.000 (0.046)	Training Prec@5 0.781 (0.326)	
2022-04-05 17:54:11,593: ============================================================
2022-04-05 17:54:50,420: time cost, forward:0.06724338059430658, backward:0.11991643640435445, data cost:0.20758337094070384 
2022-04-05 17:54:50,421: ============================================================
2022-04-05 17:54:50,421: Epoch 5/38 Batch 900/7662 eta: 1 day, 4:00:01.262466	Training Loss 23.4994 (23.5683)	Training Prec@1 0.000 (0.043)	Training Prec@5 0.195 (0.343)	
2022-04-05 17:54:50,421: ============================================================
2022-04-05 17:55:30,160: time cost, forward:0.06746795895817999, backward:0.11987334686714608, data cost:0.20764434325683104 
2022-04-05 17:55:30,160: ============================================================
2022-04-05 17:55:30,160: Epoch 5/38 Batch 1000/7662 eta: 1 day, 4:38:47.923800	Training Loss 23.4099 (23.5521)	Training Prec@1 0.000 (0.040)	Training Prec@5 0.391 (0.362)	
2022-04-05 17:55:30,161: ============================================================
2022-04-05 17:56:09,363: time cost, forward:0.06768909186206154, backward:0.11983190459267891, data cost:0.2068861982191119 
2022-04-05 17:56:09,364: ============================================================
2022-04-05 17:56:09,365: Epoch 5/38 Batch 1100/7662 eta: 1 day, 4:14:58.452629	Training Loss 23.6659 (23.5406)	Training Prec@1 0.000 (0.037)	Training Prec@5 0.586 (0.380)	
2022-04-05 17:56:09,365: ============================================================
2022-04-05 17:56:47,863: time cost, forward:0.06780003745720921, backward:0.11984002798969692, data cost:0.20616077084258957 
2022-04-05 17:56:47,864: ============================================================
2022-04-05 17:56:47,864: Epoch 5/38 Batch 1200/7662 eta: 1 day, 3:43:52.867004	Training Loss 23.3008 (23.5406)	Training Prec@1 0.000 (0.035)	Training Prec@5 0.195 (0.366)	
2022-04-05 17:56:47,864: ============================================================
2022-04-05 17:57:26,558: time cost, forward:0.06772995178657279, backward:0.11988722075858053, data cost:0.2056119240091983 
2022-04-05 17:57:26,559: ============================================================
2022-04-05 17:57:26,559: Epoch 5/38 Batch 1300/7662 eta: 1 day, 3:51:41.075642	Training Loss 23.3684 (23.5241)	Training Prec@1 0.000 (0.033)	Training Prec@5 0.000 (0.351)	
2022-04-05 17:57:26,559: ============================================================
2022-04-05 17:58:06,415: time cost, forward:0.06769582354400394, backward:0.11989672240229314, data cost:0.20594892491605812 
2022-04-05 17:58:06,416: ============================================================
2022-04-05 17:58:06,416: Epoch 5/38 Batch 1400/7662 eta: 1 day, 4:41:13.679213	Training Loss 23.2184 (23.5143)	Training Prec@1 0.000 (0.032)	Training Prec@5 0.195 (0.339)	
2022-04-05 17:58:06,416: ============================================================
2022-04-05 17:58:44,942: time cost, forward:0.06776026919175976, backward:0.11994516205358219, data cost:0.20529483889960862 
2022-04-05 17:58:44,943: ============================================================
2022-04-05 17:58:44,943: Epoch 5/38 Batch 1500/7662 eta: 1 day, 3:43:07.761250	Training Loss 23.2516 (23.5003)	Training Prec@1 0.000 (0.030)	Training Prec@5 0.195 (0.334)	
2022-04-05 17:58:44,943: ============================================================
2022-04-05 17:59:23,704: time cost, forward:0.06793351036224461, backward:0.11993641015363529, data cost:0.20468869456803523 
2022-04-05 17:59:23,705: ============================================================
2022-04-05 17:59:23,706: Epoch 5/38 Batch 1600/7662 eta: 1 day, 3:52:39.613594	Training Loss 24.5748 (23.4965)	Training Prec@1 0.000 (0.029)	Training Prec@5 0.000 (0.333)	
2022-04-05 17:59:23,706: ============================================================
2022-04-05 18:00:02,340: time cost, forward:0.06806614147767802, backward:0.11992457447646995, data cost:0.20418310881081997 
2022-04-05 18:00:02,340: ============================================================
2022-04-05 18:00:02,341: Epoch 5/38 Batch 1700/7662 eta: 1 day, 3:46:30.786782	Training Loss 23.2063 (23.5028)	Training Prec@1 0.000 (0.028)	Training Prec@5 0.195 (0.332)	
2022-04-05 18:00:02,341: ============================================================
2022-04-05 18:00:40,441: time cost, forward:0.06804724200822831, backward:0.11995075648860709, data cost:0.20355795979035968 
2022-04-05 18:00:40,442: ============================================================
2022-04-05 18:00:40,442: Epoch 5/38 Batch 1800/7662 eta: 1 day, 3:22:52.243495	Training Loss 24.2997 (23.5010)	Training Prec@1 0.000 (0.027)	Training Prec@5 0.000 (0.325)	
2022-04-05 18:00:40,442: ============================================================
2022-04-05 18:01:19,542: time cost, forward:0.06805100473621885, backward:0.11999521373509733, data cost:0.20343151074952362 
2022-04-05 18:01:19,543: ============================================================
2022-04-05 18:01:19,543: Epoch 5/38 Batch 1900/7662 eta: 1 day, 4:05:17.677447	Training Loss 23.2542 (23.5118)	Training Prec@1 0.000 (0.027)	Training Prec@5 0.195 (0.320)	
2022-04-05 18:01:19,543: ============================================================
2022-04-05 18:01:57,780: time cost, forward:0.0680203716894458, backward:0.12001648934380062, data cost:0.20293376396870005 
2022-04-05 18:01:57,781: ============================================================
2022-04-05 18:01:57,781: Epoch 5/38 Batch 2000/7662 eta: 1 day, 3:27:29.741158	Training Loss 23.7614 (23.5189)	Training Prec@1 0.000 (0.026)	Training Prec@5 0.195 (0.316)	
2022-04-05 18:01:57,781: ============================================================
2022-04-05 18:02:37,393: time cost, forward:0.06798088488094917, backward:0.11999857987944088, data cost:0.203244659774584 
2022-04-05 18:02:37,393: ============================================================
2022-04-05 18:02:37,393: Epoch 5/38 Batch 2100/7662 eta: 1 day, 4:26:01.748714	Training Loss 26.2426 (23.5228)	Training Prec@1 0.000 (0.025)	Training Prec@5 0.000 (0.312)	
2022-04-05 18:02:37,394: ============================================================
2022-04-05 18:03:16,105: time cost, forward:0.06793978443032994, backward:0.11999926679835855, data cost:0.20308589967829144 
2022-04-05 18:03:16,105: ============================================================
2022-04-05 18:03:16,106: Epoch 5/38 Batch 2200/7662 eta: 1 day, 3:46:36.741159	Training Loss 23.3024 (23.5189)	Training Prec@1 0.000 (0.024)	Training Prec@5 0.000 (0.301)	
2022-04-05 18:03:16,106: ============================================================
2022-04-05 18:03:54,548: time cost, forward:0.06790073657357314, backward:0.1200077508832642, data cost:0.20278411598089416 
2022-04-05 18:03:54,548: ============================================================
2022-04-05 18:03:54,548: Epoch 5/38 Batch 2300/7662 eta: 1 day, 3:34:22.476534	Training Loss 22.7496 (23.4996)	Training Prec@1 0.000 (0.023)	Training Prec@5 0.000 (0.288)	
2022-04-05 18:03:54,548: ============================================================
2022-04-05 18:04:34,743: time cost, forward:0.06787251988864532, backward:0.11997690693742785, data cost:0.20330327846150242 
2022-04-05 18:04:34,744: ============================================================
2022-04-05 18:04:34,744: Epoch 5/38 Batch 2400/7662 eta: 1 day, 4:49:09.201591	Training Loss 22.9149 (23.4861)	Training Prec@1 0.000 (0.022)	Training Prec@5 0.000 (0.277)	
2022-04-05 18:04:34,744: ============================================================
2022-04-05 18:05:13,377: time cost, forward:0.06784095865290085, backward:0.11998465634575363, data cost:0.2031007501877704 
2022-04-05 18:05:13,378: ============================================================
2022-04-05 18:05:13,378: Epoch 5/38 Batch 2500/7662 eta: 1 day, 3:41:18.568548	Training Loss 23.3534 (23.4791)	Training Prec@1 0.000 (0.021)	Training Prec@5 0.000 (0.266)	
2022-04-05 18:05:13,378: ============================================================
2022-04-05 18:05:51,671: time cost, forward:0.06780897970151883, backward:0.12001227259223118, data cost:0.20275136139265343 
2022-04-05 18:05:51,672: ============================================================
2022-04-05 18:05:51,672: Epoch 5/38 Batch 2600/7662 eta: 1 day, 3:26:03.712812	Training Loss 23.7358 (23.4664)	Training Prec@1 0.000 (0.021)	Training Prec@5 0.000 (0.256)	
2022-04-05 18:05:51,672: ============================================================
2022-04-05 18:06:32,043: time cost, forward:0.06777209103129536, backward:0.11997506016932315, data cost:0.2033212676936938 
2022-04-05 18:06:32,044: ============================================================
2022-04-05 18:06:32,044: Epoch 5/38 Batch 2700/7662 eta: 1 day, 4:54:42.761910	Training Loss 22.7093 (23.4577)	Training Prec@1 0.000 (0.020)	Training Prec@5 0.000 (0.247)	
2022-04-05 18:06:32,044: ============================================================
2022-04-05 18:07:12,732: time cost, forward:0.06774378350991443, backward:0.11993923439388404, data cost:0.20390849856233206 
2022-04-05 18:07:12,732: ============================================================
2022-04-05 18:07:12,732: Epoch 5/38 Batch 2800/7662 eta: 1 day, 5:07:37.309394	Training Loss 23.2533 (23.4670)	Training Prec@1 0.000 (0.019)	Training Prec@5 0.000 (0.239)	
2022-04-05 18:07:12,732: ============================================================
2022-04-05 18:07:50,846: time cost, forward:0.06772552781698826, backward:0.119956371002092, data cost:0.20354841371452698 
2022-04-05 18:07:50,846: ============================================================
2022-04-05 18:07:50,847: Epoch 5/38 Batch 2900/7662 eta: 1 day, 3:16:25.878789	Training Loss 23.0257 (23.4665)	Training Prec@1 0.000 (0.019)	Training Prec@5 0.000 (0.231)	
2022-04-05 18:07:50,847: ============================================================
2022-04-05 18:08:30,090: time cost, forward:0.06770745242743065, backward:0.1199812716585511, data cost:0.2035818015706265 
2022-04-05 18:08:30,090: ============================================================
2022-04-05 18:08:30,090: Epoch 5/38 Batch 3000/7662 eta: 1 day, 4:04:16.403315	Training Loss 23.3787 (23.4718)	Training Prec@1 0.000 (0.018)	Training Prec@5 0.000 (0.225)	
2022-04-05 18:08:30,090: ============================================================
2022-04-05 18:09:08,233: time cost, forward:0.06769983719994692, backward:0.12001945080777143, data cost:0.20318477628461082 
2022-04-05 18:09:08,233: ============================================================
2022-04-05 18:09:08,234: Epoch 5/38 Batch 3100/7662 eta: 1 day, 3:16:25.259974	Training Loss 24.5986 (23.4794)	Training Prec@1 0.000 (0.017)	Training Prec@5 0.000 (0.218)	
2022-04-05 18:09:08,235: ============================================================
2022-04-05 18:09:46,884: time cost, forward:0.06767780678687374, backward:0.12001818841455429, data cost:0.2030646561011183 
2022-04-05 18:09:46,884: ============================================================
2022-04-05 18:09:46,884: Epoch 5/38 Batch 3200/7662 eta: 1 day, 3:37:30.509090	Training Loss 23.2546 (23.4856)	Training Prec@1 0.000 (0.017)	Training Prec@5 0.000 (0.212)	
2022-04-05 18:09:46,884: ============================================================
2022-04-05 18:10:26,531: time cost, forward:0.06766268013823788, backward:0.12001500399989336, data cost:0.20324776460995056 
2022-04-05 18:10:26,531: ============================================================
2022-04-05 18:10:26,532: Epoch 5/38 Batch 3300/7662 eta: 1 day, 4:19:36.533639	Training Loss 24.1101 (23.4814)	Training Prec@1 0.000 (0.017)	Training Prec@5 0.000 (0.207)	
2022-04-05 18:10:26,532: ============================================================
2022-04-05 18:11:06,735: time cost, forward:0.06765286029524999, backward:0.12000905965909989, data cost:0.2035843877099338 
2022-04-05 18:11:06,735: ============================================================
2022-04-05 18:11:06,736: Epoch 5/38 Batch 3400/7662 eta: 1 day, 4:42:48.556580	Training Loss 23.6328 (23.4840)	Training Prec@1 0.000 (0.016)	Training Prec@5 0.000 (0.201)	
2022-04-05 18:11:06,736: ============================================================
2022-04-05 18:11:46,177: time cost, forward:0.06763992455387906, backward:0.11999608816913823, data cost:0.20368200078900456 
2022-04-05 18:11:46,177: ============================================================
2022-04-05 18:11:46,177: Epoch 5/38 Batch 3500/7662 eta: 1 day, 4:09:28.661107	Training Loss 24.5251 (23.4884)	Training Prec@1 0.000 (0.016)	Training Prec@5 0.000 (0.196)	
2022-04-05 18:11:46,178: ============================================================
2022-04-05 18:12:26,003: time cost, forward:0.06763107858124161, backward:0.11999002295290308, data cost:0.2038759973255188 
2022-04-05 18:12:26,003: ============================================================
2022-04-05 18:12:26,004: Epoch 5/38 Batch 3600/7662 eta: 1 day, 4:25:16.779182	Training Loss 23.3676 (23.4950)	Training Prec@1 0.000 (0.016)	Training Prec@5 0.000 (0.191)	
2022-04-05 18:12:26,004: ============================================================
2022-04-05 18:13:05,276: time cost, forward:0.06762118080429981, backward:0.11998615093442613, data cost:0.2039081507613447 
2022-04-05 18:13:05,276: ============================================================
2022-04-05 18:13:05,276: Epoch 5/38 Batch 3700/7662 eta: 1 day, 4:00:55.914045	Training Loss 23.4260 (23.4902)	Training Prec@1 0.000 (0.015)	Training Prec@5 0.000 (0.187)	
2022-04-05 18:13:05,276: ============================================================
2022-04-05 18:13:42,599: time cost, forward:0.06760623932637864, backward:0.12001136128103523, data cost:0.20340982930664642 
2022-04-05 18:13:42,600: ============================================================
2022-04-05 18:13:42,600: Epoch 5/38 Batch 3800/7662 eta: 1 day, 2:36:53.572939	Training Loss 23.1909 (23.4858)	Training Prec@1 0.000 (0.015)	Training Prec@5 0.000 (0.183)	
2022-04-05 18:13:42,600: ============================================================
2022-04-05 18:14:20,691: time cost, forward:0.06759435734891928, backward:0.12002366755246566, data cost:0.20314079078230132 
2022-04-05 18:14:20,692: ============================================================
2022-04-05 18:14:20,692: Epoch 5/38 Batch 3900/7662 eta: 1 day, 3:09:08.164106	Training Loss 23.5589 (23.4832)	Training Prec@1 0.000 (0.015)	Training Prec@5 0.000 (0.179)	
2022-04-05 18:14:20,693: ============================================================
2022-04-05 18:14:59,007: time cost, forward:0.0675820437453037, backward:0.12003038495324439, data cost:0.20294264090839223 
2022-04-05 18:14:59,008: ============================================================
2022-04-05 18:14:59,008: Epoch 5/38 Batch 4000/7662 eta: 1 day, 3:18:03.245349	Training Loss 23.3911 (23.4951)	Training Prec@1 0.000 (0.014)	Training Prec@5 0.000 (0.175)	
2022-04-05 18:14:59,008: ============================================================
2022-04-05 18:15:35,887: time cost, forward:0.06757580189915452, backward:0.1200730638348146, data cost:0.20236412773309145 
2022-04-05 18:15:35,888: ============================================================
2022-04-05 18:15:35,888: Epoch 5/38 Batch 4100/7662 eta: 1 day, 2:16:03.213399	Training Loss 23.2591 (23.4883)	Training Prec@1 0.000 (0.014)	Training Prec@5 0.000 (0.170)	
2022-04-05 18:15:35,888: ============================================================
2022-04-05 18:16:13,104: time cost, forward:0.06757241494600533, backward:0.12010431590606042, data cost:0.20189751685020554 
2022-04-05 18:16:13,104: ============================================================
2022-04-05 18:16:13,104: Epoch 5/38 Batch 4200/7662 eta: 1 day, 2:29:49.073493	Training Loss 23.1922 (23.4848)	Training Prec@1 0.000 (0.014)	Training Prec@5 0.000 (0.167)	
2022-04-05 18:16:13,105: ============================================================
2022-04-05 18:16:52,010: time cost, forward:0.06756583844043122, backward:0.12010226168724569, data cost:0.20188349766852162 
2022-04-05 18:16:52,011: ============================================================
2022-04-05 18:16:52,011: Epoch 5/38 Batch 4300/7662 eta: 1 day, 3:41:21.553516	Training Loss 23.5973 (23.4811)	Training Prec@1 0.000 (0.013)	Training Prec@5 0.000 (0.163)	
2022-04-05 18:16:52,011: ============================================================
2022-04-05 18:17:29,854: time cost, forward:0.06756399463160143, backward:0.12012457972251439, data cost:0.20153441952694326 
2022-04-05 18:17:29,854: ============================================================
2022-04-05 18:17:29,855: Epoch 5/38 Batch 4400/7662 eta: 1 day, 2:55:21.422491	Training Loss 22.9464 (23.4796)	Training Prec@1 0.000 (0.013)	Training Prec@5 0.000 (0.160)	
2022-04-05 18:17:29,855: ============================================================
2022-04-05 18:18:09,048: time cost, forward:0.06755913090880751, backward:0.12012544073616566, data cost:0.2016487984319188 
2022-04-05 18:18:09,049: ============================================================
2022-04-05 18:18:09,049: Epoch 5/38 Batch 4500/7662 eta: 1 day, 3:52:21.848442	Training Loss 22.9843 (23.4764)	Training Prec@1 0.000 (0.013)	Training Prec@5 0.000 (0.157)	
2022-04-05 18:18:09,049: ============================================================
2022-04-05 18:18:47,934: time cost, forward:0.06754968310988813, backward:0.12012836922249293, data cost:0.20162280006599467 
2022-04-05 18:18:47,935: ============================================================
2022-04-05 18:18:47,935: Epoch 5/38 Batch 4600/7662 eta: 1 day, 3:38:32.540459	Training Loss 23.3491 (23.4767)	Training Prec@1 0.000 (0.013)	Training Prec@5 0.000 (0.154)	
2022-04-05 18:18:47,935: ============================================================
2022-04-05 18:19:25,747: time cost, forward:0.06754574275417616, backward:0.12014121587135915, data cost:0.2013430152453877 
2022-04-05 18:19:25,747: ============================================================
2022-04-05 18:19:25,747: Epoch 5/38 Batch 4700/7662 eta: 1 day, 2:52:07.064546	Training Loss 23.0407 (23.4727)	Training Prec@1 0.000 (0.012)	Training Prec@5 0.000 (0.151)	
2022-04-05 18:19:25,747: ============================================================
2022-04-05 18:20:03,134: time cost, forward:0.06754011133110108, backward:0.12017544499783, data cost:0.20102251428046508 
2022-04-05 18:20:03,134: ============================================================
2022-04-05 18:20:03,135: Epoch 5/38 Batch 4800/7662 eta: 1 day, 2:33:22.765372	Training Loss 23.0839 (23.4703)	Training Prec@1 0.000 (0.012)	Training Prec@5 0.000 (0.148)	
2022-04-05 18:20:03,135: ============================================================
2022-04-05 18:20:39,513: time cost, forward:0.0675484712183731, backward:0.12021210919255795, data cost:0.20045852661132812 
2022-04-05 18:20:39,514: ============================================================
2022-04-05 18:20:39,514: Epoch 5/38 Batch 4900/7662 eta: 1 day, 1:49:48.308173	Training Loss 24.1681 (23.4693)	Training Prec@1 0.000 (0.012)	Training Prec@5 0.000 (0.145)	
2022-04-05 18:20:39,514: ============================================================
2022-04-05 18:21:16,236: time cost, forward:0.06755151875521283, backward:0.12024626242539768, data cost:0.19999731385867628 
2022-04-05 18:21:16,236: ============================================================
2022-04-05 18:21:16,237: Epoch 5/38 Batch 5000/7662 eta: 1 day, 2:03:50.186413	Training Loss 24.4827 (23.4702)	Training Prec@1 0.000 (0.012)	Training Prec@5 0.000 (0.143)	
2022-04-05 18:21:16,237: ============================================================
2022-04-05 18:21:51,414: time cost, forward:0.06754360977026219, backward:0.12029911920495491, data cost:0.19923929056248774 
2022-04-05 18:21:51,415: ============================================================
2022-04-05 18:21:51,415: Epoch 5/38 Batch 5100/7662 eta: 1 day, 0:57:28.252927	Training Loss 23.3117 (23.4686)	Training Prec@1 0.000 (0.012)	Training Prec@5 0.000 (0.141)	
2022-04-05 18:21:51,415: ============================================================
2022-04-05 18:22:27,640: time cost, forward:0.06754054668248216, backward:0.12033636736443327, data cost:0.19871563158440667 
2022-04-05 18:22:27,641: ============================================================
2022-04-05 18:22:27,641: Epoch 5/38 Batch 5200/7662 eta: 1 day, 1:41:28.869746	Training Loss 23.6163 (23.4671)	Training Prec@1 0.000 (0.012)	Training Prec@5 0.000 (0.139)	
2022-04-05 18:22:27,641: ============================================================
2022-04-05 18:23:03,659: time cost, forward:0.06753333835562213, backward:0.12037842510825937, data cost:0.19816930317883222 
2022-04-05 18:23:03,660: ============================================================
2022-04-05 18:23:03,660: Epoch 5/38 Batch 5300/7662 eta: 1 day, 1:32:03.603894	Training Loss 23.0468 (23.4663)	Training Prec@1 0.000 (0.011)	Training Prec@5 0.000 (0.137)	
2022-04-05 18:23:03,660: ============================================================
2022-04-05 18:23:40,064: time cost, forward:0.0675255695875761, backward:0.12040240593719624, data cost:0.1977342737451883 
2022-04-05 18:23:40,064: ============================================================
2022-04-05 18:23:40,064: Epoch 5/38 Batch 5400/7662 eta: 1 day, 1:47:50.795854	Training Loss 23.3277 (23.4773)	Training Prec@1 0.000 (0.011)	Training Prec@5 0.000 (0.135)	
2022-04-05 18:23:40,065: ============================================================
2022-04-05 18:24:18,273: time cost, forward:0.06751686465764051, backward:0.12040971226856088, data cost:0.1976600049690542 
2022-04-05 18:24:18,273: ============================================================
2022-04-05 18:24:18,273: Epoch 5/38 Batch 5500/7662 eta: 1 day, 3:03:55.770159	Training Loss 23.0892 (23.4781)	Training Prec@1 0.000 (0.011)	Training Prec@5 0.000 (0.133)	
2022-04-05 18:24:18,273: ============================================================
2022-04-05 18:24:55,454: time cost, forward:0.06751138292310238, backward:0.12042364709652796, data cost:0.19740179398801203 
2022-04-05 18:24:55,454: ============================================================
2022-04-05 18:24:55,454: Epoch 5/38 Batch 5600/7662 eta: 1 day, 2:19:38.002741	Training Loss 23.2628 (23.4775)	Training Prec@1 0.000 (0.011)	Training Prec@5 0.000 (0.131)	
2022-04-05 18:24:55,454: ============================================================
2022-04-05 18:25:32,043: time cost, forward:0.06750759272684986, backward:0.12045252153718235, data cost:0.1970254959066953 
2022-04-05 18:25:32,043: ============================================================
2022-04-05 18:25:32,044: Epoch 5/38 Batch 5700/7662 eta: 1 day, 1:53:53.114149	Training Loss 23.7601 (23.4768)	Training Prec@1 0.000 (0.011)	Training Prec@5 0.000 (0.129)	
2022-04-05 18:25:32,044: ============================================================
2022-04-05 18:26:10,733: time cost, forward:0.06750694552995683, backward:0.12045121349164341, data cost:0.19704665626240386 
2022-04-05 18:26:10,734: ============================================================
2022-04-05 18:26:10,734: Epoch 5/38 Batch 5800/7662 eta: 1 day, 3:22:28.191164	Training Loss 23.5845 (23.4754)	Training Prec@1 0.000 (0.011)	Training Prec@5 0.000 (0.128)	
2022-04-05 18:26:10,734: ============================================================
2022-04-05 18:26:48,169: time cost, forward:0.06749832038051659, backward:0.12045738215688165, data cost:0.1968595598042668 
2022-04-05 18:26:48,169: ============================================================
2022-04-05 18:26:48,169: Epoch 5/38 Batch 5900/7662 eta: 1 day, 2:28:33.172809	Training Loss 23.2756 (23.4743)	Training Prec@1 0.000 (0.011)	Training Prec@5 0.195 (0.126)	
2022-04-05 18:26:48,169: ============================================================
