2022-03-28 08:58:24,829: [('name', 'amsoft-36'), ('backbone_model_name', 'SimpleResnet_36'), ('classify_model_name', 'Sphereface2'), ('resume_net_model', '/home/ubuntu/data2/lk/recognition/pytorch_new/snapshot/SR_36_ddp_sphereface2/backbone_25_checkpoint.pth'), ('resume_net_classifier', '/home/ubuntu/data2/lk/recognition/pytorch_new/snapshot/SR_36_ddp_sphereface2/classifier_status_25_checkpoint.pth'), ('no_cuda', False), ('gpu_num', 1), ('log_interval', 100), ('log_path', './logs/SR_36_ddp_sphereface2_25continue.log'), ('log_pic_path', './logs/pic/SR_36_ddp_sphereface2_25continue/'), ('save_path', 'snapshot/SR_36_ddp_sphereface2_25continue/'), ('lmdb_path', '/home/ubuntu/data4/lk/data/lmdb_default'), ('batch_size', 512), ('datanum', 3923399), ('num_class', 86876), ('lmdb_workers', 4), ('num_workers', 4), ('start_epoch', 1), ('max_epoch', 45), ('lr', 0.01), ('base', 'epoch'), ('step_size', [10, 20, 30]), ('momentum', 0.9), ('gama', 0.1), ('weight_decay', 0.0005), ('rank', 0), ('dist_url', 'env://'), ('world_size', 2), ('gpu', 0), ('dist_backend', 'nccl'), ('distributed', True), ('master_port', 22000), ('multiprocessing_distributed', False), ('SEED', 1337), ('local_rank', 0)]
2022-03-28 08:58:24,829: SimpleResidualBackbone(
  (conv1): ConvPrelu(
    (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (prelu): PReLU(num_parameters=64)
  )
  (layer1): Sequential(
    (0): SimpleResidualUnit(
      (conv1): ConvPrelu(
        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=64)
      )
      (conv2): ConvPrelu(
        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=64)
      )
    )
    (1): SimpleResidualUnit(
      (conv1): ConvPrelu(
        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=64)
      )
      (conv2): ConvPrelu(
        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=64)
      )
    )
  )
  (conv2): ConvPrelu(
    (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (prelu): PReLU(num_parameters=128)
  )
  (layer2): Sequential(
    (0): SimpleResidualUnit(
      (conv1): ConvPrelu(
        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=128)
      )
      (conv2): ConvPrelu(
        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=128)
      )
    )
    (1): SimpleResidualUnit(
      (conv1): ConvPrelu(
        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=128)
      )
      (conv2): ConvPrelu(
        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=128)
      )
    )
    (2): SimpleResidualUnit(
      (conv1): ConvPrelu(
        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=128)
      )
      (conv2): ConvPrelu(
        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=128)
      )
    )
    (3): SimpleResidualUnit(
      (conv1): ConvPrelu(
        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=128)
      )
      (conv2): ConvPrelu(
        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=128)
      )
    )
  )
  (conv3): ConvPrelu(
    (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (prelu): PReLU(num_parameters=256)
  )
  (layer3): Sequential(
    (0): SimpleResidualUnit(
      (conv1): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
      (conv2): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
    )
    (1): SimpleResidualUnit(
      (conv1): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
      (conv2): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
    )
    (2): SimpleResidualUnit(
      (conv1): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
      (conv2): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
    )
    (3): SimpleResidualUnit(
      (conv1): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
      (conv2): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
    )
    (4): SimpleResidualUnit(
      (conv1): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
      (conv2): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
    )
    (5): SimpleResidualUnit(
      (conv1): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
      (conv2): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
    )
    (6): SimpleResidualUnit(
      (conv1): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
      (conv2): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
    )
    (7): SimpleResidualUnit(
      (conv1): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
      (conv2): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
    )
  )
  (conv4): ConvPrelu(
    (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (prelu): PReLU(num_parameters=512)
  )
  (layer4): Sequential(
    (0): SimpleResidualUnit(
      (conv1): ConvPrelu(
        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=512)
      )
      (conv2): ConvPrelu(
        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=512)
      )
    )
    (1): SimpleResidualUnit(
      (conv1): ConvPrelu(
        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=512)
      )
      (conv2): ConvPrelu(
        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=512)
      )
    )
  )
  (fc5): Linear(in_features=25088, out_features=512, bias=True)
)
2022-03-28 08:58:25,173: Loading resume (model) network...
2022-03-28 08:58:26,344: resume net (model) loaded
2022-03-28 08:58:26,344: Loading resume (classifier) network...
2022-03-28 08:58:29,911: start epoch: 25
2022-03-28 08:58:30,315: resume net (classifier) loaded
2022-03-28 08:58:30,488: data balance
2022-03-28 08:59:05,358: time cost, forward:0.11337325789711693, backward:0.040937647675022934, data cost:0.19350126295378714 
2022-03-28 08:59:05,359: ============================================================
2022-03-28 08:59:05,359: Epoch 25/45 Batch 100/7662 eta: 15:28:41.498803	Training Loss 0.4352 (0.4370)	Training Prec@1 91.797 (91.643)	Training Prec@5 94.141 (94.642)	
2022-03-28 08:59:05,359: ============================================================
2022-03-28 08:59:37,770: time cost, forward:0.11092624113188317, backward:0.036435785006039105, data cost:0.1883541842800888 
2022-03-28 08:59:37,771: ============================================================
2022-03-28 08:59:37,771: Epoch 25/45 Batch 200/7662 eta: 14:28:07.245922	Training Loss 0.4389 (0.4362)	Training Prec@1 92.773 (91.762)	Training Prec@5 96.094 (94.765)	
2022-03-28 08:59:37,771: ============================================================
2022-03-28 09:00:10,294: time cost, forward:0.10972384625055319, backward:0.034648532452790634, data cost:0.18768138231641074 
2022-03-28 09:00:10,294: ============================================================
2022-03-28 09:00:10,294: Epoch 25/45 Batch 300/7662 eta: 14:30:33.533237	Training Loss 0.4251 (0.4362)	Training Prec@1 93.555 (91.739)	Training Prec@5 96.094 (94.783)	
2022-03-28 09:00:10,295: ============================================================
2022-03-28 09:00:43,000: time cost, forward:0.10933646104090794, backward:0.03395542285794901, data cost:0.18742504095971435 
2022-03-28 09:00:43,000: ============================================================
2022-03-28 09:00:43,000: Epoch 25/45 Batch 400/7662 eta: 14:34:54.006473	Training Loss 0.4361 (0.4365)	Training Prec@1 91.406 (91.744)	Training Prec@5 94.141 (94.799)	
2022-03-28 09:00:43,001: ============================================================
2022-03-28 09:01:16,024: time cost, forward:0.10987817357202809, backward:0.03353569407262401, data cost:0.1871405264179788 
2022-03-28 09:01:16,024: ============================================================
2022-03-28 09:01:16,025: Epoch 25/45 Batch 500/7662 eta: 14:42:51.715300	Training Loss 0.4438 (0.4366)	Training Prec@1 90.820 (91.735)	Training Prec@5 93.359 (94.821)	
2022-03-28 09:01:16,025: ============================================================
2022-03-28 09:01:49,277: time cost, forward:0.11042515343935143, backward:0.03346527717348331, data cost:0.18692611215110613 
2022-03-28 09:01:49,277: ============================================================
2022-03-28 09:01:49,278: Epoch 25/45 Batch 600/7662 eta: 14:48:25.945695	Training Loss 0.4358 (0.4367)	Training Prec@1 93.359 (91.692)	Training Prec@5 96.680 (94.782)	
2022-03-28 09:01:49,278: ============================================================
2022-03-28 09:02:22,444: time cost, forward:0.1101946254315465, backward:0.03343026900666637, data cost:0.18724866827499542 
2022-03-28 09:02:22,444: ============================================================
2022-03-28 09:02:22,444: Epoch 25/45 Batch 700/7662 eta: 14:45:33.873284	Training Loss 0.4432 (0.4366)	Training Prec@1 91.992 (91.690)	Training Prec@5 96.094 (94.780)	
2022-03-28 09:02:22,445: ============================================================
2022-03-28 09:02:55,793: time cost, forward:0.11003651189266964, backward:0.03322302713262871, data cost:0.18789516462104042 
2022-03-28 09:02:55,793: ============================================================
2022-03-28 09:02:55,793: Epoch 25/45 Batch 800/7662 eta: 14:49:52.912044	Training Loss 0.4271 (0.4366)	Training Prec@1 92.188 (91.673)	Training Prec@5 94.531 (94.773)	
2022-03-28 09:02:55,794: ============================================================
2022-03-28 09:03:29,370: time cost, forward:0.10989148333022274, backward:0.03300182599246966, data cost:0.18873665887070976 
2022-03-28 09:03:29,371: ============================================================
2022-03-28 09:03:29,371: Epoch 25/45 Batch 900/7662 eta: 14:55:24.730541	Training Loss 0.4389 (0.4365)	Training Prec@1 92.188 (91.680)	Training Prec@5 95.312 (94.779)	
2022-03-28 09:03:29,371: ============================================================
2022-03-28 09:04:03,222: time cost, forward:0.10995796564463023, backward:0.032885312556743145, data cost:0.18943153940760218 
2022-03-28 09:04:03,223: ============================================================
2022-03-28 09:04:03,223: Epoch 25/45 Batch 1000/7662 eta: 15:02:10.600687	Training Loss 0.4409 (0.4366)	Training Prec@1 91.797 (91.675)	Training Prec@5 95.117 (94.777)	
2022-03-28 09:04:03,223: ============================================================
2022-03-28 09:04:37,519: time cost, forward:0.10982720411507188, backward:0.032795558743741536, data cost:0.19060183504259076 
2022-03-28 09:04:37,519: ============================================================
2022-03-28 09:04:37,520: Epoch 25/45 Batch 1100/7662 eta: 15:13:27.111687	Training Loss 0.4406 (0.4367)	Training Prec@1 93.555 (91.672)	Training Prec@5 96.094 (94.765)	
2022-03-28 09:04:37,520: ============================================================
2022-03-28 09:05:11,740: time cost, forward:0.10982938584334856, backward:0.03284484550692421, data cost:0.19126274826329784 
2022-03-28 09:05:11,740: ============================================================
2022-03-28 09:05:11,741: Epoch 25/45 Batch 1200/7662 eta: 15:10:51.871035	Training Loss 0.4529 (0.4368)	Training Prec@1 88.477 (91.640)	Training Prec@5 91.992 (94.752)	
2022-03-28 09:05:11,741: ============================================================
2022-03-28 09:05:47,058: time cost, forward:0.10979291142822689, backward:0.03311302663731153, data cost:0.19248589871019653 
2022-03-28 09:05:47,059: ============================================================
2022-03-28 09:05:47,059: Epoch 25/45 Batch 1300/7662 eta: 15:39:29.223519	Training Loss 0.4414 (0.4367)	Training Prec@1 89.258 (91.638)	Training Prec@5 94.141 (94.754)	
2022-03-28 09:05:47,059: ============================================================
2022-03-28 09:06:22,835: time cost, forward:0.10978210713711699, backward:0.03322191729214977, data cost:0.1939585024838451 
2022-03-28 09:06:22,836: ============================================================
2022-03-28 09:06:22,836: Epoch 25/45 Batch 1400/7662 eta: 15:51:05.645303	Training Loss 0.4411 (0.4368)	Training Prec@1 91.992 (91.630)	Training Prec@5 93.945 (94.751)	
2022-03-28 09:06:22,836: ============================================================
2022-03-28 09:06:59,436: time cost, forward:0.10982818997963975, backward:0.033315548346470485, data cost:0.19572814390450974 
2022-03-28 09:06:59,436: ============================================================
2022-03-28 09:06:59,437: Epoch 25/45 Batch 1500/7662 eta: 16:12:22.896419	Training Loss 0.4485 (0.4368)	Training Prec@1 87.500 (91.630)	Training Prec@5 92.578 (94.754)	
2022-03-28 09:06:59,438: ============================================================
2022-03-28 09:07:34,777: time cost, forward:0.10986369963807564, backward:0.03335146250912664, data cost:0.1965347013300549 
2022-03-28 09:07:34,778: ============================================================
2022-03-28 09:07:34,778: Epoch 25/45 Batch 1600/7662 eta: 15:38:19.111559	Training Loss 0.4328 (0.4368)	Training Prec@1 91.406 (91.622)	Training Prec@5 95.117 (94.754)	
2022-03-28 09:07:34,778: ============================================================
2022-03-28 09:08:12,161: time cost, forward:0.11000958535866291, backward:0.03347364841593652, data cost:0.1982540515958035 
2022-03-28 09:08:12,162: ============================================================
2022-03-28 09:08:12,162: Epoch 25/45 Batch 1700/7662 eta: 16:31:56.190374	Training Loss 0.4384 (0.4368)	Training Prec@1 91.602 (91.632)	Training Prec@5 95.117 (94.759)	
2022-03-28 09:08:12,162: ============================================================
2022-03-28 09:08:48,203: time cost, forward:0.11002173776292615, backward:0.03338989779444255, data cost:0.19932715027381342 
2022-03-28 09:08:48,203: ============================================================
2022-03-28 09:08:48,203: Epoch 25/45 Batch 1800/7662 eta: 15:55:43.188688	Training Loss 0.4459 (0.4368)	Training Prec@1 91.016 (91.636)	Training Prec@5 93.945 (94.772)	
2022-03-28 09:08:48,204: ============================================================
2022-03-28 09:09:24,680: time cost, forward:0.10999627587919551, backward:0.0332978380674309, data cost:0.20058933303756674 
2022-03-28 09:09:24,680: ============================================================
2022-03-28 09:09:24,680: Epoch 25/45 Batch 1900/7662 eta: 16:06:39.386317	Training Loss 0.4362 (0.4368)	Training Prec@1 92.188 (91.632)	Training Prec@5 95.312 (94.765)	
2022-03-28 09:09:24,680: ============================================================
2022-03-28 09:10:03,251: time cost, forward:0.10995721840870386, backward:0.03324916852957729, data cost:0.20275262071228792 
2022-03-28 09:10:03,252: ============================================================
2022-03-28 09:10:03,252: Epoch 25/45 Batch 2000/7662 eta: 17:01:31.935255	Training Loss 0.4236 (0.4368)	Training Prec@1 92.578 (91.626)	Training Prec@5 96.094 (94.758)	
2022-03-28 09:10:03,252: ============================================================
2022-03-28 09:10:39,479: time cost, forward:0.10999147059407445, backward:0.03323880941201074, data cost:0.20347233611439 
2022-03-28 09:10:39,480: ============================================================
2022-03-28 09:10:39,480: Epoch 25/45 Batch 2100/7662 eta: 15:58:50.667608	Training Loss 0.4288 (0.4368)	Training Prec@1 92.383 (91.621)	Training Prec@5 95.898 (94.751)	
2022-03-28 09:10:39,480: ============================================================
2022-03-28 09:11:17,614: time cost, forward:0.11007044791307923, backward:0.03337297855913233, data cost:0.20482622660957395 
2022-03-28 09:11:17,615: ============================================================
2022-03-28 09:11:17,615: Epoch 25/45 Batch 2200/7662 eta: 16:48:42.034168	Training Loss 0.4293 (0.4368)	Training Prec@1 91.992 (91.624)	Training Prec@5 95.703 (94.756)	
2022-03-28 09:11:17,616: ============================================================
2022-03-28 09:11:55,487: time cost, forward:0.11017484828771432, backward:0.033509484785128905, data cost:0.20590397045372777 
2022-03-28 09:11:55,488: ============================================================
2022-03-28 09:11:55,488: Epoch 25/45 Batch 2300/7662 eta: 16:41:07.168765	Training Loss 0.4307 (0.4368)	Training Prec@1 94.531 (91.615)	Training Prec@5 96.289 (94.753)	
2022-03-28 09:11:55,488: ============================================================
2022-03-28 09:12:32,538: time cost, forward:0.11017776579099578, backward:0.033470843920562605, data cost:0.2067793181857848 
2022-03-28 09:12:32,538: ============================================================
2022-03-28 09:12:32,539: Epoch 25/45 Batch 2400/7662 eta: 16:18:46.179957	Training Loss 0.4349 (0.4368)	Training Prec@1 92.188 (91.602)	Training Prec@5 94.531 (94.743)	
2022-03-28 09:12:32,539: ============================================================
2022-03-28 09:13:11,027: time cost, forward:0.11019903500111593, backward:0.03349547874646074, data cost:0.20809603853672207 
2022-03-28 09:13:11,027: ============================================================
2022-03-28 09:13:11,027: Epoch 25/45 Batch 2500/7662 eta: 16:56:07.710028	Training Loss 0.4370 (0.4369)	Training Prec@1 92.383 (91.587)	Training Prec@5 95.117 (94.733)	
2022-03-28 09:13:11,028: ============================================================
2022-03-28 09:13:49,204: time cost, forward:0.11018967600958583, backward:0.03354980396096089, data cost:0.20919295667271837 
2022-03-28 09:13:49,204: ============================================================
2022-03-28 09:13:49,204: Epoch 25/45 Batch 2600/7662 eta: 16:47:15.161635	Training Loss 0.4394 (0.4369)	Training Prec@1 90.430 (91.586)	Training Prec@5 93.555 (94.732)	
2022-03-28 09:13:49,205: ============================================================
2022-03-28 09:14:28,511: time cost, forward:0.11016148300071962, backward:0.03356362652363447, data cost:0.2106833574550158 
2022-03-28 09:14:28,511: ============================================================
2022-03-28 09:14:28,512: Epoch 25/45 Batch 2700/7662 eta: 17:16:25.348331	Training Loss 0.4390 (0.4369)	Training Prec@1 93.359 (91.583)	Training Prec@5 95.312 (94.731)	
2022-03-28 09:14:28,512: ============================================================
2022-03-28 09:15:07,828: time cost, forward:0.11014158481272172, backward:0.033562033548658346, data cost:0.21205858163809768 
2022-03-28 09:15:07,829: ============================================================
2022-03-28 09:15:07,829: Epoch 25/45 Batch 2800/7662 eta: 17:16:01.752543	Training Loss 0.4359 (0.4369)	Training Prec@1 90.234 (91.584)	Training Prec@5 92.969 (94.728)	
2022-03-28 09:15:07,829: ============================================================
2022-03-28 09:15:47,763: time cost, forward:0.11010530431667827, backward:0.033580424992040586, data cost:0.21358014765012753 
2022-03-28 09:15:47,764: ============================================================
2022-03-28 09:15:47,764: Epoch 25/45 Batch 2900/7662 eta: 17:31:38.713060	Training Loss 0.4380 (0.4369)	Training Prec@1 90.625 (91.579)	Training Prec@5 94.141 (94.727)	
2022-03-28 09:15:47,764: ============================================================
2022-03-28 09:16:25,811: time cost, forward:0.11006258049977943, backward:0.033601919703978064, data cost:0.2143365704802602 
2022-03-28 09:16:25,811: ============================================================
2022-03-28 09:16:25,812: Epoch 25/45 Batch 3000/7662 eta: 16:41:17.970583	Training Loss 0.4314 (0.4369)	Training Prec@1 91.992 (91.580)	Training Prec@5 93.750 (94.723)	
2022-03-28 09:16:25,812: ============================================================
2022-03-28 09:17:04,954: time cost, forward:0.11001493708477285, backward:0.033569659675925574, data cost:0.2154983979496274 
2022-03-28 09:17:04,955: ============================================================
2022-03-28 09:17:04,955: Epoch 25/45 Batch 3100/7662 eta: 17:09:29.892096	Training Loss 0.4389 (0.4369)	Training Prec@1 92.383 (91.581)	Training Prec@5 94.922 (94.725)	
2022-03-28 09:17:04,955: ============================================================
2022-03-28 09:17:44,348: time cost, forward:0.10996199727393792, backward:0.03355480850245961, data cost:0.2166519336455984 
2022-03-28 09:17:44,349: ============================================================
2022-03-28 09:17:44,349: Epoch 25/45 Batch 3200/7662 eta: 17:15:25.523664	Training Loss 0.4421 (0.4370)	Training Prec@1 92.188 (91.578)	Training Prec@5 95.117 (94.724)	
2022-03-28 09:17:44,349: ============================================================
2022-03-28 09:18:23,453: time cost, forward:0.1099139612781239, backward:0.033547689640800245, data cost:0.21762346664895865 
2022-03-28 09:18:23,453: ============================================================
2022-03-28 09:18:23,454: Epoch 25/45 Batch 3300/7662 eta: 17:07:09.674339	Training Loss 0.4380 (0.4370)	Training Prec@1 92.383 (91.578)	Training Prec@5 95.117 (94.722)	
2022-03-28 09:18:23,454: ============================================================
2022-03-28 09:19:01,934: time cost, forward:0.10988571546329544, backward:0.03353496873052585, data cost:0.21834976163181216 
2022-03-28 09:19:01,935: ============================================================
2022-03-28 09:19:01,935: Epoch 25/45 Batch 3400/7662 eta: 16:50:09.583270	Training Loss 0.4334 (0.4370)	Training Prec@1 91.602 (91.572)	Training Prec@5 95.312 (94.718)	
2022-03-28 09:19:01,935: ============================================================
2022-03-28 09:19:44,078: time cost, forward:0.10987344971041095, backward:0.03351825780615053, data cost:0.2200773055296552 
2022-03-28 09:19:44,078: ============================================================
2022-03-28 09:19:44,078: Epoch 25/45 Batch 3500/7662 eta: 18:25:34.861370	Training Loss 0.4420 (0.4370)	Training Prec@1 89.648 (91.568)	Training Prec@5 94.141 (94.717)	
2022-03-28 09:19:44,079: ============================================================
2022-03-28 09:20:24,518: time cost, forward:0.10984065314735694, backward:0.03351736128346792, data cost:0.22123870925129568 
2022-03-28 09:20:24,519: ============================================================
2022-03-28 09:20:24,519: Epoch 25/45 Batch 3600/7662 eta: 17:40:14.151251	Training Loss 0.4354 (0.4370)	Training Prec@1 91.406 (91.563)	Training Prec@5 93.945 (94.714)	
2022-03-28 09:20:24,519: ============================================================
2022-03-28 09:21:04,431: time cost, forward:0.10979857932042031, backward:0.03353349180858372, data cost:0.22218318396756764 
2022-03-28 09:21:04,432: ============================================================
2022-03-28 09:21:04,432: Epoch 25/45 Batch 3700/7662 eta: 17:25:44.212050	Training Loss 0.4285 (0.4371)	Training Prec@1 93.164 (91.561)	Training Prec@5 94.727 (94.713)	
2022-03-28 09:21:04,432: ============================================================
2022-03-28 09:21:45,334: time cost, forward:0.1097545688420291, backward:0.03353340734334706, data cost:0.2233710171015961 
2022-03-28 09:21:45,335: ============================================================
2022-03-28 09:21:45,335: Epoch 25/45 Batch 3800/7662 eta: 17:50:59.860874	Training Loss 0.4399 (0.4371)	Training Prec@1 91.211 (91.560)	Training Prec@5 95.117 (94.716)	
2022-03-28 09:21:45,335: ============================================================
2022-03-28 09:22:24,051: time cost, forward:0.10972989275076965, backward:0.033496749312426986, data cost:0.22394776460481747 
2022-03-28 09:22:24,052: ============================================================
2022-03-28 09:22:24,052: Epoch 25/45 Batch 3900/7662 eta: 16:53:07.359480	Training Loss 0.4313 (0.4371)	Training Prec@1 90.625 (91.555)	Training Prec@5 95.312 (94.712)	
2022-03-28 09:22:24,052: ============================================================
2022-03-28 09:23:06,433: time cost, forward:0.10970903891687185, backward:0.033480623955427335, data cost:0.22537868992928775 
2022-03-28 09:23:06,434: ============================================================
2022-03-28 09:23:06,434: Epoch 25/45 Batch 4000/7662 eta: 18:28:18.260938	Training Loss 0.4342 (0.4371)	Training Prec@1 92.188 (91.556)	Training Prec@5 94.922 (94.713)	
2022-03-28 09:23:06,434: ============================================================
2022-03-28 09:23:46,563: time cost, forward:0.10972008038567578, backward:0.03347886542222302, data cost:0.22617288134859434 
2022-03-28 09:23:46,564: ============================================================
2022-03-28 09:23:46,564: Epoch 25/45 Batch 4100/7662 eta: 17:28:45.120344	Training Loss 0.4302 (0.4371)	Training Prec@1 92.188 (91.556)	Training Prec@5 95.312 (94.714)	
2022-03-28 09:23:46,564: ============================================================
2022-03-28 09:24:27,816: time cost, forward:0.10971868688306742, backward:0.03346752933048867, data cost:0.22717666688434396 
2022-03-28 09:24:27,816: ============================================================
2022-03-28 09:24:27,816: Epoch 25/45 Batch 4200/7662 eta: 17:57:23.499425	Training Loss 0.4324 (0.4371)	Training Prec@1 93.359 (91.555)	Training Prec@5 95.898 (94.713)	
2022-03-28 09:24:27,816: ============================================================
2022-03-28 09:25:08,222: time cost, forward:0.1097057080318773, backward:0.0334772440299624, data cost:0.22797882692790025 
2022-03-28 09:25:08,223: ============================================================
2022-03-28 09:25:08,223: Epoch 25/45 Batch 4300/7662 eta: 17:34:38.261054	Training Loss 0.4481 (0.4371)	Training Prec@1 90.430 (91.558)	Training Prec@5 92.969 (94.716)	
2022-03-28 09:25:08,223: ============================================================
2022-03-28 09:25:49,958: time cost, forward:0.11022492245290193, backward:0.03348341761027772, data cost:0.22849546602894757 
2022-03-28 09:25:49,979: ============================================================
2022-03-28 09:25:49,979: Epoch 25/45 Batch 4400/7662 eta: 18:09:10.037518	Training Loss 0.4293 (0.4371)	Training Prec@1 91.797 (91.557)	Training Prec@5 95.117 (94.714)	
2022-03-28 09:25:49,980: ============================================================
2022-03-28 09:26:30,849: time cost, forward:0.11084087347025129, backward:0.03350752299296694, data cost:0.22866358415739726 
2022-03-28 09:26:30,850: ============================================================
2022-03-28 09:26:30,851: Epoch 25/45 Batch 4500/7662 eta: 17:45:23.645844	Training Loss 0.4515 (0.4371)	Training Prec@1 90.234 (91.555)	Training Prec@5 94.336 (94.712)	
2022-03-28 09:26:30,851: ============================================================
2022-03-28 09:27:16,138: time cost, forward:0.11162477634502095, backward:0.0335241159321718, data cost:0.22958337516726607 
2022-03-28 09:27:16,139: ============================================================
2022-03-28 09:27:16,140: Epoch 25/45 Batch 4600/7662 eta: 19:39:48.238263	Training Loss 0.4415 (0.4371)	Training Prec@1 90.234 (91.546)	Training Prec@5 94.531 (94.706)	
2022-03-28 09:27:16,140: ============================================================
2022-03-28 09:27:57,313: time cost, forward:0.11210328615784873, backward:0.03356294237215181, data cost:0.2298484988150685 
2022-03-28 09:27:57,313: ============================================================
2022-03-28 09:27:57,314: Epoch 25/45 Batch 4700/7662 eta: 17:51:55.283206	Training Loss 0.4417 (0.4371)	Training Prec@1 89.453 (91.549)	Training Prec@5 92.773 (94.709)	
2022-03-28 09:27:57,314: ============================================================
2022-03-28 09:28:39,892: time cost, forward:0.11258743578852602, backward:0.033603593095388726, data cost:0.23036808355521798 
2022-03-28 09:28:39,892: ============================================================
2022-03-28 09:28:39,892: Epoch 25/45 Batch 4800/7662 eta: 18:27:46.463288	Training Loss 0.4310 (0.4371)	Training Prec@1 92.578 (91.549)	Training Prec@5 94.727 (94.708)	
2022-03-28 09:28:39,893: ============================================================
2022-03-28 09:29:21,030: time cost, forward:0.11255830388868261, backward:0.03359096272669658, data cost:0.2311145507892703 
2022-03-28 09:29:21,030: ============================================================
2022-03-28 09:29:21,031: Epoch 25/45 Batch 4900/7662 eta: 17:49:36.582967	Training Loss 0.4346 (0.4372)	Training Prec@1 88.867 (91.546)	Training Prec@5 93.164 (94.707)	
2022-03-28 09:29:21,031: ============================================================
2022-03-28 09:30:02,592: time cost, forward:0.11247648437348907, backward:0.03356372544612377, data cost:0.23197764772299553 
2022-03-28 09:30:02,593: ============================================================
2022-03-28 09:30:02,593: Epoch 25/45 Batch 5000/7662 eta: 17:59:56.827170	Training Loss 0.4304 (0.4371)	Training Prec@1 92.578 (91.549)	Training Prec@5 96.094 (94.705)	
2022-03-28 09:30:02,593: ============================================================
2022-03-28 09:30:42,065: time cost, forward:0.11244796734039492, backward:0.033574541261743675, data cost:0.2323002003529278 
2022-03-28 09:30:42,065: ============================================================
2022-03-28 09:30:42,066: Epoch 25/45 Batch 5100/7662 eta: 17:04:59.880531	Training Loss 0.4331 (0.4371)	Training Prec@1 93.164 (91.550)	Training Prec@5 96.289 (94.709)	
2022-03-28 09:30:42,066: ============================================================
2022-03-28 09:31:35,943: time cost, forward:0.11513409111953328, backward:0.03387858024306608, data cost:0.2323880864235272 
2022-03-28 09:31:35,944: ============================================================
2022-03-28 09:31:35,944: Epoch 25/45 Batch 5200/7662 eta: 23:18:09.940049	Training Loss 0.4252 (0.4372)	Training Prec@1 91.406 (91.549)	Training Prec@5 94.922 (94.710)	
2022-03-28 09:31:35,944: ============================================================
2022-03-28 09:32:22,887: time cost, forward:0.11589985322133396, backward:0.033977674614553026, data cost:0.2331726324290549 
2022-03-28 09:32:22,887: ============================================================
2022-03-28 09:32:22,888: Epoch 25/45 Batch 5300/7662 eta: 20:17:25.874260	Training Loss 0.4356 (0.4372)	Training Prec@1 91.992 (91.551)	Training Prec@5 94.531 (94.712)	
2022-03-28 09:32:22,888: ============================================================
2022-03-28 09:33:04,024: time cost, forward:0.1157626804984881, backward:0.03395754824216376, data cost:0.2338559424839631 
2022-03-28 09:33:04,025: ============================================================
2022-03-28 09:33:04,025: Epoch 25/45 Batch 5400/7662 eta: 17:46:10.040396	Training Loss 0.4364 (0.4371)	Training Prec@1 89.453 (91.557)	Training Prec@5 92.578 (94.715)	
2022-03-28 09:33:04,025: ============================================================
2022-03-28 09:33:43,190: time cost, forward:0.11560851900246734, backward:0.03392942651095532, data cost:0.23417340375917958 
2022-03-28 09:33:43,190: ============================================================
2022-03-28 09:33:43,191: Epoch 25/45 Batch 5500/7662 eta: 16:54:24.355180	Training Loss 0.4290 (0.4372)	Training Prec@1 93.945 (91.553)	Training Prec@5 97.070 (94.714)	
2022-03-28 09:33:43,191: ============================================================
2022-03-28 09:34:24,744: time cost, forward:0.11546358229284052, backward:0.033908078193153564, data cost:0.23483883084772228 
2022-03-28 09:34:24,745: ============================================================
2022-03-28 09:34:24,745: Epoch 25/45 Batch 5600/7662 eta: 17:55:35.632266	Training Loss 0.4388 (0.4372)	Training Prec@1 89.453 (91.551)	Training Prec@5 92.578 (94.712)	
2022-03-28 09:34:24,745: ============================================================
2022-03-28 09:35:07,957: time cost, forward:0.11579404104841573, backward:0.03389126456689575, data cost:0.23541531367853327 
2022-03-28 09:35:07,971: ============================================================
2022-03-28 09:35:07,971: Epoch 25/45 Batch 5700/7662 eta: 18:38:07.694768	Training Loss 0.4350 (0.4372)	Training Prec@1 90.039 (91.551)	Training Prec@5 93.750 (94.712)	
2022-03-28 09:35:07,971: ============================================================
2022-03-28 09:35:53,985: time cost, forward:0.11636106325318267, backward:0.033886292532078664, data cost:0.23614558241946632 
2022-03-28 09:35:53,985: ============================================================
2022-03-28 09:35:53,986: Epoch 25/45 Batch 5800/7662 eta: 19:49:29.911201	Training Loss 0.4340 (0.4372)	Training Prec@1 91.797 (91.551)	Training Prec@5 94.531 (94.713)	
2022-03-28 09:35:53,986: ============================================================
2022-03-28 09:36:45,637: time cost, forward:0.1174023691365872, backward:0.03388135779568817, data cost:0.2372961762113518 
2022-03-28 09:36:45,637: ============================================================
2022-03-28 09:36:45,638: Epoch 25/45 Batch 5900/7662 eta: 22:14:22.020139	Training Loss 0.4329 (0.4372)	Training Prec@1 90.820 (91.547)	Training Prec@5 93.945 (94.711)	
2022-03-28 09:36:45,638: ============================================================
2022-03-28 09:37:30,014: time cost, forward:0.11766312066307265, backward:0.03388971514573871, data cost:0.23795512068726696 
2022-03-28 09:37:30,014: ============================================================
2022-03-28 09:37:30,015: Epoch 25/45 Batch 6000/7662 eta: 19:05:41.437040	Training Loss 0.4513 (0.4372)	Training Prec@1 89.453 (91.547)	Training Prec@5 93.945 (94.712)	
2022-03-28 09:37:30,015: ============================================================
2022-03-28 09:38:32,823: time cost, forward:0.12060285396391417, backward:0.034295284038568095, data cost:0.23848515147947058 
2022-03-28 09:38:32,827: ============================================================
2022-03-28 09:38:32,828: Epoch 25/45 Batch 6100/7662 eta: 1 day, 3:00:35.874646	Training Loss 0.4364 (0.4372)	Training Prec@1 92.383 (91.547)	Training Prec@5 95.117 (94.711)	
2022-03-28 09:38:32,828: ============================================================
2022-03-28 09:39:42,476: time cost, forward:0.12446955435313338, backward:0.03466296961354063, data cost:0.23910559714080865 
2022-03-28 09:39:42,476: ============================================================
2022-03-28 09:39:42,476: Epoch 25/45 Batch 6200/7662 eta: 1 day, 5:55:48.882122	Training Loss 0.4314 (0.4372)	Training Prec@1 92.578 (91.549)	Training Prec@5 94.531 (94.712)	
2022-03-28 09:39:42,476: ============================================================
2022-03-28 09:40:22,754: time cost, forward:0.12460629200818028, backward:0.03464354744296658, data cost:0.23908999628974514 
2022-03-28 09:40:22,755: ============================================================
2022-03-28 09:40:22,755: Epoch 25/45 Batch 6300/7662 eta: 17:17:52.578929	Training Loss 0.4416 (0.4372)	Training Prec@1 89.648 (91.549)	Training Prec@5 93.750 (94.713)	
2022-03-28 09:40:22,755: ============================================================
2022-03-28 09:41:11,198: time cost, forward:0.12542952289989653, backward:0.03475518851824935, data cost:0.23948530320693487 
2022-03-28 09:41:11,199: ============================================================
2022-03-28 09:41:11,199: Epoch 25/45 Batch 6400/7662 eta: 20:47:27.231097	Training Loss 0.4383 (0.4372)	Training Prec@1 91.406 (91.548)	Training Prec@5 94.727 (94.711)	
2022-03-28 09:41:11,199: ============================================================
2022-03-28 09:42:02,855: time cost, forward:0.12705913065469968, backward:0.034912330808227185, data cost:0.23947192118853527 
2022-03-28 09:42:02,855: ============================================================
2022-03-28 09:42:02,856: Epoch 25/45 Batch 6500/7662 eta: 22:09:19.358502	Training Loss 0.4335 (0.4372)	Training Prec@1 90.430 (91.549)	Training Prec@5 92.773 (94.711)	
2022-03-28 09:42:02,856: ============================================================
2022-03-28 09:42:43,584: time cost, forward:0.12677370553523198, backward:0.034875522510339534, data cost:0.2398784557315938 
2022-03-28 09:42:43,585: ============================================================
2022-03-28 09:42:43,585: Epoch 25/45 Batch 6600/7662 eta: 17:27:26.102121	Training Loss 0.4365 (0.4372)	Training Prec@1 91.406 (91.547)	Training Prec@5 94.336 (94.713)	
2022-03-28 09:42:43,585: ============================================================
2022-03-28 09:43:23,069: time cost, forward:0.12651064577770332, backward:0.03486816316135678, data cost:0.24004686503361225 
2022-03-28 09:43:23,069: ============================================================
2022-03-28 09:43:23,070: Epoch 25/45 Batch 6700/7662 eta: 16:54:46.961373	Training Loss 0.4323 (0.4372)	Training Prec@1 91.406 (91.548)	Training Prec@5 94.141 (94.713)	
2022-03-28 09:43:23,070: ============================================================
2022-03-28 09:44:04,815: time cost, forward:0.12623566612634154, backward:0.034839766641665354, data cost:0.24057244447982212 
2022-03-28 09:44:04,816: ============================================================
2022-03-28 09:44:04,816: Epoch 25/45 Batch 6800/7662 eta: 17:52:12.052659	Training Loss 0.4328 (0.4373)	Training Prec@1 92.383 (91.546)	Training Prec@5 96.484 (94.712)	
2022-03-28 09:44:04,816: ============================================================
2022-03-28 09:44:46,322: time cost, forward:0.12596105796597215, backward:0.03481576203919784, data cost:0.24106609159525727 
2022-03-28 09:44:46,323: ============================================================
2022-03-28 09:44:46,323: Epoch 25/45 Batch 6900/7662 eta: 17:45:22.226373	Training Loss 0.4375 (0.4373)	Training Prec@1 91.016 (91.543)	Training Prec@5 94.141 (94.711)	
2022-03-28 09:44:46,323: ============================================================
2022-03-28 09:45:27,915: time cost, forward:0.1256850251539688, backward:0.03478741472083205, data cost:0.24156643809855266 
2022-03-28 09:45:27,916: ============================================================
2022-03-28 09:45:27,916: Epoch 25/45 Batch 7000/7662 eta: 17:46:52.746264	Training Loss 0.4340 (0.4373)	Training Prec@1 91.016 (91.543)	Training Prec@5 94.336 (94.711)	
2022-03-28 09:45:27,916: ============================================================
2022-03-28 09:46:12,585: time cost, forward:0.12580073741241682, backward:0.03485258872583897, data cost:0.24200666711471133 
2022-03-28 09:46:12,585: ============================================================
2022-03-28 09:46:12,586: Epoch 25/45 Batch 7100/7662 eta: 19:05:03.582401	Training Loss 0.4345 (0.4373)	Training Prec@1 92.578 (91.543)	Training Prec@5 94.727 (94.710)	
2022-03-28 09:46:12,586: ============================================================
2022-03-28 09:47:03,643: time cost, forward:0.12676489414448772, backward:0.034956127160919226, data cost:0.24241041994737345 
2022-03-28 09:47:03,643: ============================================================
2022-03-28 09:47:03,643: Epoch 25/45 Batch 7200/7662 eta: 21:47:56.993334	Training Loss 0.4245 (0.4373)	Training Prec@1 92.383 (91.545)	Training Prec@5 95.703 (94.710)	
2022-03-28 09:47:03,643: ============================================================
2022-03-28 09:47:44,369: time cost, forward:0.12652033403289206, backward:0.03492774001734439, data cost:0.24272955285472272 
2022-03-28 09:47:44,369: ============================================================
2022-03-28 09:47:44,370: Epoch 25/45 Batch 7300/7662 eta: 17:22:36.939077	Training Loss 0.4429 (0.4373)	Training Prec@1 91.016 (91.542)	Training Prec@5 94.336 (94.708)	
2022-03-28 09:47:44,370: ============================================================
2022-03-28 09:48:26,234: time cost, forward:0.12626253786820432, backward:0.034897801424622746, data cost:0.2432113783893464 
2022-03-28 09:48:26,235: ============================================================
2022-03-28 09:48:26,235: Epoch 25/45 Batch 7400/7662 eta: 17:51:04.833558	Training Loss 0.4399 (0.4373)	Training Prec@1 90.820 (91.540)	Training Prec@5 93.945 (94.706)	
2022-03-28 09:48:26,235: ============================================================
2022-03-28 09:49:07,452: time cost, forward:0.12599434151873937, backward:0.034880989248680425, data cost:0.24359465856713636 
2022-03-28 09:49:07,452: ============================================================
2022-03-28 09:49:07,452: Epoch 25/45 Batch 7500/7662 eta: 17:33:47.956521	Training Loss 0.4233 (0.4373)	Training Prec@1 93.945 (91.537)	Training Prec@5 96.484 (94.703)	
2022-03-28 09:49:07,452: ============================================================
2022-03-28 09:49:48,320: time cost, forward:0.12573316025410786, backward:0.03486066370327891, data cost:0.24392886807501574 
2022-03-28 09:49:48,320: ============================================================
2022-03-28 09:49:48,320: Epoch 25/45 Batch 7600/7662 eta: 17:24:12.327696	Training Loss 0.4391 (0.4373)	Training Prec@1 90.234 (91.534)	Training Prec@5 93.164 (94.701)	
2022-03-28 09:49:48,321: ============================================================
2022-03-28 09:50:15,118: Epoch: 25/45 eta: 17:23:46.580668	Training Loss 0.4263 (0.4373)	Training Prec@1 93.945 (91.536)	Training Prec@5 96.289 (94.702)
2022-03-28 09:50:15,119: ============================================================
2022-03-28 09:50:15,151: Save Checkpoint...
2022-03-28 09:50:15,152: ============================================================
2022-03-28 09:50:17,134: Save done!
2022-03-28 09:50:17,134: ============================================================
2022-03-28 09:50:58,875: time cost, forward:0.10680579898333309, backward:0.03742268350389269, data cost:0.27441931493354565 
2022-03-28 09:50:58,876: ============================================================
2022-03-28 09:50:58,876: Epoch 26/45 Batch 100/7662 eta: 17:45:23.552273	Training Loss 0.4311 (0.4354)	Training Prec@1 91.992 (91.641)	Training Prec@5 94.336 (94.831)	
2022-03-28 09:50:58,877: ============================================================
2022-03-28 09:51:40,336: time cost, forward:0.10667357732303179, backward:0.036427883646596014, data cost:0.2730499895373781 
2022-03-28 09:51:40,336: ============================================================
2022-03-28 09:51:40,336: Epoch 26/45 Batch 200/7662 eta: 17:37:30.868715	Training Loss 0.4454 (0.4352)	Training Prec@1 91.602 (91.686)	Training Prec@5 94.531 (94.868)	
2022-03-28 09:51:40,337: ============================================================
2022-03-28 09:52:23,795: time cost, forward:0.1056871286602722, backward:0.03521460753220778, data cost:0.281578473024145 
2022-03-28 09:52:23,796: ============================================================
2022-03-28 09:52:23,796: Epoch 26/45 Batch 300/7662 eta: 18:27:47.334937	Training Loss 0.4373 (0.4353)	Training Prec@1 90.234 (91.709)	Training Prec@5 93.945 (94.847)	
2022-03-28 09:52:23,796: ============================================================
2022-03-28 09:53:06,500: time cost, forward:0.10548638819453113, backward:0.03480366178622521, data cost:0.28305152006316603 
2022-03-28 09:53:06,500: ============================================================
2022-03-28 09:53:06,500: Epoch 26/45 Batch 400/7662 eta: 18:07:50.013571	Training Loss 0.4357 (0.4355)	Training Prec@1 91.406 (91.707)	Training Prec@5 94.922 (94.815)	
2022-03-28 09:53:06,501: ============================================================
2022-03-28 09:53:49,596: time cost, forward:0.10585061533895428, backward:0.03447352048151479, data cost:0.28458203533608356 
2022-03-28 09:53:49,597: ============================================================
2022-03-28 09:53:49,597: Epoch 26/45 Batch 500/7662 eta: 18:17:05.707757	Training Loss 0.4361 (0.4358)	Training Prec@1 93.555 (91.706)	Training Prec@5 96.094 (94.824)	
2022-03-28 09:53:49,597: ============================================================
2022-03-28 09:54:31,515: time cost, forward:0.10624622462786895, backward:0.034034843237849825, data cost:0.28356455522706 
2022-03-28 09:54:31,516: ============================================================
2022-03-28 09:54:31,516: Epoch 26/45 Batch 600/7662 eta: 17:46:26.411129	Training Loss 0.4397 (0.4358)	Training Prec@1 92.578 (91.670)	Training Prec@5 95.312 (94.790)	
2022-03-28 09:54:31,517: ============================================================
2022-03-28 09:55:12,265: time cost, forward:0.10661149127288949, backward:0.03396719787936013, data cost:0.28088918230223214 
2022-03-28 09:55:12,266: ============================================================
2022-03-28 09:55:12,266: Epoch 26/45 Batch 700/7662 eta: 17:16:00.002165	Training Loss 0.4386 (0.4357)	Training Prec@1 91.211 (91.672)	Training Prec@5 95.703 (94.799)	
2022-03-28 09:55:12,266: ============================================================
2022-03-28 09:55:56,066: time cost, forward:0.10924210506625408, backward:0.03394298499755478, data cost:0.28037243253447686 
2022-03-28 09:55:56,087: ============================================================
2022-03-28 09:55:56,088: Epoch 26/45 Batch 800/7662 eta: 18:33:22.027771	Training Loss 0.4281 (0.4357)	Training Prec@1 92.188 (91.688)	Training Prec@5 94.336 (94.803)	
2022-03-28 09:55:56,088: ============================================================
2022-03-28 09:56:38,248: time cost, forward:0.11239364865889671, backward:0.03400954627354232, data cost:0.27691899099127204 
2022-03-28 09:56:38,248: ============================================================
2022-03-28 09:56:38,249: Epoch 26/45 Batch 900/7662 eta: 17:50:28.426195	Training Loss 0.4354 (0.4358)	Training Prec@1 91.211 (91.670)	Training Prec@5 94.531 (94.789)	
2022-03-28 09:56:38,249: ============================================================
2022-03-28 09:57:20,840: time cost, forward:0.11494449738625649, backward:0.034164626557786425, data cost:0.2744411918613407 
2022-03-28 09:57:20,840: ============================================================
2022-03-28 09:57:20,841: Epoch 26/45 Batch 1000/7662 eta: 18:00:42.565133	Training Loss 0.4276 (0.4358)	Training Prec@1 92.578 (91.676)	Training Prec@5 96.680 (94.796)	
2022-03-28 09:57:20,841: ============================================================
2022-03-28 09:58:04,274: time cost, forward:0.11701678926018393, backward:0.03423944942727753, data cost:0.2732441635756627 
2022-03-28 09:58:04,274: ============================================================
2022-03-28 09:58:04,275: Epoch 26/45 Batch 1100/7662 eta: 18:21:20.866102	Training Loss 0.4458 (0.4357)	Training Prec@1 90.039 (91.681)	Training Prec@5 93.164 (94.796)	
2022-03-28 09:58:04,275: ============================================================
2022-03-28 09:58:46,511: time cost, forward:0.11907780876350561, backward:0.0343373426305343, data cost:0.27089643538047914 
2022-03-28 09:58:46,512: ============================================================
2022-03-28 09:58:46,512: Epoch 26/45 Batch 1200/7662 eta: 17:50:18.426776	Training Loss 0.4191 (0.4358)	Training Prec@1 95.117 (91.683)	Training Prec@5 96.289 (94.790)	
2022-03-28 09:58:46,513: ============================================================
2022-03-28 09:59:28,193: time cost, forward:0.11957010202356445, backward:0.03421362002507093, data cost:0.26986777644785115 
2022-03-28 09:59:28,193: ============================================================
2022-03-28 09:59:28,193: Epoch 26/45 Batch 1300/7662 eta: 17:35:30.974862	Training Loss 0.4429 (0.4359)	Training Prec@1 92.383 (91.672)	Training Prec@5 94.727 (94.785)	
2022-03-28 09:59:28,194: ============================================================
2022-03-28 10:00:09,745: time cost, forward:0.11880243957852192, backward:0.033661549052142345, data cost:0.27049253513507965 
2022-03-28 10:00:09,745: ============================================================
2022-03-28 10:00:09,746: Epoch 26/45 Batch 1400/7662 eta: 17:31:33.075202	Training Loss 0.4330 (0.4359)	Training Prec@1 91.992 (91.672)	Training Prec@5 95.117 (94.785)	
2022-03-28 10:00:09,746: ============================================================
2022-03-28 10:00:52,423: time cost, forward:0.11812606105969857, backward:0.033021763533731555, data cost:0.27214135003932877 
2022-03-28 10:00:52,423: ============================================================
2022-03-28 10:00:52,424: Epoch 26/45 Batch 1500/7662 eta: 17:59:20.408148	Training Loss 0.4407 (0.4360)	Training Prec@1 90.039 (91.665)	Training Prec@5 93.555 (94.785)	
2022-03-28 10:00:52,424: ============================================================
2022-03-28 10:01:33,323: time cost, forward:0.1175642029951333, backward:0.03245332928431489, data cost:0.2723378522311098 
2022-03-28 10:01:33,323: ============================================================
2022-03-28 10:01:33,324: Epoch 26/45 Batch 1600/7662 eta: 17:13:40.965959	Training Loss 0.4315 (0.4359)	Training Prec@1 91.797 (91.669)	Training Prec@5 94.336 (94.789)	
2022-03-28 10:01:33,324: ============================================================
2022-03-28 10:02:14,701: time cost, forward:0.1170554020743008, backward:0.0319467975926301, data cost:0.2728366227343617 
2022-03-28 10:02:14,702: ============================================================
2022-03-28 10:02:14,702: Epoch 26/45 Batch 1700/7662 eta: 17:25:04.768332	Training Loss 0.4334 (0.4358)	Training Prec@1 92.773 (91.668)	Training Prec@5 95.117 (94.785)	
2022-03-28 10:02:14,702: ============================================================
2022-03-28 10:02:55,735: time cost, forward:0.11661619315749079, backward:0.031733177581583547, data cost:0.2728342906576584 
2022-03-28 10:02:55,736: ============================================================
2022-03-28 10:02:55,736: Epoch 26/45 Batch 1800/7662 eta: 17:15:42.409943	Training Loss 0.4326 (0.4359)	Training Prec@1 91.602 (91.667)	Training Prec@5 96.680 (94.792)	
2022-03-28 10:02:55,736: ============================================================
2022-03-28 10:03:37,964: time cost, forward:0.116247187921284, backward:0.03184946315297332, data cost:0.2731606764439346 
2022-03-28 10:03:37,965: ============================================================
2022-03-28 10:03:37,965: Epoch 26/45 Batch 1900/7662 eta: 17:45:10.259019	Training Loss 0.4324 (0.4360)	Training Prec@1 91.602 (91.667)	Training Prec@5 94.141 (94.790)	
2022-03-28 10:03:37,966: ============================================================
2022-03-28 10:04:20,809: time cost, forward:0.11588406419682468, backward:0.032017042423379966, data cost:0.2736510128423892 
2022-03-28 10:04:20,809: ============================================================
2022-03-28 10:04:20,809: Epoch 26/45 Batch 2000/7662 eta: 17:59:57.775325	Training Loss 0.4377 (0.4360)	Training Prec@1 92.773 (91.659)	Training Prec@5 94.727 (94.786)	
2022-03-28 10:04:20,809: ============================================================
2022-03-28 10:05:04,172: time cost, forward:0.11709810950291503, backward:0.0323653940134471, data cost:0.272645001300123 
2022-03-28 10:05:04,173: ============================================================
2022-03-28 10:05:04,173: Epoch 26/45 Batch 2100/7662 eta: 18:12:20.481752	Training Loss 0.4403 (0.4360)	Training Prec@1 91.797 (91.658)	Training Prec@5 95.117 (94.784)	
2022-03-28 10:05:04,173: ============================================================
2022-03-28 10:05:44,563: time cost, forward:0.11708585876614032, backward:0.03246936618116239, data cost:0.2717403699832377 
2022-03-28 10:05:44,564: ============================================================
2022-03-28 10:05:44,564: Epoch 26/45 Batch 2200/7662 eta: 16:56:47.197948	Training Loss 0.4263 (0.4360)	Training Prec@1 93.359 (91.663)	Training Prec@5 95.898 (94.789)	
2022-03-28 10:05:44,564: ============================================================
2022-03-28 10:06:25,674: time cost, forward:0.11665942794398258, backward:0.03247833106765648, data cost:0.27166022285786645 
2022-03-28 10:06:25,674: ============================================================
2022-03-28 10:06:25,674: Epoch 26/45 Batch 2300/7662 eta: 17:14:12.081377	Training Loss 0.4359 (0.4360)	Training Prec@1 92.188 (91.665)	Training Prec@5 94.727 (94.793)	
2022-03-28 10:06:25,675: ============================================================
2022-03-28 10:07:08,546: time cost, forward:0.11614706447691955, backward:0.03250277166617021, data cost:0.2724926986511472 
2022-03-28 10:07:08,546: ============================================================
2022-03-28 10:07:08,546: Epoch 26/45 Batch 2400/7662 eta: 17:57:48.767110	Training Loss 0.4379 (0.4360)	Training Prec@1 92.773 (91.662)	Training Prec@5 95.508 (94.792)	
2022-03-28 10:07:08,547: ============================================================
2022-03-28 10:07:51,147: time cost, forward:0.11571166647964118, backward:0.032498030912499275, data cost:0.273114611597813 
2022-03-28 10:07:51,148: ============================================================
2022-03-28 10:07:51,148: Epoch 26/45 Batch 2500/7662 eta: 17:50:18.193838	Training Loss 0.4580 (0.4360)	Training Prec@1 89.453 (91.664)	Training Prec@5 92.188 (94.789)	
2022-03-28 10:07:51,148: ============================================================
2022-03-28 10:08:31,576: time cost, forward:0.11538508874996665, backward:0.032478499018077256, data cost:0.2727823600534202 
2022-03-28 10:08:31,576: ============================================================
2022-03-28 10:08:31,576: Epoch 26/45 Batch 2600/7662 eta: 16:55:01.234665	Training Loss 0.4312 (0.4360)	Training Prec@1 91.211 (91.670)	Training Prec@5 93.750 (94.791)	
2022-03-28 10:08:31,576: ============================================================
2022-03-28 10:09:12,740: time cost, forward:0.11511114218712913, backward:0.03251788041113747, data cost:0.2726831456474482 
2022-03-28 10:09:12,740: ============================================================
2022-03-28 10:09:12,741: Epoch 26/45 Batch 2700/7662 eta: 17:12:49.306734	Training Loss 0.4242 (0.4360)	Training Prec@1 91.406 (91.667)	Training Prec@5 94.336 (94.791)	
2022-03-28 10:09:12,741: ============================================================
2022-03-28 10:09:55,384: time cost, forward:0.11482708034195105, backward:0.03255184985518924, data cost:0.2731244718232041 
2022-03-28 10:09:55,385: ============================================================
2022-03-28 10:09:55,385: Epoch 26/45 Batch 2800/7662 eta: 17:49:14.294338	Training Loss 0.4442 (0.4360)	Training Prec@1 86.719 (91.657)	Training Prec@5 91.992 (94.783)	
2022-03-28 10:09:55,385: ============================================================
2022-03-28 10:10:37,147: time cost, forward:0.11460517471763502, backward:0.03258487322117469, data cost:0.27321411437435617 
2022-03-28 10:10:37,148: ============================================================
2022-03-28 10:10:37,148: Epoch 26/45 Batch 2900/7662 eta: 17:26:26.984969	Training Loss 0.4348 (0.4361)	Training Prec@1 92.773 (91.650)	Training Prec@5 96.094 (94.779)	
2022-03-28 10:10:37,148: ============================================================
2022-03-28 10:11:16,802: time cost, forward:0.11439422497394762, backward:0.03261120838497272, data cost:0.2725974328122802 
2022-03-28 10:11:16,802: ============================================================
2022-03-28 10:11:16,802: Epoch 26/45 Batch 3000/7662 eta: 16:32:57.165837	Training Loss 0.4418 (0.4361)	Training Prec@1 93.555 (91.654)	Training Prec@5 96.289 (94.780)	
2022-03-28 10:11:16,802: ============================================================
2022-03-28 10:11:58,681: time cost, forward:0.1141662612273717, backward:0.032651520421636535, data cost:0.2727394479442004 
2022-03-28 10:11:58,682: ============================================================
2022-03-28 10:11:58,682: Epoch 26/45 Batch 3100/7662 eta: 17:27:58.841921	Training Loss 0.4413 (0.4360)	Training Prec@1 90.820 (91.655)	Training Prec@5 95.117 (94.781)	
2022-03-28 10:11:58,682: ============================================================
2022-03-28 10:12:41,404: time cost, forward:0.11394840808688346, backward:0.03273014621311294, data cost:0.2731154493407631 
2022-03-28 10:12:41,405: ============================================================
2022-03-28 10:12:41,405: Epoch 26/45 Batch 3200/7662 eta: 17:48:22.338100	Training Loss 0.4334 (0.4361)	Training Prec@1 89.648 (91.652)	Training Prec@5 93.945 (94.782)	
2022-03-28 10:12:41,406: ============================================================
2022-03-28 10:13:22,996: time cost, forward:0.11377252300929648, backward:0.03283561847902999, data cost:0.2730739732408711 
2022-03-28 10:13:22,996: ============================================================
2022-03-28 10:13:22,997: Epoch 26/45 Batch 3300/7662 eta: 17:19:22.206309	Training Loss 0.4257 (0.4361)	Training Prec@1 92.383 (91.647)	Training Prec@5 95.898 (94.780)	
2022-03-28 10:13:22,997: ============================================================
2022-03-28 10:14:03,631: time cost, forward:0.11362946093099964, backward:0.032851598023596704, data cost:0.27276605253676944 
2022-03-28 10:14:03,632: ============================================================
2022-03-28 10:14:03,632: Epoch 26/45 Batch 3400/7662 eta: 16:54:48.300815	Training Loss 0.4334 (0.4361)	Training Prec@1 91.992 (91.644)	Training Prec@5 93.555 (94.773)	
2022-03-28 10:14:03,632: ============================================================
2022-03-28 10:14:46,031: time cost, forward:0.11349621511384671, backward:0.032828900022961884, data cost:0.27306139084161024 
2022-03-28 10:14:46,032: ============================================================
2022-03-28 10:14:46,032: Epoch 26/45 Batch 3500/7662 eta: 17:38:10.565787	Training Loss 0.4338 (0.4361)	Training Prec@1 92.969 (91.648)	Training Prec@5 96.680 (94.775)	
2022-03-28 10:14:46,032: ============================================================
2022-03-28 10:15:27,173: time cost, forward:0.11337750267406145, backward:0.032800298824877366, data cost:0.27293833960225494 
2022-03-28 10:15:27,173: ============================================================
2022-03-28 10:15:27,174: Epoch 26/45 Batch 3600/7662 eta: 17:06:04.785338	Training Loss 0.4292 (0.4361)	Training Prec@1 92.578 (91.646)	Training Prec@5 95.508 (94.774)	
2022-03-28 10:15:27,174: ============================================================
2022-03-28 10:16:08,108: time cost, forward:0.11326431667330975, backward:0.03278452133803665, data cost:0.2728307137588579 
2022-03-28 10:16:08,109: ============================================================
2022-03-28 10:16:08,109: Epoch 26/45 Batch 3700/7662 eta: 17:00:15.524556	Training Loss 0.4343 (0.4362)	Training Prec@1 92.188 (91.644)	Training Prec@5 95.312 (94.771)	
2022-03-28 10:16:08,110: ============================================================
2022-03-28 10:16:49,998: time cost, forward:0.11314730005347123, backward:0.03275796450197963, data cost:0.27296307589638136 
2022-03-28 10:16:49,999: ============================================================
2022-03-28 10:16:49,999: Epoch 26/45 Batch 3800/7662 eta: 17:23:20.243251	Training Loss 0.4336 (0.4362)	Training Prec@1 91.211 (91.647)	Training Prec@5 94.531 (94.772)	
2022-03-28 10:16:49,999: ============================================================
2022-03-28 10:17:31,894: time cost, forward:0.11302087942310772, backward:0.032743216600440106, data cost:0.2731099167858279 
2022-03-28 10:17:31,894: ============================================================
2022-03-28 10:17:31,895: Epoch 26/45 Batch 3900/7662 eta: 17:22:47.387873	Training Loss 0.4561 (0.4362)	Training Prec@1 86.719 (91.645)	Training Prec@5 91.211 (94.771)	
2022-03-28 10:17:31,895: ============================================================
2022-03-28 10:18:13,660: time cost, forward:0.11289737164363112, backward:0.03273247331045484, data cost:0.27319614360796923 
2022-03-28 10:18:13,660: ============================================================
2022-03-28 10:18:13,661: Epoch 26/45 Batch 4000/7662 eta: 17:18:52.214581	Training Loss 0.4333 (0.4362)	Training Prec@1 93.750 (91.645)	Training Prec@5 95.508 (94.773)	
2022-03-28 10:18:13,661: ============================================================
2022-03-28 10:18:55,900: time cost, forward:0.11278169659411916, backward:0.032710440596826894, data cost:0.27341989273849304 
2022-03-28 10:18:55,900: ============================================================
2022-03-28 10:18:55,901: Epoch 26/45 Batch 4100/7662 eta: 17:29:57.143124	Training Loss 0.4262 (0.4362)	Training Prec@1 92.383 (91.648)	Training Prec@5 96.094 (94.775)	
2022-03-28 10:18:55,901: ============================================================
2022-03-28 10:19:37,912: time cost, forward:0.11267419462119946, backward:0.032719488795753544, data cost:0.27353919900920054 
2022-03-28 10:19:37,913: ============================================================
2022-03-28 10:19:37,913: Epoch 26/45 Batch 4200/7662 eta: 17:23:35.898167	Training Loss 0.4351 (0.4362)	Training Prec@1 92.969 (91.652)	Training Prec@5 95.312 (94.779)	
2022-03-28 10:19:37,914: ============================================================
2022-03-28 10:20:18,256: time cost, forward:0.112581815905947, backward:0.0327881088531913, data cost:0.27319546166229425 
2022-03-28 10:20:18,257: ============================================================
2022-03-28 10:20:18,257: Epoch 26/45 Batch 4300/7662 eta: 16:41:28.161328	Training Loss 0.4358 (0.4362)	Training Prec@1 91.992 (91.650)	Training Prec@5 94.531 (94.778)	
2022-03-28 10:20:18,257: ============================================================
2022-03-28 10:21:01,854: time cost, forward:0.11248740355354409, backward:0.03281895310587708, data cost:0.2736354038969553 
2022-03-28 10:21:01,855: ============================================================
2022-03-28 10:21:01,855: Epoch 26/45 Batch 4400/7662 eta: 18:01:31.721610	Training Loss 0.4221 (0.4362)	Training Prec@1 93.359 (91.655)	Training Prec@5 95.117 (94.782)	
2022-03-28 10:21:01,855: ============================================================
2022-03-28 10:21:42,582: time cost, forward:0.11239932134008905, backward:0.03281604854179292, data cost:0.2734303856510723 
2022-03-28 10:21:42,582: ============================================================
2022-03-28 10:21:42,583: Epoch 26/45 Batch 4500/7662 eta: 16:49:38.845699	Training Loss 0.4417 (0.4362)	Training Prec@1 91.992 (91.654)	Training Prec@5 95.703 (94.783)	
2022-03-28 10:21:42,583: ============================================================
2022-03-28 10:22:24,945: time cost, forward:0.11231648639430945, backward:0.032813951123613565, data cost:0.2736583960836518 
2022-03-28 10:22:24,946: ============================================================
2022-03-28 10:22:24,946: Epoch 26/45 Batch 4600/7662 eta: 17:29:29.472446	Training Loss 0.4334 (0.4362)	Training Prec@1 90.625 (91.656)	Training Prec@5 94.922 (94.782)	
2022-03-28 10:22:24,946: ============================================================
2022-03-28 10:23:05,822: time cost, forward:0.11220889899243089, backward:0.03283630068897313, data cost:0.2735171901035573 
2022-03-28 10:23:05,823: ============================================================
2022-03-28 10:23:05,823: Epoch 26/45 Batch 4700/7662 eta: 16:51:58.746310	Training Loss 0.4250 (0.4363)	Training Prec@1 92.188 (91.652)	Training Prec@5 95.508 (94.780)	
2022-03-28 10:23:05,823: ============================================================
2022-03-28 10:23:47,930: time cost, forward:0.11211973469712928, backward:0.03280907443325379, data cost:0.2736766171917416 
2022-03-28 10:23:47,930: ============================================================
2022-03-28 10:23:47,931: Epoch 26/45 Batch 4800/7662 eta: 17:21:45.102799	Training Loss 0.4376 (0.4363)	Training Prec@1 91.016 (91.650)	Training Prec@5 93.750 (94.778)	
2022-03-28 10:23:47,931: ============================================================
2022-03-28 10:24:30,014: time cost, forward:0.1120435705377657, backward:0.03281115161859154, data cost:0.273801082201115 
2022-03-28 10:24:30,014: ============================================================
2022-03-28 10:24:30,014: Epoch 26/45 Batch 4900/7662 eta: 17:20:27.647999	Training Loss 0.4403 (0.4363)	Training Prec@1 92.383 (91.651)	Training Prec@5 93.750 (94.778)	
2022-03-28 10:24:30,015: ============================================================
2022-03-28 10:25:11,412: time cost, forward:0.11197060612493287, backward:0.03282101499149622, data cost:0.2737611701283891 
2022-03-28 10:25:11,412: ============================================================
2022-03-28 10:25:11,412: Epoch 26/45 Batch 5000/7662 eta: 17:02:48.568848	Training Loss 0.4378 (0.4363)	Training Prec@1 91.211 (91.650)	Training Prec@5 95.117 (94.778)	
2022-03-28 10:25:11,413: ============================================================
2022-03-28 10:25:55,472: time cost, forward:0.11190100542866359, backward:0.03280248841156467, data cost:0.2742759288350936 
2022-03-28 10:25:55,473: ============================================================
2022-03-28 10:25:55,473: Epoch 26/45 Batch 5100/7662 eta: 18:07:52.091121	Training Loss 0.4308 (0.4363)	Training Prec@1 93.164 (91.647)	Training Prec@5 95.312 (94.776)	
2022-03-28 10:25:55,473: ============================================================
2022-03-28 10:26:36,864: time cost, forward:0.1118365458557986, backward:0.032788242782898006, data cost:0.27424467401014746 
2022-03-28 10:26:36,864: ============================================================
2022-03-28 10:26:36,865: Epoch 26/45 Batch 5200/7662 eta: 17:01:16.634403	Training Loss 0.4392 (0.4363)	Training Prec@1 92.773 (91.647)	Training Prec@5 95.312 (94.776)	
2022-03-28 10:26:36,865: ============================================================
2022-03-28 10:27:18,017: time cost, forward:0.111777272488086, backward:0.032756644164555294, data cost:0.27419862353052593 
2022-03-28 10:27:18,018: ============================================================
2022-03-28 10:27:18,018: Epoch 26/45 Batch 5300/7662 eta: 16:54:42.584576	Training Loss 0.4550 (0.4363)	Training Prec@1 91.016 (91.646)	Training Prec@5 93.945 (94.775)	
2022-03-28 10:27:18,018: ============================================================
2022-03-28 10:27:58,749: time cost, forward:0.11171634141152204, backward:0.03278176655127265, data cost:0.27402627916330763 
2022-03-28 10:27:58,749: ============================================================
2022-03-28 10:27:58,750: Epoch 26/45 Batch 5400/7662 eta: 16:43:37.794041	Training Loss 0.4390 (0.4363)	Training Prec@1 89.453 (91.646)	Training Prec@5 93.750 (94.775)	
2022-03-28 10:27:58,750: ============================================================
2022-03-28 10:28:41,766: time cost, forward:0.11165705965440649, backward:0.032756302221880584, data cost:0.2743147743639241 
2022-03-28 10:28:41,767: ============================================================
2022-03-28 10:28:41,767: Epoch 26/45 Batch 5500/7662 eta: 17:39:14.250674	Training Loss 0.4311 (0.4363)	Training Prec@1 92.773 (91.645)	Training Prec@5 95.508 (94.774)	
2022-03-28 10:28:41,767: ============================================================
2022-03-28 10:29:24,862: time cost, forward:0.11160398015040844, backward:0.03274434488742431, data cost:0.27459596846652556 
2022-03-28 10:29:24,862: ============================================================
2022-03-28 10:29:24,863: Epoch 26/45 Batch 5600/7662 eta: 17:40:27.122765	Training Loss 0.4445 (0.4363)	Training Prec@1 90.625 (91.642)	Training Prec@5 94.336 (94.774)	
2022-03-28 10:29:24,863: ============================================================
2022-03-28 10:30:06,834: time cost, forward:0.11155332069811977, backward:0.0327351059824108, data cost:0.27464656734449816 
2022-03-28 10:30:06,834: ============================================================
2022-03-28 10:30:06,835: Epoch 26/45 Batch 5700/7662 eta: 17:12:05.921364	Training Loss 0.4410 (0.4363)	Training Prec@1 90.820 (91.642)	Training Prec@5 94.727 (94.772)	
2022-03-28 10:30:06,835: ============================================================
2022-03-28 10:30:48,269: time cost, forward:0.11151212111570276, backward:0.03272676369222532, data cost:0.2746169711006245 
2022-03-28 10:30:48,269: ============================================================
2022-03-28 10:30:48,269: Epoch 26/45 Batch 5800/7662 eta: 16:58:11.320892	Training Loss 0.4432 (0.4364)	Training Prec@1 92.383 (91.641)	Training Prec@5 95.117 (94.771)	
2022-03-28 10:30:48,269: ============================================================
2022-03-28 10:31:31,149: time cost, forward:0.11145621312838688, backward:0.032695995546797814, data cost:0.27486489218844257 
2022-03-28 10:31:31,149: ============================================================
2022-03-28 10:31:31,150: Epoch 26/45 Batch 5900/7662 eta: 17:33:00.571320	Training Loss 0.4419 (0.4364)	Training Prec@1 93.555 (91.639)	Training Prec@5 96.094 (94.772)	
2022-03-28 10:31:31,150: ============================================================
2022-03-28 10:32:12,205: time cost, forward:0.11141372442364712, backward:0.03269620521165149, data cost:0.27476851827682186 
2022-03-28 10:32:12,205: ============================================================
2022-03-28 10:32:12,205: Epoch 26/45 Batch 6000/7662 eta: 16:47:30.811044	Training Loss 0.4427 (0.4364)	Training Prec@1 91.992 (91.636)	Training Prec@5 95.508 (94.772)	
2022-03-28 10:32:12,206: ============================================================
2022-03-28 10:32:53,235: time cost, forward:0.11136660311608691, backward:0.03266881962138368, data cost:0.27469202115274216 
2022-03-28 10:32:53,235: ============================================================
2022-03-28 10:32:53,235: Epoch 26/45 Batch 6100/7662 eta: 16:46:12.005581	Training Loss 0.4257 (0.4364)	Training Prec@1 91.602 (91.635)	Training Prec@5 95.312 (94.771)	
2022-03-28 10:32:53,236: ============================================================
2022-03-28 10:33:35,500: time cost, forward:0.11131746747028752, backward:0.03264863253755134, data cost:0.27482232656723493 
2022-03-28 10:33:35,500: ============================================================
2022-03-28 10:33:35,500: Epoch 26/45 Batch 6200/7662 eta: 17:15:46.909655	Training Loss 0.4426 (0.4364)	Training Prec@1 93.555 (91.636)	Training Prec@5 95.117 (94.772)	
2022-03-28 10:33:35,501: ============================================================
2022-03-28 10:34:17,192: time cost, forward:0.11128343379881708, backward:0.03266035490403915, data cost:0.27480741522959024 
2022-03-28 10:34:17,192: ============================================================
2022-03-28 10:34:17,193: Epoch 26/45 Batch 6300/7662 eta: 17:01:03.092472	Training Loss 0.4380 (0.4364)	Training Prec@1 91.406 (91.634)	Training Prec@5 94.531 (94.771)	
2022-03-28 10:34:17,193: ============================================================
2022-03-28 10:34:59,343: time cost, forward:0.11124873653131083, backward:0.032656105370572966, data cost:0.2748850016393481 
2022-03-28 10:34:59,343: ============================================================
2022-03-28 10:34:59,343: Epoch 26/45 Batch 6400/7662 eta: 17:11:34.550834	Training Loss 0.4451 (0.4364)	Training Prec@1 92.188 (91.636)	Training Prec@5 95.703 (94.775)	
2022-03-28 10:34:59,344: ============================================================
2022-03-28 10:35:41,060: time cost, forward:0.11121550650390447, backward:0.032710925599834996, data cost:0.27483599500337696 
2022-03-28 10:35:41,061: ============================================================
2022-03-28 10:35:41,061: Epoch 26/45 Batch 6500/7662 eta: 17:00:16.791037	Training Loss 0.4325 (0.4364)	Training Prec@1 90.820 (91.638)	Training Prec@5 94.336 (94.776)	
2022-03-28 10:35:41,061: ============================================================
2022-03-28 10:36:21,808: time cost, forward:0.11117655682841979, backward:0.032771925435569865, data cost:0.2746318156908165 
2022-03-28 10:36:21,808: ============================================================
2022-03-28 10:36:21,809: Epoch 26/45 Batch 6600/7662 eta: 16:35:52.763050	Training Loss 0.4401 (0.4364)	Training Prec@1 90.820 (91.637)	Training Prec@5 94.336 (94.774)	
2022-03-28 10:36:21,809: ============================================================
2022-03-28 10:37:02,277: time cost, forward:0.11114284945594889, backward:0.032806204368612944, data cost:0.2744199369786159 
2022-03-28 10:37:02,277: ============================================================
2022-03-28 10:37:02,277: Epoch 26/45 Batch 6700/7662 eta: 16:28:23.413656	Training Loss 0.4351 (0.4364)	Training Prec@1 91.406 (91.636)	Training Prec@5 95.508 (94.772)	
2022-03-28 10:37:02,278: ============================================================
2022-03-28 10:37:44,305: time cost, forward:0.1111109449260637, backward:0.03280955869531891, data cost:0.27447643436426833 
2022-03-28 10:37:44,305: ============================================================
2022-03-28 10:37:44,306: Epoch 26/45 Batch 6800/7662 eta: 17:05:46.283005	Training Loss 0.4289 (0.4365)	Training Prec@1 93.164 (91.636)	Training Prec@5 95.898 (94.772)	
2022-03-28 10:37:44,306: ============================================================
2022-03-28 10:38:25,865: time cost, forward:0.11108126020341666, backward:0.0327977946848606, data cost:0.2744699094689122 
2022-03-28 10:38:25,865: ============================================================
2022-03-28 10:38:25,866: Epoch 26/45 Batch 6900/7662 eta: 16:53:39.206122	Training Loss 0.4473 (0.4365)	Training Prec@1 91.797 (91.635)	Training Prec@5 95.117 (94.770)	
2022-03-28 10:38:25,866: ============================================================
2022-03-28 10:39:08,177: time cost, forward:0.11104837460660273, backward:0.03279035073345875, data cost:0.27456564899171515 
2022-03-28 10:39:08,178: ============================================================
2022-03-28 10:39:08,178: Epoch 26/45 Batch 7000/7662 eta: 17:11:18.253459	Training Loss 0.4452 (0.4365)	Training Prec@1 91.406 (91.634)	Training Prec@5 95.117 (94.770)	
2022-03-28 10:39:08,178: ============================================================
2022-03-28 10:39:50,750: time cost, forward:0.11101864132852282, backward:0.0327766146151377, data cost:0.27471557810770353 
2022-03-28 10:39:50,750: ============================================================
2022-03-28 10:39:50,750: Epoch 26/45 Batch 7100/7662 eta: 17:16:55.715192	Training Loss 0.4563 (0.4365)	Training Prec@1 90.820 (91.632)	Training Prec@5 94.727 (94.768)	
2022-03-28 10:39:50,751: ============================================================
2022-03-28 10:40:33,484: time cost, forward:0.11098600874544598, backward:0.032773217950898555, data cost:0.27486708638137836 
2022-03-28 10:40:33,484: ============================================================
2022-03-28 10:40:33,485: Epoch 26/45 Batch 7200/7662 eta: 17:20:09.623636	Training Loss 0.4354 (0.4365)	Training Prec@1 90.234 (91.629)	Training Prec@5 93.945 (94.766)	
2022-03-28 10:40:33,485: ============================================================
2022-03-28 10:41:14,643: time cost, forward:0.11095816018663378, backward:0.03276310673901962, data cost:0.2748060794150377 
2022-03-28 10:41:14,643: ============================================================
2022-03-28 10:41:14,644: Epoch 26/45 Batch 7300/7662 eta: 16:41:07.549877	Training Loss 0.4434 (0.4365)	Training Prec@1 89.062 (91.626)	Training Prec@5 93.750 (94.765)	
2022-03-28 10:41:14,644: ============================================================
2022-03-28 10:41:56,806: time cost, forward:0.11092957410800777, backward:0.03277396891532452, data cost:0.27486592696347517 
2022-03-28 10:41:56,806: ============================================================
2022-03-28 10:41:56,807: Epoch 26/45 Batch 7400/7662 eta: 17:04:51.139529	Training Loss 0.4331 (0.4366)	Training Prec@1 92.773 (91.627)	Training Prec@5 95.117 (94.765)	
2022-03-28 10:41:56,807: ============================================================
2022-03-28 10:42:39,325: time cost, forward:0.11089874894034944, backward:0.03283963080707718, data cost:0.2749109489788356 
2022-03-28 10:42:39,326: ============================================================
2022-03-28 10:42:39,326: Epoch 26/45 Batch 7500/7662 eta: 17:12:47.830954	Training Loss 0.4438 (0.4366)	Training Prec@1 91.602 (91.623)	Training Prec@5 94.922 (94.762)	
2022-03-28 10:42:39,326: ============================================================
2022-03-28 10:43:21,071: time cost, forward:0.11086689012429325, backward:0.03285223626042529, data cost:0.2749062006780075 
2022-03-28 10:43:21,071: ============================================================
2022-03-28 10:43:21,071: Epoch 26/45 Batch 7600/7662 eta: 16:53:18.673162	Training Loss 0.4380 (0.4366)	Training Prec@1 92.188 (91.619)	Training Prec@5 95.117 (94.761)	
2022-03-28 10:43:21,072: ============================================================
2022-03-28 10:43:48,261: Epoch: 26/45 eta: 16:52:52.373450	Training Loss 0.4490 (0.4366)	Training Prec@1 92.188 (91.617)	Training Prec@5 95.898 (94.760)
2022-03-28 10:43:48,262: ============================================================
2022-03-28 10:44:31,091: time cost, forward:0.10823825633887088, backward:0.02345164857729517, data cost:0.2951794055977253 
2022-03-28 10:44:31,092: ============================================================
2022-03-28 10:44:31,092: Epoch 27/45 Batch 100/7662 eta: 17:14:00.846220	Training Loss 0.4258 (0.4343)	Training Prec@1 91.602 (92.063)	Training Prec@5 94.531 (94.940)	
2022-03-28 10:44:31,092: ============================================================
2022-03-28 10:45:12,382: time cost, forward:0.10932624040536545, backward:0.023355444472039764, data cost:0.2874619673244917 
2022-03-28 10:45:12,383: ============================================================
2022-03-28 10:45:12,383: Epoch 27/45 Batch 200/7662 eta: 16:40:28.348451	Training Loss 0.4314 (0.4345)	Training Prec@1 92.188 (91.980)	Training Prec@5 96.289 (94.919)	
2022-03-28 10:45:12,383: ============================================================
2022-03-28 10:45:54,957: time cost, forward:0.11056693660774358, backward:0.02588994526942837, data cost:0.285361345795086 
2022-03-28 10:45:54,957: ============================================================
2022-03-28 10:45:54,958: Epoch 27/45 Batch 300/7662 eta: 17:10:51.746323	Training Loss 0.4334 (0.4344)	Training Prec@1 91.992 (91.956)	Training Prec@5 94.727 (94.969)	
2022-03-28 10:45:54,958: ============================================================
2022-03-28 10:46:35,958: time cost, forward:0.1119042159919452, backward:0.027613262783614614, data cost:0.2793819731040706 
2022-03-28 10:46:35,959: ============================================================
2022-03-28 10:46:35,959: Epoch 27/45 Batch 400/7662 eta: 16:32:05.433227	Training Loss 0.4362 (0.4346)	Training Prec@1 91.406 (91.930)	Training Prec@5 94.531 (94.959)	
2022-03-28 10:46:35,959: ============================================================
2022-03-28 10:47:17,851: time cost, forward:0.11129601063852558, backward:0.028517370950243995, data cost:0.27907760730965103 
2022-03-28 10:47:17,851: ============================================================
2022-03-28 10:47:17,851: Epoch 27/45 Batch 500/7662 eta: 16:52:57.295024	Training Loss 0.4343 (0.4346)	Training Prec@1 91.602 (91.906)	Training Prec@5 95.508 (94.953)	
2022-03-28 10:47:17,852: ============================================================
2022-03-28 10:48:00,191: time cost, forward:0.11093823061960567, backward:0.02921192634085781, data cost:0.2794086658496889 
2022-03-28 10:48:00,191: ============================================================
2022-03-28 10:48:00,192: Epoch 27/45 Batch 600/7662 eta: 17:03:04.200766	Training Loss 0.4296 (0.4345)	Training Prec@1 93.750 (91.894)	Training Prec@5 96.875 (94.948)	
2022-03-28 10:48:00,192: ============================================================
2022-03-28 10:48:42,994: time cost, forward:0.11061317692157707, backward:0.029775442142513858, data cost:0.2803156546427627 
2022-03-28 10:48:42,995: ============================================================
2022-03-28 10:48:42,995: Epoch 27/45 Batch 700/7662 eta: 17:13:32.861263	Training Loss 0.4336 (0.4348)	Training Prec@1 91.797 (91.866)	Training Prec@5 95.117 (94.931)	
2022-03-28 10:48:42,995: ============================================================
2022-03-28 10:49:25,945: time cost, forward:0.11039925158695225, backward:0.03010388638110871, data cost:0.28107450960276276 
2022-03-28 10:49:25,946: ============================================================
2022-03-28 10:49:25,946: Epoch 27/45 Batch 800/7662 eta: 17:16:24.705497	Training Loss 0.4387 (0.4349)	Training Prec@1 92.383 (91.846)	Training Prec@5 93.945 (94.921)	
2022-03-28 10:49:25,946: ============================================================
2022-03-28 10:50:07,851: time cost, forward:0.11022350361137687, backward:0.030751294898774654, data cost:0.28043890370675534 
2022-03-28 10:50:07,851: ============================================================
2022-03-28 10:50:07,852: Epoch 27/45 Batch 900/7662 eta: 16:50:28.387193	Training Loss 0.4310 (0.4350)	Training Prec@1 91.797 (91.830)	Training Prec@5 94.531 (94.916)	
2022-03-28 10:50:07,852: ============================================================
2022-03-28 10:50:49,481: time cost, forward:0.11009219888452296, backward:0.030970229042900935, data cost:0.27972152044584564 
2022-03-28 10:50:49,481: ============================================================
2022-03-28 10:50:49,481: Epoch 27/45 Batch 1000/7662 eta: 16:43:07.658419	Training Loss 0.4294 (0.4351)	Training Prec@1 91.992 (91.828)	Training Prec@5 94.727 (94.910)	
2022-03-28 10:50:49,481: ============================================================
2022-03-28 10:51:32,563: time cost, forward:0.10992079392902628, backward:0.03079193564736919, data cost:0.2810090518843813 
2022-03-28 10:51:32,564: ============================================================
2022-03-28 10:51:32,564: Epoch 27/45 Batch 1100/7662 eta: 17:17:25.233351	Training Loss 0.4337 (0.4352)	Training Prec@1 91.211 (91.812)	Training Prec@5 94.531 (94.908)	
2022-03-28 10:51:32,564: ============================================================
2022-03-28 10:52:12,988: time cost, forward:0.10979123051908236, backward:0.030817518639902557, data cost:0.2796307409475802 
2022-03-28 10:52:12,988: ============================================================
2022-03-28 10:52:12,988: Epoch 27/45 Batch 1200/7662 eta: 16:12:44.307775	Training Loss 0.4450 (0.4352)	Training Prec@1 90.430 (91.797)	Training Prec@5 93.555 (94.891)	
2022-03-28 10:52:12,988: ============================================================
2022-03-28 10:52:53,661: time cost, forward:0.10970200622329536, backward:0.03074384047674894, data cost:0.2787328269318308 
2022-03-28 10:52:53,662: ============================================================
2022-03-28 10:52:53,662: Epoch 27/45 Batch 1300/7662 eta: 16:18:03.652133	Training Loss 0.4396 (0.4353)	Training Prec@1 91.992 (91.780)	Training Prec@5 95.508 (94.888)	
2022-03-28 10:52:53,662: ============================================================
2022-03-28 10:53:35,788: time cost, forward:0.10960830646893908, backward:0.0306814863819834, data cost:0.2789950706517381 
2022-03-28 10:53:35,789: ============================================================
2022-03-28 10:53:35,789: Epoch 27/45 Batch 1400/7662 eta: 16:52:18.130027	Training Loss 0.4228 (0.4352)	Training Prec@1 93.164 (91.785)	Training Prec@5 95.312 (94.889)	
2022-03-28 10:53:35,789: ============================================================
2022-03-28 10:54:18,823: time cost, forward:0.10950228991390785, backward:0.030777233612386285, data cost:0.2797388527853001 
2022-03-28 10:54:18,824: ============================================================
2022-03-28 10:54:18,824: Epoch 27/45 Batch 1500/7662 eta: 17:13:25.240063	Training Loss 0.4294 (0.4352)	Training Prec@1 91.406 (91.791)	Training Prec@5 95.703 (94.892)	
2022-03-28 10:54:18,825: ============================================================
2022-03-28 10:55:02,543: time cost, forward:0.10946002418060016, backward:0.030849012902112512, data cost:0.2807594944045572 
2022-03-28 10:55:02,543: ============================================================
2022-03-28 10:55:02,543: Epoch 27/45 Batch 1600/7662 eta: 17:29:06.020220	Training Loss 0.4397 (0.4352)	Training Prec@1 91.211 (91.791)	Training Prec@5 95.508 (94.890)	
2022-03-28 10:55:02,543: ============================================================
2022-03-28 10:55:44,531: time cost, forward:0.1093911594191041, backward:0.030904398447208506, data cost:0.28064221280543084 
2022-03-28 10:55:44,532: ============================================================
2022-03-28 10:55:44,532: Epoch 27/45 Batch 1700/7662 eta: 16:46:52.532547	Training Loss 0.4342 (0.4352)	Training Prec@1 92.383 (91.791)	Training Prec@5 95.508 (94.884)	
2022-03-28 10:55:44,532: ============================================================
2022-03-28 10:56:27,510: time cost, forward:0.10928645128671033, backward:0.030927012402723737, data cost:0.2812232987094813 
2022-03-28 10:56:27,511: ============================================================
2022-03-28 10:56:27,512: Epoch 27/45 Batch 1800/7662 eta: 17:09:56.082937	Training Loss 0.4318 (0.4352)	Training Prec@1 91.211 (91.788)	Training Prec@5 93.359 (94.874)	
2022-03-28 10:56:27,512: ============================================================
2022-03-28 10:57:09,568: time cost, forward:0.10916255159463426, backward:0.031136615330322973, data cost:0.28106318968983307 
2022-03-28 10:57:09,568: ============================================================
2022-03-28 10:57:09,568: Epoch 27/45 Batch 1900/7662 eta: 16:47:06.668932	Training Loss 0.4338 (0.4352)	Training Prec@1 93.359 (91.781)	Training Prec@5 95.703 (94.873)	
2022-03-28 10:57:09,569: ============================================================
2022-03-28 10:57:51,888: time cost, forward:0.10911001796541123, backward:0.03133828214194072, data cost:0.28100887163094485 
2022-03-28 10:57:51,889: ============================================================
2022-03-28 10:57:51,889: Epoch 27/45 Batch 2000/7662 eta: 16:52:43.680543	Training Loss 0.4318 (0.4351)	Training Prec@1 90.820 (91.781)	Training Prec@5 96.094 (94.872)	
2022-03-28 10:57:51,889: ============================================================
2022-03-28 10:58:35,379: time cost, forward:0.10900133595459799, backward:0.031406594663077506, data cost:0.2816803905156296 
2022-03-28 10:58:35,409: ============================================================
2022-03-28 10:58:35,409: Epoch 27/45 Batch 2100/7662 eta: 17:20:41.991672	Training Loss 0.4356 (0.4351)	Training Prec@1 91.406 (91.781)	Training Prec@5 95.312 (94.869)	
2022-03-28 10:58:35,409: ============================================================
2022-03-28 10:59:18,229: time cost, forward:0.10889708741028886, backward:0.031508307394085826, data cost:0.281969881751636 
2022-03-28 10:59:18,229: ============================================================
2022-03-28 10:59:18,229: Epoch 27/45 Batch 2200/7662 eta: 17:03:15.293471	Training Loss 0.4326 (0.4351)	Training Prec@1 91.211 (91.784)	Training Prec@5 93.555 (94.874)	
2022-03-28 10:59:18,230: ============================================================
2022-03-28 11:00:00,016: time cost, forward:0.10882682839700998, backward:0.03160791077682276, data cost:0.28171608892924477 
2022-03-28 11:00:00,016: ============================================================
2022-03-28 11:00:00,016: Epoch 27/45 Batch 2300/7662 eta: 16:37:51.583330	Training Loss 0.4322 (0.4351)	Training Prec@1 91.016 (91.773)	Training Prec@5 94.336 (94.869)	
2022-03-28 11:00:00,016: ============================================================
2022-03-28 11:00:42,928: time cost, forward:0.1087787337181915, backward:0.0316309978585285, data cost:0.28201736694278695 
2022-03-28 11:00:42,928: ============================================================
2022-03-28 11:00:42,929: Epoch 27/45 Batch 2400/7662 eta: 17:04:01.877696	Training Loss 0.4298 (0.4351)	Training Prec@1 92.578 (91.779)	Training Prec@5 95.312 (94.870)	
2022-03-28 11:00:42,929: ============================================================
2022-03-28 11:01:27,407: time cost, forward:0.10874566320134621, backward:0.03165633977055788, data cost:0.28290351985597095 
2022-03-28 11:01:27,407: ============================================================
2022-03-28 11:01:27,408: Epoch 27/45 Batch 2500/7662 eta: 17:40:40.266824	Training Loss 0.4363 (0.4351)	Training Prec@1 92.188 (91.787)	Training Prec@5 95.898 (94.875)	
2022-03-28 11:01:27,408: ============================================================
2022-03-28 11:02:09,350: time cost, forward:0.10873313810606101, backward:0.03166255963770597, data cost:0.2827509037574102 
2022-03-28 11:02:09,350: ============================================================
2022-03-28 11:02:09,351: Epoch 27/45 Batch 2600/7662 eta: 16:39:29.441755	Training Loss 0.4339 (0.4351)	Training Prec@1 91.406 (91.789)	Training Prec@5 95.117 (94.877)	
2022-03-28 11:02:09,351: ============================================================
2022-03-28 11:02:50,867: time cost, forward:0.10873016943618871, backward:0.03168964094477877, data cost:0.28239394073443924 
2022-03-28 11:02:50,867: ============================================================
2022-03-28 11:02:50,867: Epoch 27/45 Batch 2700/7662 eta: 16:28:38.712602	Training Loss 0.4376 (0.4351)	Training Prec@1 92.188 (91.787)	Training Prec@5 95.508 (94.875)	
2022-03-28 11:02:50,868: ============================================================
2022-03-28 11:03:33,337: time cost, forward:0.1087137529278449, backward:0.03178163160805874, data cost:0.282374183038764 
2022-03-28 11:03:33,338: ============================================================
2022-03-28 11:03:33,338: Epoch 27/45 Batch 2800/7662 eta: 16:50:39.290490	Training Loss 0.4322 (0.4351)	Training Prec@1 92.773 (91.785)	Training Prec@5 95.508 (94.877)	
2022-03-28 11:03:33,338: ============================================================
2022-03-28 11:04:16,471: time cost, forward:0.10869981651266676, backward:0.031890787885862616, data cost:0.2825799124041028 
2022-03-28 11:04:16,472: ============================================================
2022-03-28 11:04:16,472: Epoch 27/45 Batch 2900/7662 eta: 17:05:42.731332	Training Loss 0.4322 (0.4351)	Training Prec@1 92.188 (91.787)	Training Prec@5 95.117 (94.874)	
2022-03-28 11:04:16,472: ============================================================
2022-03-28 11:05:00,946: time cost, forward:0.10869458191551419, backward:0.03186459579480493, data cost:0.2833192676017586 
2022-03-28 11:05:00,972: ============================================================
2022-03-28 11:05:00,973: Epoch 27/45 Batch 3000/7662 eta: 17:37:28.824036	Training Loss 0.4406 (0.4352)	Training Prec@1 92.578 (91.782)	Training Prec@5 95.312 (94.867)	
2022-03-28 11:05:00,973: ============================================================
2022-03-28 11:05:42,725: time cost, forward:0.10869676453638707, backward:0.03187212616291766, data cost:0.2830954556620556 
2022-03-28 11:05:42,725: ============================================================
2022-03-28 11:05:42,725: Epoch 27/45 Batch 3100/7662 eta: 16:31:28.736825	Training Loss 0.4340 (0.4351)	Training Prec@1 93.750 (91.784)	Training Prec@5 95.508 (94.869)	
2022-03-28 11:05:42,725: ============================================================
2022-03-28 11:06:25,230: time cost, forward:0.10869799311662622, backward:0.03186139340473734, data cost:0.28313992477350214 
2022-03-28 11:06:25,230: ============================================================
2022-03-28 11:06:25,231: Epoch 27/45 Batch 3200/7662 eta: 16:48:38.488423	Training Loss 0.4479 (0.4352)	Training Prec@1 92.188 (91.787)	Training Prec@5 95.117 (94.874)	
2022-03-28 11:06:25,231: ============================================================
2022-03-28 11:07:07,540: time cost, forward:0.10868794204755854, backward:0.031867795367788714, data cost:0.28308943091973854 
2022-03-28 11:07:07,540: ============================================================
2022-03-28 11:07:07,540: Epoch 27/45 Batch 3300/7662 eta: 16:43:17.731919	Training Loss 0.4309 (0.4352)	Training Prec@1 92.578 (91.777)	Training Prec@5 94.727 (94.867)	
2022-03-28 11:07:07,540: ============================================================
2022-03-28 11:07:49,801: time cost, forward:0.10869128860491589, backward:0.031994700326608116, data cost:0.2829596730462591 
2022-03-28 11:07:49,802: ============================================================
2022-03-28 11:07:49,802: Epoch 27/45 Batch 3400/7662 eta: 16:41:27.223339	Training Loss 0.4310 (0.4352)	Training Prec@1 91.992 (91.775)	Training Prec@5 95.703 (94.864)	
2022-03-28 11:07:49,802: ============================================================
2022-03-28 11:08:31,694: time cost, forward:0.10870423784389534, backward:0.032100740791423284, data cost:0.28269691990593154 
2022-03-28 11:08:31,695: ============================================================
2022-03-28 11:08:31,695: Epoch 27/45 Batch 3500/7662 eta: 16:32:01.071302	Training Loss 0.4312 (0.4352)	Training Prec@1 89.453 (91.771)	Training Prec@5 93.555 (94.861)	
2022-03-28 11:08:31,695: ============================================================
2022-03-28 11:09:14,740: time cost, forward:0.10870530036264872, backward:0.03219437685301649, data cost:0.28276976442032303 
2022-03-28 11:09:14,741: ============================================================
2022-03-28 11:09:14,741: Epoch 27/45 Batch 3600/7662 eta: 16:58:36.622235	Training Loss 0.4309 (0.4353)	Training Prec@1 91.992 (91.766)	Training Prec@5 94.922 (94.859)	
2022-03-28 11:09:14,741: ============================================================
2022-03-28 11:09:58,522: time cost, forward:0.10870312690477044, backward:0.03229828936630212, data cost:0.2830427059066331 
2022-03-28 11:09:58,523: ============================================================
2022-03-28 11:09:58,523: Epoch 27/45 Batch 3700/7662 eta: 17:15:17.119179	Training Loss 0.4316 (0.4353)	Training Prec@1 93.750 (91.762)	Training Prec@5 96.875 (94.859)	
2022-03-28 11:09:58,523: ============================================================
2022-03-28 11:10:40,640: time cost, forward:0.10871236491874821, backward:0.032360586626776336, data cost:0.28285606122700974 
2022-03-28 11:10:40,641: ============================================================
2022-03-28 11:10:40,641: Epoch 27/45 Batch 3800/7662 eta: 16:35:14.688164	Training Loss 0.4327 (0.4353)	Training Prec@1 91.406 (91.762)	Training Prec@5 95.508 (94.861)	
2022-03-28 11:10:40,641: ============================================================
2022-03-28 11:11:22,909: time cost, forward:0.10871011234302036, backward:0.0323934751952358, data cost:0.2828078708394303 
2022-03-28 11:11:22,910: ============================================================
2022-03-28 11:11:22,910: Epoch 27/45 Batch 3900/7662 eta: 16:38:06.348267	Training Loss 0.4315 (0.4354)	Training Prec@1 92.578 (91.753)	Training Prec@5 94.336 (94.854)	
2022-03-28 11:11:22,910: ============================================================
2022-03-28 11:12:04,914: time cost, forward:0.10870768601192657, backward:0.03240200566422734, data cost:0.2827109598344372 
2022-03-28 11:12:04,914: ============================================================
2022-03-28 11:12:04,915: Epoch 27/45 Batch 4000/7662 eta: 16:31:09.698440	Training Loss 0.4455 (0.4354)	Training Prec@1 88.867 (91.751)	Training Prec@5 93.359 (94.851)	
2022-03-28 11:12:04,915: ============================================================
2022-03-28 11:12:46,838: time cost, forward:0.10870310672174636, backward:0.03238271079025026, data cost:0.28259952221302964 
2022-03-28 11:12:46,838: ============================================================
2022-03-28 11:12:46,838: Epoch 27/45 Batch 4100/7662 eta: 16:28:33.205009	Training Loss 0.4498 (0.4354)	Training Prec@1 90.039 (91.752)	Training Prec@5 94.336 (94.850)	
2022-03-28 11:12:46,838: ============================================================
2022-03-28 11:13:28,797: time cost, forward:0.10871227152661557, backward:0.0323798958532411, data cost:0.2824925987968617 
2022-03-28 11:13:28,798: ============================================================
2022-03-28 11:13:28,798: Epoch 27/45 Batch 4200/7662 eta: 16:28:42.087450	Training Loss 0.4467 (0.4354)	Training Prec@1 91.602 (91.746)	Training Prec@5 94.336 (94.848)	
2022-03-28 11:13:28,798: ============================================================
2022-03-28 11:14:10,444: time cost, forward:0.1087182302978544, backward:0.03239085430820533, data cost:0.2823005155176028 
2022-03-28 11:14:10,444: ============================================================
2022-03-28 11:14:10,444: Epoch 27/45 Batch 4300/7662 eta: 16:20:37.589463	Training Loss 0.4305 (0.4354)	Training Prec@1 90.820 (91.747)	Training Prec@5 95.117 (94.851)	
2022-03-28 11:14:10,444: ============================================================
2022-03-28 11:14:52,326: time cost, forward:0.10872209410852778, backward:0.0324037469064791, data cost:0.2821545727715272 
2022-03-28 11:14:52,327: ============================================================
2022-03-28 11:14:52,327: Epoch 27/45 Batch 4400/7662 eta: 16:25:29.834146	Training Loss 0.4341 (0.4354)	Training Prec@1 93.945 (91.748)	Training Prec@5 96.484 (94.853)	
2022-03-28 11:14:52,327: ============================================================
2022-03-28 11:15:34,663: time cost, forward:0.10899475559337216, backward:0.03239132553027667, data cost:0.2819205107861663 
2022-03-28 11:15:34,663: ============================================================
2022-03-28 11:15:34,663: Epoch 27/45 Batch 4500/7662 eta: 16:35:27.566641	Training Loss 0.4431 (0.4355)	Training Prec@1 90.039 (91.746)	Training Prec@5 92.969 (94.850)	
2022-03-28 11:15:34,664: ============================================================
2022-03-28 11:16:17,204: time cost, forward:0.10907637842273318, backward:0.03238280920081357, data cost:0.28184576842234843 
2022-03-28 11:16:17,205: ============================================================
2022-03-28 11:16:17,205: Epoch 27/45 Batch 4600/7662 eta: 16:39:34.706115	Training Loss 0.4455 (0.4355)	Training Prec@1 91.016 (91.745)	Training Prec@5 94.531 (94.847)	
2022-03-28 11:16:17,205: ============================================================
2022-03-28 11:16:59,661: time cost, forward:0.109064588396568, backward:0.03237510757462525, data cost:0.2819024855391881 
2022-03-28 11:16:59,662: ============================================================
2022-03-28 11:16:59,662: Epoch 27/45 Batch 4700/7662 eta: 16:36:52.828106	Training Loss 0.4389 (0.4355)	Training Prec@1 91.797 (91.743)	Training Prec@5 94.922 (94.845)	
2022-03-28 11:16:59,662: ============================================================
2022-03-28 11:17:42,144: time cost, forward:0.10905428567661, backward:0.032403848796716504, data cost:0.2819112755154838 
2022-03-28 11:17:42,144: ============================================================
2022-03-28 11:17:42,145: Epoch 27/45 Batch 4800/7662 eta: 16:36:46.769055	Training Loss 0.4446 (0.4355)	Training Prec@1 92.383 (91.741)	Training Prec@5 95.508 (94.844)	
2022-03-28 11:17:42,145: ============================================================
2022-03-28 11:18:24,367: time cost, forward:0.10904989619234626, backward:0.03245178503171598, data cost:0.2818471794105058 
2022-03-28 11:18:24,367: ============================================================
2022-03-28 11:18:24,367: Epoch 27/45 Batch 4900/7662 eta: 16:29:58.389455	Training Loss 0.4427 (0.4355)	Training Prec@1 90.625 (91.740)	Training Prec@5 94.336 (94.845)	
2022-03-28 11:18:24,367: ============================================================
2022-03-28 11:19:07,601: time cost, forward:0.10902597756833166, backward:0.032539853813124076, data cost:0.2819567578486095 
2022-03-28 11:19:07,601: ============================================================
2022-03-28 11:19:07,602: Epoch 27/45 Batch 5000/7662 eta: 16:52:58.383657	Training Loss 0.4437 (0.4355)	Training Prec@1 91.797 (91.740)	Training Prec@5 93.945 (94.847)	
2022-03-28 11:19:07,602: ============================================================
2022-03-28 11:19:51,056: time cost, forward:0.10901431958800228, backward:0.03258272254436618, data cost:0.28213799072261136 
2022-03-28 11:19:51,057: ============================================================
2022-03-28 11:19:51,057: Epoch 27/45 Batch 5100/7662 eta: 16:57:25.876472	Training Loss 0.4343 (0.4355)	Training Prec@1 92.383 (91.740)	Training Prec@5 95.508 (94.845)	
2022-03-28 11:19:51,057: ============================================================
2022-03-28 11:20:31,871: time cost, forward:0.10900269629611811, backward:0.03258640515297554, data cost:0.28182469218665535 
2022-03-28 11:20:31,871: ============================================================
2022-03-28 11:20:31,871: Epoch 27/45 Batch 5200/7662 eta: 15:54:54.491411	Training Loss 0.4337 (0.4355)	Training Prec@1 91.797 (91.735)	Training Prec@5 95.508 (94.840)	
2022-03-28 11:20:31,872: ============================================================
2022-03-28 11:21:13,370: time cost, forward:0.10898449938799845, backward:0.0325700887128258, data cost:0.28169330792014025 
2022-03-28 11:21:13,370: ============================================================
2022-03-28 11:21:13,370: Epoch 27/45 Batch 5300/7662 eta: 16:10:14.239167	Training Loss 0.4440 (0.4355)	Training Prec@1 90.625 (91.734)	Training Prec@5 93.164 (94.838)	
2022-03-28 11:21:13,370: ============================================================
2022-03-28 11:21:55,869: time cost, forward:0.10897204500146432, backward:0.03253372034820766, data cost:0.28177498848179044 
2022-03-28 11:21:55,870: ============================================================
2022-03-28 11:21:55,870: Epoch 27/45 Batch 5400/7662 eta: 16:32:55.869125	Training Loss 0.4403 (0.4355)	Training Prec@1 90.625 (91.736)	Training Prec@5 93.945 (94.839)	
2022-03-28 11:21:55,870: ============================================================
2022-03-28 11:22:38,222: time cost, forward:0.10895885036390117, backward:0.03248411608860913, data cost:0.28183072444460006 
2022-03-28 11:22:38,222: ============================================================
2022-03-28 11:22:38,223: Epoch 27/45 Batch 5500/7662 eta: 16:28:47.153463	Training Loss 0.4382 (0.4356)	Training Prec@1 92.188 (91.734)	Training Prec@5 95.703 (94.839)	
2022-03-28 11:22:38,223: ============================================================
2022-03-28 11:23:19,371: time cost, forward:0.10913295472470921, backward:0.03253271797338241, data cost:0.2813980196560892 
2022-03-28 11:23:19,372: ============================================================
2022-03-28 11:23:19,373: Epoch 27/45 Batch 5600/7662 eta: 16:00:01.430387	Training Loss 0.4552 (0.4356)	Training Prec@1 89.648 (91.734)	Training Prec@5 93.555 (94.842)	
2022-03-28 11:23:19,374: ============================================================
2022-03-28 11:24:01,862: time cost, forward:0.10912868373748356, backward:0.03253048460699839, data cost:0.28142915380316924 
2022-03-28 11:24:01,863: ============================================================
2022-03-28 11:24:01,863: Epoch 27/45 Batch 5700/7662 eta: 16:30:34.636471	Training Loss 0.4310 (0.4356)	Training Prec@1 92.969 (91.737)	Training Prec@5 95.508 (94.843)	
2022-03-28 11:24:01,863: ============================================================
2022-03-28 11:24:43,972: time cost, forward:0.10911454605961651, backward:0.032528606565435665, data cost:0.2814041301081316 
2022-03-28 11:24:43,972: ============================================================
2022-03-28 11:24:43,973: Epoch 27/45 Batch 5800/7662 eta: 16:21:00.860104	Training Loss 0.4453 (0.4356)	Training Prec@1 91.406 (91.735)	Training Prec@5 94.727 (94.840)	
2022-03-28 11:24:43,973: ============================================================
2022-03-28 11:25:27,300: time cost, forward:0.1091011498495368, backward:0.032534120971620194, data cost:0.2815790133145648 
2022-03-28 11:25:27,300: ============================================================
2022-03-28 11:25:27,300: Epoch 27/45 Batch 5900/7662 eta: 16:48:39.615021	Training Loss 0.4440 (0.4356)	Training Prec@1 88.867 (91.733)	Training Prec@5 94.922 (94.839)	
2022-03-28 11:25:27,301: ============================================================
2022-03-28 11:26:10,502: time cost, forward:0.10908444918559221, backward:0.03254884480436478, data cost:0.2817156367310684 
2022-03-28 11:26:10,502: ============================================================
2022-03-28 11:26:10,502: Epoch 27/45 Batch 6000/7662 eta: 16:45:00.681505	Training Loss 0.4432 (0.4356)	Training Prec@1 92.578 (91.730)	Training Prec@5 96.289 (94.839)	
2022-03-28 11:26:10,502: ============================================================
2022-03-28 11:26:53,234: time cost, forward:0.10907219757621729, backward:0.032564649739838525, data cost:0.281777166620515 
2022-03-28 11:26:53,235: ============================================================
2022-03-28 11:26:53,235: Epoch 27/45 Batch 6100/7662 eta: 16:33:23.607017	Training Loss 0.4218 (0.4356)	Training Prec@1 93.359 (91.729)	Training Prec@5 95.312 (94.838)	
2022-03-28 11:26:53,235: ============================================================
2022-03-28 11:27:36,039: time cost, forward:0.10906466435455357, backward:0.032552110012317205, data cost:0.28186218083568265 
2022-03-28 11:27:36,039: ============================================================
2022-03-28 11:27:36,039: Epoch 27/45 Batch 6200/7662 eta: 16:34:19.815932	Training Loss 0.4286 (0.4356)	Training Prec@1 91.602 (91.723)	Training Prec@5 94.727 (94.835)	
2022-03-28 11:27:36,039: ============================================================
2022-03-28 11:28:17,855: time cost, forward:0.10905555998224893, backward:0.032553148527034864, data cost:0.28178254357026217 
2022-03-28 11:28:17,856: ============================================================
2022-03-28 11:28:17,856: Epoch 27/45 Batch 6300/7662 eta: 16:10:41.829765	Training Loss 0.4396 (0.4356)	Training Prec@1 93.164 (91.720)	Training Prec@5 96.094 (94.833)	
2022-03-28 11:28:17,856: ============================================================
2022-03-28 11:28:59,328: time cost, forward:0.10903821160670724, backward:0.03257059067334024, data cost:0.28163861587543493 
2022-03-28 11:28:59,329: ============================================================
2022-03-28 11:28:59,330: Epoch 27/45 Batch 6400/7662 eta: 16:02:02.952755	Training Loss 0.4439 (0.4357)	Training Prec@1 89.844 (91.717)	Training Prec@5 92.969 (94.831)	
2022-03-28 11:28:59,330: ============================================================
2022-03-28 11:29:41,176: time cost, forward:0.10903583425506296, backward:0.03251569970678707, data cost:0.2816163249191164 
2022-03-28 11:29:41,176: ============================================================
2022-03-28 11:29:41,176: Epoch 27/45 Batch 6500/7662 eta: 16:09:59.401824	Training Loss 0.4360 (0.4357)	Training Prec@1 91.406 (91.719)	Training Prec@5 94.531 (94.833)	
2022-03-28 11:29:41,176: ============================================================
2022-03-28 11:30:22,668: time cost, forward:0.10933935907794266, backward:0.03253111721512259, data cost:0.2811777864844352 
2022-03-28 11:30:22,668: ============================================================
2022-03-28 11:30:22,668: Epoch 27/45 Batch 6600/7662 eta: 16:01:05.592942	Training Loss 0.4398 (0.4357)	Training Prec@1 91.406 (91.716)	Training Prec@5 94.336 (94.831)	
2022-03-28 11:30:22,669: ============================================================
2022-03-28 11:31:05,604: time cost, forward:0.10940778093527352, backward:0.03253053497744952, data cost:0.28118693228888325 
2022-03-28 11:31:05,604: ============================================================
2022-03-28 11:31:05,605: Epoch 27/45 Batch 6700/7662 eta: 16:33:49.255134	Training Loss 0.4285 (0.4357)	Training Prec@1 91.406 (91.715)	Training Prec@5 94.727 (94.829)	
2022-03-28 11:31:05,605: ============================================================
2022-03-28 11:31:46,952: time cost, forward:0.1094041832195764, backward:0.03252647743275594, data cost:0.2810353412227011 
2022-03-28 11:31:46,952: ============================================================
2022-03-28 11:31:46,952: Epoch 27/45 Batch 6800/7662 eta: 15:56:22.190110	Training Loss 0.4377 (0.4357)	Training Prec@1 90.625 (91.715)	Training Prec@5 93.945 (94.830)	
2022-03-28 11:31:46,953: ============================================================
2022-03-28 11:32:28,587: time cost, forward:0.10940604473235106, backward:0.03252562959568451, data cost:0.2809455939040078 
2022-03-28 11:32:28,588: ============================================================
2022-03-28 11:32:28,588: Epoch 27/45 Batch 6900/7662 eta: 16:02:19.697957	Training Loss 0.4316 (0.4357)	Training Prec@1 91.797 (91.714)	Training Prec@5 95.898 (94.828)	
2022-03-28 11:32:28,588: ============================================================
2022-03-28 11:33:11,615: time cost, forward:0.10939810289316713, backward:0.03252094213341556, data cost:0.281063219226587 
2022-03-28 11:33:11,616: ============================================================
2022-03-28 11:33:11,616: Epoch 27/45 Batch 7000/7662 eta: 16:33:48.198825	Training Loss 0.4372 (0.4357)	Training Prec@1 91.602 (91.714)	Training Prec@5 95.312 (94.829)	
2022-03-28 11:33:11,617: ============================================================
2022-03-28 11:33:54,917: time cost, forward:0.10945269466705902, backward:0.03252239196396895, data cost:0.28115173312438546 
2022-03-28 11:33:54,917: ============================================================
2022-03-28 11:33:54,917: Epoch 27/45 Batch 7100/7662 eta: 16:39:22.692576	Training Loss 0.4329 (0.4357)	Training Prec@1 92.383 (91.712)	Training Prec@5 94.922 (94.827)	
2022-03-28 11:33:54,917: ============================================================
2022-03-28 11:34:37,910: time cost, forward:0.10944303957550604, backward:0.032517960906475844, data cost:0.2812482425116353 
2022-03-28 11:34:37,910: ============================================================
2022-03-28 11:34:37,910: Epoch 27/45 Batch 7200/7662 eta: 16:31:33.272295	Training Loss 0.4447 (0.4358)	Training Prec@1 92.188 (91.709)	Training Prec@5 96.484 (94.826)	
2022-03-28 11:34:37,910: ============================================================
2022-03-28 11:35:20,263: time cost, forward:0.10943147433524361, backward:0.032523692119675214, data cost:0.2812541195228829 
2022-03-28 11:35:20,264: ============================================================
2022-03-28 11:35:20,264: Epoch 27/45 Batch 7300/7662 eta: 16:16:06.613608	Training Loss 0.4441 (0.4358)	Training Prec@1 90.039 (91.707)	Training Prec@5 94.531 (94.822)	
2022-03-28 11:35:20,264: ============================================================
2022-03-28 11:36:03,628: time cost, forward:0.10963095715117142, backward:0.03254786416120925, data cost:0.2811646160523236 
2022-03-28 11:36:03,628: ============================================================
2022-03-28 11:36:03,628: Epoch 27/45 Batch 7400/7662 eta: 16:38:39.977912	Training Loss 0.4323 (0.4358)	Training Prec@1 92.383 (91.703)	Training Prec@5 94.727 (94.820)	
2022-03-28 11:36:03,628: ============================================================
2022-03-28 11:36:46,925: time cost, forward:0.10962290572458815, backward:0.03254002424093354, data cost:0.2813029055882174 
2022-03-28 11:36:46,925: ============================================================
2022-03-28 11:36:46,925: Epoch 27/45 Batch 7500/7662 eta: 16:36:24.324279	Training Loss 0.4438 (0.4358)	Training Prec@1 91.406 (91.700)	Training Prec@5 94.336 (94.818)	
2022-03-28 11:36:46,926: ============================================================
2022-03-28 11:37:29,668: time cost, forward:0.10962241149949532, backward:0.032529381109328785, data cost:0.2813641805368939 
2022-03-28 11:37:29,668: ============================================================
2022-03-28 11:37:29,669: Epoch 27/45 Batch 7600/7662 eta: 16:22:56.568301	Training Loss 0.4269 (0.4359)	Training Prec@1 93.750 (91.700)	Training Prec@5 95.703 (94.818)	
2022-03-28 11:37:29,669: ============================================================
2022-03-28 11:37:56,738: Epoch: 27/45 eta: 16:22:29.640118	Training Loss 0.4419 (0.4359)	Training Prec@1 92.383 (91.700)	Training Prec@5 96.484 (94.817)
2022-03-28 11:37:56,738: ============================================================
2022-03-28 11:38:40,569: time cost, forward:0.11033832665645715, backward:0.032744814651181, data cost:0.29494393714750655 
2022-03-28 11:38:40,570: ============================================================
2022-03-28 11:38:40,570: Epoch 28/45 Batch 100/7662 eta: 16:42:49.537063	Training Loss 0.4362 (0.4330)	Training Prec@1 91.797 (92.195)	Training Prec@5 95.703 (95.147)	
2022-03-28 11:38:40,570: ============================================================
2022-03-28 11:39:22,702: time cost, forward:0.11036006649534906, backward:0.03324599841132236, data cost:0.28576096697668335 
2022-03-28 11:39:22,702: ============================================================
2022-03-28 11:39:22,702: Epoch 28/45 Batch 200/7662 eta: 16:07:03.119135	Training Loss 0.4252 (0.4328)	Training Prec@1 93.359 (92.123)	Training Prec@5 95.898 (95.135)	
2022-03-28 11:39:22,702: ============================================================
2022-03-28 11:40:04,472: time cost, forward:0.1116549992641079, backward:0.034719474339564904, data cost:0.2790994173706973 
2022-03-28 11:40:04,472: ============================================================
2022-03-28 11:40:04,472: Epoch 28/45 Batch 300/7662 eta: 15:58:03.006725	Training Loss 0.4351 (0.4334)	Training Prec@1 91.406 (92.068)	Training Prec@5 94.727 (95.096)	
2022-03-28 11:40:04,473: ============================================================
2022-03-28 11:40:47,230: time cost, forward:0.11329966678954008, backward:0.03536906875763322, data cost:0.2770930310538538 
2022-03-28 11:40:47,230: ============================================================
2022-03-28 11:40:47,231: Epoch 28/45 Batch 400/7662 eta: 16:19:59.874453	Training Loss 0.4446 (0.4336)	Training Prec@1 89.648 (92.027)	Training Prec@5 93.945 (95.078)	
2022-03-28 11:40:47,231: ============================================================
2022-03-28 11:41:30,776: time cost, forward:0.11233921471483005, backward:0.03513313152030379, data cost:0.2801803921410937 
2022-03-28 11:41:30,776: ============================================================
2022-03-28 11:41:30,777: Epoch 28/45 Batch 500/7662 eta: 16:37:19.502967	Training Loss 0.4359 (0.4337)	Training Prec@1 90.820 (91.996)	Training Prec@5 95.312 (95.052)	
2022-03-28 11:41:30,777: ============================================================
2022-03-28 11:42:12,572: time cost, forward:0.11163592975406296, backward:0.034796837773267336, data cost:0.27954351165656854 
2022-03-28 11:42:12,573: ============================================================
2022-03-28 11:42:12,573: Epoch 28/45 Batch 600/7662 eta: 15:56:33.150174	Training Loss 0.4313 (0.4340)	Training Prec@1 91.211 (91.969)	Training Prec@5 94.336 (95.031)	
2022-03-28 11:42:12,573: ============================================================
2022-03-28 11:42:55,085: time cost, forward:0.11114283897334414, backward:0.03485073245134476, data cost:0.2797431372777587 
2022-03-28 11:42:55,085: ============================================================
2022-03-28 11:42:55,086: Epoch 28/45 Batch 700/7662 eta: 16:12:14.811974	Training Loss 0.4281 (0.4339)	Training Prec@1 91.992 (91.962)	Training Prec@5 95.703 (95.011)	
2022-03-28 11:42:55,086: ============================================================
2022-03-28 11:43:38,958: time cost, forward:0.11094391092340997, backward:0.034894986803152686, data cost:0.2815339150506355 
2022-03-28 11:43:38,959: ============================================================
2022-03-28 11:43:38,959: Epoch 28/45 Batch 800/7662 eta: 16:42:37.904225	Training Loss 0.4311 (0.4340)	Training Prec@1 92.773 (91.921)	Training Prec@5 95.898 (94.988)	
2022-03-28 11:43:38,959: ============================================================
2022-03-28 11:44:23,751: time cost, forward:0.11064764469431028, backward:0.03494903508229834, data cost:0.2840307413934998 
2022-03-28 11:44:23,752: ============================================================
2022-03-28 11:44:23,752: Epoch 28/45 Batch 900/7662 eta: 17:02:53.894444	Training Loss 0.4154 (0.4342)	Training Prec@1 91.992 (91.914)	Training Prec@5 95.508 (94.976)	
2022-03-28 11:44:23,752: ============================================================
2022-03-28 11:45:05,381: time cost, forward:0.11172675919365715, backward:0.0348129124493451, data cost:0.2817388494928797 
2022-03-28 11:45:05,381: ============================================================
2022-03-28 11:45:05,382: Epoch 28/45 Batch 1000/7662 eta: 15:49:58.113424	Training Loss 0.4357 (0.4343)	Training Prec@1 91.406 (91.900)	Training Prec@5 95.117 (94.964)	
2022-03-28 11:45:05,382: ============================================================
2022-03-28 11:45:48,670: time cost, forward:0.11139909064802286, backward:0.03465460103896664, data cost:0.28262476621702437 
2022-03-28 11:45:48,670: ============================================================
2022-03-28 11:45:48,670: Epoch 28/45 Batch 1100/7662 eta: 16:27:06.492299	Training Loss 0.4295 (0.4343)	Training Prec@1 91.602 (91.894)	Training Prec@5 95.508 (94.964)	
2022-03-28 11:45:48,671: ============================================================
2022-03-28 11:46:31,466: time cost, forward:0.11144199423038333, backward:0.034544974988057676, data cost:0.2826182132367793 
2022-03-28 11:46:31,466: ============================================================
2022-03-28 11:46:31,466: Epoch 28/45 Batch 1200/7662 eta: 16:15:09.264555	Training Loss 0.4403 (0.4342)	Training Prec@1 91.016 (91.888)	Training Prec@5 94.141 (94.962)	
2022-03-28 11:46:31,466: ============================================================
2022-03-28 11:47:14,574: time cost, forward:0.11118364517646537, backward:0.03446115026848421, data cost:0.2831047363149101 
2022-03-28 11:47:14,575: ============================================================
2022-03-28 11:47:14,575: Epoch 28/45 Batch 1300/7662 eta: 16:21:33.881131	Training Loss 0.4420 (0.4342)	Training Prec@1 92.188 (91.874)	Training Prec@5 94.531 (94.946)	
2022-03-28 11:47:14,575: ============================================================
2022-03-28 11:47:57,213: time cost, forward:0.11096470265664571, backward:0.034385063547675655, data cost:0.2831666329828989 
2022-03-28 11:47:57,213: ============================================================
2022-03-28 11:47:57,214: Epoch 28/45 Batch 1400/7662 eta: 16:10:08.779204	Training Loss 0.4324 (0.4343)	Training Prec@1 92.773 (91.863)	Training Prec@5 95.703 (94.936)	
2022-03-28 11:47:57,214: ============================================================
2022-03-28 11:48:40,640: time cost, forward:0.11079409553497294, backward:0.034386247694691154, data cost:0.28370428260284075 
2022-03-28 11:48:40,640: ============================================================
2022-03-28 11:48:40,640: Epoch 28/45 Batch 1500/7662 eta: 16:27:21.341651	Training Loss 0.4276 (0.4343)	Training Prec@1 93.945 (91.853)	Training Prec@5 96.484 (94.932)	
2022-03-28 11:48:40,640: ============================================================
2022-03-28 11:49:23,155: time cost, forward:0.11061128785716064, backward:0.03432695652411236, data cost:0.2837243379839813 
2022-03-28 11:49:23,156: ============================================================
2022-03-28 11:49:23,156: Epoch 28/45 Batch 1600/7662 eta: 16:05:56.082707	Training Loss 0.4516 (0.4343)	Training Prec@1 88.477 (91.843)	Training Prec@5 91.797 (94.929)	
2022-03-28 11:49:23,156: ============================================================
2022-03-28 11:50:05,626: time cost, forward:0.110488028018316, backward:0.0343188004608783, data cost:0.28360947740295483 
2022-03-28 11:50:05,626: ============================================================
2022-03-28 11:50:05,626: Epoch 28/45 Batch 1700/7662 eta: 16:04:11.941921	Training Loss 0.4367 (0.4343)	Training Prec@1 91.016 (91.844)	Training Prec@5 93.164 (94.926)	
2022-03-28 11:50:05,626: ============================================================
2022-03-28 11:50:47,588: time cost, forward:0.11069458454698242, backward:0.03427254736191568, data cost:0.2829184235301927 
2022-03-28 11:50:47,589: ============================================================
2022-03-28 11:50:47,589: Epoch 28/45 Batch 1800/7662 eta: 15:51:58.392549	Training Loss 0.4277 (0.4344)	Training Prec@1 93.164 (91.841)	Training Prec@5 95.703 (94.919)	
2022-03-28 11:50:47,589: ============================================================
2022-03-28 11:51:30,986: time cost, forward:0.11165207520857053, backward:0.03453602697925859, data cost:0.2819913453086544 
2022-03-28 11:51:30,986: ============================================================
2022-03-28 11:51:30,986: Epoch 28/45 Batch 1900/7662 eta: 16:23:47.672794	Training Loss 0.4237 (0.4343)	Training Prec@1 90.234 (91.840)	Training Prec@5 92.969 (94.921)	
2022-03-28 11:51:30,987: ============================================================
2022-03-28 11:52:13,227: time cost, forward:0.11238172556412941, backward:0.0346058016123922, data cost:0.2808936134823088 
2022-03-28 11:52:13,228: ============================================================
2022-03-28 11:52:13,228: Epoch 28/45 Batch 2000/7662 eta: 15:56:53.888505	Training Loss 0.4224 (0.4343)	Training Prec@1 91.406 (91.848)	Training Prec@5 94.531 (94.918)	
2022-03-28 11:52:13,228: ============================================================
2022-03-28 11:52:57,150: time cost, forward:0.11328126510703718, backward:0.0345586996637338, data cost:0.28055646227563547 
2022-03-28 11:52:57,150: ============================================================
2022-03-28 11:52:57,151: Epoch 28/45 Batch 2100/7662 eta: 16:34:13.995985	Training Loss 0.4385 (0.4343)	Training Prec@1 93.359 (91.847)	Training Prec@5 95.312 (94.919)	
2022-03-28 11:52:57,151: ============================================================
2022-03-28 11:53:40,806: time cost, forward:0.11426795065646933, backward:0.03472195098811033, data cost:0.2797255728558119 
2022-03-28 11:53:40,806: ============================================================
2022-03-28 11:53:40,806: Epoch 28/45 Batch 2200/7662 eta: 16:27:28.314076	Training Loss 0.4396 (0.4343)	Training Prec@1 90.234 (91.840)	Training Prec@5 94.141 (94.914)	
2022-03-28 11:53:40,807: ============================================================
2022-03-28 11:54:25,168: time cost, forward:0.11401694440074256, backward:0.03471048866577281, data cost:0.280622245540511 
2022-03-28 11:54:25,169: ============================================================
2022-03-28 11:54:25,169: Epoch 28/45 Batch 2300/7662 eta: 16:42:43.224128	Training Loss 0.4371 (0.4343)	Training Prec@1 91.211 (91.840)	Training Prec@5 94.531 (94.916)	
2022-03-28 11:54:25,169: ============================================================
2022-03-28 11:55:07,012: time cost, forward:0.11375461463482989, backward:0.03464144028540003, data cost:0.28048896093873393 
2022-03-28 11:55:07,013: ============================================================
2022-03-28 11:55:07,013: Epoch 28/45 Batch 2400/7662 eta: 15:45:06.067054	Training Loss 0.4353 (0.4344)	Training Prec@1 90.039 (91.832)	Training Prec@5 94.531 (94.914)	
2022-03-28 11:55:07,013: ============================================================
2022-03-28 11:55:50,427: time cost, forward:0.11350462788722666, backward:0.03462634863210421, data cost:0.28093950154066755 
2022-03-28 11:55:50,427: ============================================================
2022-03-28 11:55:50,427: Epoch 28/45 Batch 2500/7662 eta: 16:19:50.236040	Training Loss 0.4293 (0.4344)	Training Prec@1 92.188 (91.826)	Training Prec@5 95.508 (94.911)	
2022-03-28 11:55:50,428: ============================================================
2022-03-28 11:56:32,563: time cost, forward:0.1132898855411167, backward:0.03464807643941752, data cost:0.2808346862103124 
2022-03-28 11:56:32,564: ============================================================
2022-03-28 11:56:32,564: Epoch 28/45 Batch 2600/7662 eta: 15:50:18.116933	Training Loss 0.4358 (0.4344)	Training Prec@1 92.383 (91.831)	Training Prec@5 95.312 (94.918)	
2022-03-28 11:56:32,564: ============================================================
2022-03-28 11:57:14,701: time cost, forward:0.11309688353635683, backward:0.03460836101346831, data cost:0.28074001276568156 
2022-03-28 11:57:14,701: ============================================================
2022-03-28 11:57:14,701: Epoch 28/45 Batch 2700/7662 eta: 15:49:36.525706	Training Loss 0.4353 (0.4344)	Training Prec@1 91.211 (91.835)	Training Prec@5 94.922 (94.918)	
2022-03-28 11:57:14,701: ============================================================
2022-03-28 11:57:56,914: time cost, forward:0.11288797885190167, backward:0.0345169916115475, data cost:0.2808303115792937 
2022-03-28 11:57:56,915: ============================================================
2022-03-28 11:57:56,915: Epoch 28/45 Batch 2800/7662 eta: 15:50:37.972728	Training Loss 0.4376 (0.4345)	Training Prec@1 90.039 (91.834)	Training Prec@5 94.531 (94.914)	
2022-03-28 11:57:56,915: ============================================================
2022-03-28 11:58:40,627: time cost, forward:0.11269108605162774, backward:0.0345102942948014, data cost:0.28134133907053627 
2022-03-28 11:58:40,628: ============================================================
2022-03-28 11:58:40,628: Epoch 28/45 Batch 2900/7662 eta: 16:23:39.690609	Training Loss 0.4307 (0.4345)	Training Prec@1 92.383 (91.826)	Training Prec@5 94.727 (94.909)	
2022-03-28 11:58:40,628: ============================================================
2022-03-28 11:59:23,659: time cost, forward:0.11249149334593668, backward:0.034466758733115305, data cost:0.28162132036133425 
2022-03-28 11:59:23,659: ============================================================
2022-03-28 11:59:23,659: Epoch 28/45 Batch 3000/7662 eta: 16:07:36.676786	Training Loss 0.4242 (0.4345)	Training Prec@1 93.164 (91.828)	Training Prec@5 95.898 (94.909)	
2022-03-28 11:59:23,659: ============================================================
2022-03-28 12:00:06,209: time cost, forward:0.11276752612559247, backward:0.0344199272616135, data cost:0.28128105988614827 
2022-03-28 12:00:06,219: ============================================================
2022-03-28 12:00:06,219: Epoch 28/45 Batch 3100/7662 eta: 15:56:18.158837	Training Loss 0.4238 (0.4345)	Training Prec@1 93.750 (91.827)	Training Prec@5 96.289 (94.909)	
2022-03-28 12:00:06,219: ============================================================
2022-03-28 12:00:48,385: time cost, forward:0.11278436921022206, backward:0.03444887608727577, data cost:0.28098548468517937 
2022-03-28 12:00:48,386: ============================================================
2022-03-28 12:00:48,386: Epoch 28/45 Batch 3200/7662 eta: 15:46:46.083097	Training Loss 0.4328 (0.4345)	Training Prec@1 93.359 (91.829)	Training Prec@5 95.703 (94.910)	
2022-03-28 12:00:48,387: ============================================================
2022-03-28 12:01:30,926: time cost, forward:0.11271833867584441, backward:0.03448335255012616, data cost:0.2809274835491152 
2022-03-28 12:01:30,926: ============================================================
2022-03-28 12:01:30,926: Epoch 28/45 Batch 3300/7662 eta: 15:54:26.383714	Training Loss 0.4195 (0.4346)	Training Prec@1 93.555 (91.825)	Training Prec@5 96.484 (94.908)	
2022-03-28 12:01:30,927: ============================================================
2022-03-28 12:02:14,514: time cost, forward:0.11298851182650875, backward:0.03451436735244666, data cost:0.28085959571429864 
2022-03-28 12:02:14,514: ============================================================
2022-03-28 12:02:14,514: Epoch 28/45 Batch 3400/7662 eta: 16:17:13.257852	Training Loss 0.4339 (0.4345)	Training Prec@1 91.797 (91.825)	Training Prec@5 94.141 (94.911)	
2022-03-28 12:02:14,515: ============================================================
2022-03-28 12:02:57,164: time cost, forward:0.11371374695803241, backward:0.034499619749144984, data cost:0.28006396432643826 
2022-03-28 12:02:57,165: ============================================================
2022-03-28 12:02:57,166: Epoch 28/45 Batch 3500/7662 eta: 15:55:30.284113	Training Loss 0.4293 (0.4346)	Training Prec@1 88.867 (91.824)	Training Prec@5 93.555 (94.913)	
2022-03-28 12:02:57,166: ============================================================
2022-03-28 12:03:41,738: time cost, forward:0.11486782554654024, backward:0.034546432246033036, data cost:0.2793201597308609 
2022-03-28 12:03:41,738: ============================================================
2022-03-28 12:03:41,738: Epoch 28/45 Batch 3600/7662 eta: 16:37:48.707086	Training Loss 0.4363 (0.4346)	Training Prec@1 91.211 (91.827)	Training Prec@5 94.922 (94.914)	
2022-03-28 12:03:41,738: ============================================================
2022-03-28 12:04:24,946: time cost, forward:0.11486645865098757, backward:0.034505354259168175, data cost:0.27944210646248535 
2022-03-28 12:04:24,946: ============================================================
2022-03-28 12:04:24,947: Epoch 28/45 Batch 3700/7662 eta: 16:06:32.903043	Training Loss 0.4413 (0.4346)	Training Prec@1 90.625 (91.827)	Training Prec@5 94.336 (94.911)	
2022-03-28 12:04:24,947: ============================================================
2022-03-28 12:05:08,996: time cost, forward:0.11550779604980839, backward:0.034495153437165346, data cost:0.2790858872974192 
2022-03-28 12:05:08,996: ============================================================
2022-03-28 12:05:08,996: Epoch 28/45 Batch 3800/7662 eta: 16:24:38.354254	Training Loss 0.4348 (0.4346)	Training Prec@1 89.648 (91.823)	Training Prec@5 92.188 (94.905)	
2022-03-28 12:05:08,997: ============================================================
2022-03-28 12:05:53,814: time cost, forward:0.1157889887747871, backward:0.034503462865555036, data cost:0.2792793980437262 
2022-03-28 12:05:53,815: ============================================================
2022-03-28 12:05:53,815: Epoch 28/45 Batch 3900/7662 eta: 16:41:04.694853	Training Loss 0.4248 (0.4347)	Training Prec@1 93.359 (91.823)	Training Prec@5 95.312 (94.905)	
2022-03-28 12:05:53,815: ============================================================
2022-03-28 12:06:36,468: time cost, forward:0.11561276650959386, backward:0.0344795938669726, data cost:0.27941623453081593 
2022-03-28 12:06:36,469: ============================================================
2022-03-28 12:06:36,469: Epoch 28/45 Batch 4000/7662 eta: 15:52:00.949576	Training Loss 0.4351 (0.4347)	Training Prec@1 91.797 (91.819)	Training Prec@5 94.727 (94.902)	
2022-03-28 12:06:36,469: ============================================================
2022-03-28 12:07:18,689: time cost, forward:0.11542434731935053, backward:0.03445236721862436, data cost:0.2794335830150566 
2022-03-28 12:07:18,690: ============================================================
2022-03-28 12:07:18,690: Epoch 28/45 Batch 4100/7662 eta: 15:41:39.020358	Training Loss 0.4255 (0.4347)	Training Prec@1 91.992 (91.820)	Training Prec@5 94.922 (94.902)	
2022-03-28 12:07:18,690: ============================================================
2022-03-28 12:08:01,631: time cost, forward:0.11556729012144325, backward:0.03445404176060204, data cost:0.2793005020966499 
2022-03-28 12:08:01,631: ============================================================
2022-03-28 12:08:01,632: Epoch 28/45 Batch 4200/7662 eta: 15:57:00.016624	Training Loss 0.4422 (0.4347)	Training Prec@1 92.773 (91.822)	Training Prec@5 94.922 (94.903)	
2022-03-28 12:08:01,632: ============================================================
2022-03-28 12:08:44,688: time cost, forward:0.11557404671638172, backward:0.034456194265134445, data cost:0.2792844998057317 
2022-03-28 12:08:44,688: ============================================================
2022-03-28 12:08:44,688: Epoch 28/45 Batch 4300/7662 eta: 15:58:50.945949	Training Loss 0.4351 (0.4347)	Training Prec@1 89.453 (91.819)	Training Prec@5 93.359 (94.902)	
2022-03-28 12:08:44,688: ============================================================
2022-03-28 12:09:27,788: time cost, forward:0.11540371603682843, backward:0.034492704374569604, data cost:0.2794683914505425 
2022-03-28 12:09:27,788: ============================================================
2022-03-28 12:09:27,788: Epoch 28/45 Batch 4400/7662 eta: 15:59:05.792493	Training Loss 0.4373 (0.4348)	Training Prec@1 90.820 (91.818)	Training Prec@5 94.531 (94.901)	
2022-03-28 12:09:27,788: ============================================================
2022-03-28 12:10:11,473: time cost, forward:0.11523705118098347, backward:0.03431417412640227, data cost:0.2799599968769678 
2022-03-28 12:10:11,474: ============================================================
2022-03-28 12:10:11,474: Epoch 28/45 Batch 4500/7662 eta: 16:11:24.469334	Training Loss 0.4463 (0.4348)	Training Prec@1 91.211 (91.814)	Training Prec@5 94.531 (94.900)	
2022-03-28 12:10:11,474: ============================================================
2022-03-28 12:10:54,684: time cost, forward:0.11507724995041806, backward:0.034209571887317186, data cost:0.280273215463302 
2022-03-28 12:10:54,684: ============================================================
2022-03-28 12:10:54,684: Epoch 28/45 Batch 4600/7662 eta: 16:00:06.433406	Training Loss 0.4379 (0.4348)	Training Prec@1 91.992 (91.811)	Training Prec@5 95.117 (94.898)	
2022-03-28 12:10:54,684: ============================================================
2022-03-28 12:11:37,948: time cost, forward:0.11493157077074102, backward:0.034155552172615164, data cost:0.28052830711327403 
2022-03-28 12:11:37,948: ============================================================
2022-03-28 12:11:37,948: Epoch 28/45 Batch 4700/7662 eta: 16:00:34.996446	Training Loss 0.4443 (0.4348)	Training Prec@1 90.625 (91.809)	Training Prec@5 93.359 (94.896)	
2022-03-28 12:11:37,949: ============================================================
2022-03-28 12:12:20,857: time cost, forward:0.1148222398449913, backward:0.03417992889943235, data cost:0.28058539179122105 
2022-03-28 12:12:20,857: ============================================================
2022-03-28 12:12:20,858: Epoch 28/45 Batch 4800/7662 eta: 15:51:59.599341	Training Loss 0.4349 (0.4348)	Training Prec@1 91.406 (91.806)	Training Prec@5 94.727 (94.896)	
2022-03-28 12:12:20,858: ============================================================
2022-03-28 12:13:03,499: time cost, forward:0.11469484426751869, backward:0.03416575093881576, data cost:0.2806599910075773 
2022-03-28 12:13:03,500: ============================================================
2022-03-28 12:13:03,500: Epoch 28/45 Batch 4900/7662 eta: 15:45:21.243553	Training Loss 0.4362 (0.4348)	Training Prec@1 92.188 (91.804)	Training Prec@5 95.117 (94.896)	
2022-03-28 12:13:03,500: ============================================================
2022-03-28 12:13:47,452: time cost, forward:0.11497040046742259, backward:0.03418430506551139, data cost:0.28056303456583653 
2022-03-28 12:13:47,453: ============================================================
2022-03-28 12:13:47,453: Epoch 28/45 Batch 5000/7662 eta: 16:13:41.075650	Training Loss 0.4357 (0.4348)	Training Prec@1 91.602 (91.802)	Training Prec@5 94.141 (94.894)	
2022-03-28 12:13:47,453: ============================================================
2022-03-28 12:14:30,512: time cost, forward:0.11512577596845101, backward:0.03417732346686786, data cost:0.28041031094573715 
2022-03-28 12:14:30,512: ============================================================
2022-03-28 12:14:30,512: Epoch 28/45 Batch 5100/7662 eta: 15:53:10.508984	Training Loss 0.4289 (0.4348)	Training Prec@1 91.992 (91.801)	Training Prec@5 96.484 (94.897)	
2022-03-28 12:14:30,513: ============================================================
2022-03-28 12:15:13,773: time cost, forward:0.11544690775995095, backward:0.0342140744387771, data cost:0.2800753088175368 
2022-03-28 12:15:13,773: ============================================================
2022-03-28 12:15:13,774: Epoch 28/45 Batch 5200/7662 eta: 15:56:55.108496	Training Loss 0.4331 (0.4349)	Training Prec@1 91.992 (91.797)	Training Prec@5 95.703 (94.895)	
2022-03-28 12:15:13,774: ============================================================
2022-03-28 12:15:56,040: time cost, forward:0.11554446722521244, backward:0.03415553100515388, data cost:0.2799262218507737 
2022-03-28 12:15:56,040: ============================================================
2022-03-28 12:15:56,041: Epoch 28/45 Batch 5300/7662 eta: 15:34:13.137225	Training Loss 0.4385 (0.4349)	Training Prec@1 90.430 (91.796)	Training Prec@5 93.750 (94.892)	
2022-03-28 12:15:56,041: ============================================================
2022-03-28 12:16:39,243: time cost, forward:0.11620776766780394, backward:0.03415121513729869, data cost:0.27926424830726926 
2022-03-28 12:16:39,243: ============================================================
2022-03-28 12:16:39,244: Epoch 28/45 Batch 5400/7662 eta: 15:54:11.290850	Training Loss 0.4332 (0.4349)	Training Prec@1 90.430 (91.794)	Training Prec@5 94.141 (94.890)	
2022-03-28 12:16:39,244: ============================================================
2022-03-28 12:17:22,226: time cost, forward:0.11605882939478987, backward:0.034106607154448, data cost:0.27948325194972584 
2022-03-28 12:17:22,226: ============================================================
2022-03-28 12:17:22,226: Epoch 28/45 Batch 5500/7662 eta: 15:48:36.265619	Training Loss 0.4382 (0.4349)	Training Prec@1 91.602 (91.792)	Training Prec@5 94.336 (94.889)	
2022-03-28 12:17:22,226: ============================================================
2022-03-28 12:18:05,009: time cost, forward:0.11595180431079131, backward:0.034118942670214064, data cost:0.27951275227133304 
2022-03-28 12:18:05,009: ============================================================
2022-03-28 12:18:05,010: Epoch 28/45 Batch 5600/7662 eta: 15:43:29.795055	Training Loss 0.4300 (0.4349)	Training Prec@1 91.797 (91.790)	Training Prec@5 94.336 (94.887)	
2022-03-28 12:18:05,010: ============================================================
2022-03-28 12:18:49,281: time cost, forward:0.11647089261049805, backward:0.034167301071215184, data cost:0.27912692861361216 
2022-03-28 12:18:49,282: ============================================================
2022-03-28 12:18:49,282: Epoch 28/45 Batch 5700/7662 eta: 16:15:35.344239	Training Loss 0.4350 (0.4349)	Training Prec@1 91.016 (91.789)	Training Prec@5 93.164 (94.887)	
2022-03-28 12:18:49,282: ============================================================
2022-03-28 12:19:32,913: time cost, forward:0.11689331573049536, backward:0.03422989120029503, data cost:0.2788034480365109 
2022-03-28 12:19:32,913: ============================================================
2022-03-28 12:19:32,914: Epoch 28/45 Batch 5800/7662 eta: 16:00:44.828567	Training Loss 0.4358 (0.4350)	Training Prec@1 91.406 (91.793)	Training Prec@5 95.117 (94.890)	
2022-03-28 12:19:32,914: ============================================================
2022-03-28 12:20:15,132: time cost, forward:0.11695645667472357, backward:0.0342351240836113, data cost:0.2785994601180986 
2022-03-28 12:20:15,133: ============================================================
2022-03-28 12:20:15,133: Epoch 28/45 Batch 5900/7662 eta: 15:28:57.041413	Training Loss 0.4347 (0.4350)	Training Prec@1 92.188 (91.792)	Training Prec@5 95.117 (94.887)	
2022-03-28 12:20:15,133: ============================================================
2022-03-28 12:20:58,751: time cost, forward:0.11733643927958076, backward:0.034292579591423296, data cost:0.2782472928656521 
2022-03-28 12:20:58,751: ============================================================
2022-03-28 12:20:58,752: Epoch 28/45 Batch 6000/7662 eta: 15:59:00.216008	Training Loss 0.4292 (0.4350)	Training Prec@1 90.820 (91.790)	Training Prec@5 94.727 (94.886)	
2022-03-28 12:20:58,752: ============================================================
2022-03-28 12:21:42,950: time cost, forward:0.11778401339478249, backward:0.0343542987937946, data cost:0.27794822526185275 
2022-03-28 12:21:42,950: ============================================================
2022-03-28 12:21:42,951: Epoch 28/45 Batch 6100/7662 eta: 16:11:01.674678	Training Loss 0.4443 (0.4350)	Training Prec@1 89.062 (91.792)	Training Prec@5 92.773 (94.885)	
2022-03-28 12:21:42,951: ============================================================
2022-03-28 12:22:24,919: time cost, forward:0.11791510408127341, backward:0.034343171492759825, data cost:0.27764745730895307 
2022-03-28 12:22:24,920: ============================================================
2022-03-28 12:22:24,920: Epoch 28/45 Batch 6200/7662 eta: 15:21:21.051505	Training Loss 0.4305 (0.4350)	Training Prec@1 93.945 (91.793)	Training Prec@5 96.875 (94.886)	
2022-03-28 12:22:24,920: ============================================================
2022-03-28 12:23:08,105: time cost, forward:0.11777198206793602, backward:0.034326542595262126, data cost:0.27782590723469214 
2022-03-28 12:23:08,106: ============================================================
2022-03-28 12:23:08,106: Epoch 28/45 Batch 6300/7662 eta: 15:47:20.045903	Training Loss 0.4251 (0.4350)	Training Prec@1 94.336 (91.796)	Training Prec@5 96.680 (94.888)	
2022-03-28 12:23:08,106: ============================================================
2022-03-28 12:23:51,743: time cost, forward:0.11784615757353958, backward:0.03436361046242777, data cost:0.27779791559683453 
2022-03-28 12:23:51,743: ============================================================
2022-03-28 12:23:51,743: Epoch 28/45 Batch 6400/7662 eta: 15:56:30.473504	Training Loss 0.4401 (0.4350)	Training Prec@1 91.211 (91.793)	Training Prec@5 94.531 (94.888)	
2022-03-28 12:23:51,744: ============================================================
2022-03-28 12:24:34,982: time cost, forward:0.11789435462816658, backward:0.03435293229328044, data cost:0.27779892247022897 
2022-03-28 12:24:34,982: ============================================================
2022-03-28 12:24:34,982: Epoch 28/45 Batch 6500/7662 eta: 15:47:03.285822	Training Loss 0.4300 (0.4350)	Training Prec@1 90.820 (91.791)	Training Prec@5 93.945 (94.885)	
2022-03-28 12:24:34,983: ============================================================
2022-03-28 12:25:18,039: time cost, forward:0.11774922703446575, backward:0.034341205230426164, data cost:0.2779568590382985 
2022-03-28 12:25:18,039: ============================================================
2022-03-28 12:25:18,040: Epoch 28/45 Batch 6600/7662 eta: 15:42:21.591391	Training Loss 0.4315 (0.4350)	Training Prec@1 90.820 (91.788)	Training Prec@5 94.141 (94.882)	
2022-03-28 12:25:18,040: ============================================================
2022-03-28 12:26:01,582: time cost, forward:0.11811309067913624, backward:0.03437142045413546, data cost:0.2776443168180596 
2022-03-28 12:26:01,582: ============================================================
2022-03-28 12:26:01,582: Epoch 28/45 Batch 6700/7662 eta: 15:52:15.442560	Training Loss 0.4373 (0.4351)	Training Prec@1 90.234 (91.787)	Training Prec@5 92.773 (94.881)	
2022-03-28 12:26:01,583: ============================================================
2022-03-28 12:26:43,640: time cost, forward:0.11821367253554466, backward:0.03435431780017707, data cost:0.2773961877451169 
2022-03-28 12:26:43,641: ============================================================
2022-03-28 12:26:43,641: Epoch 28/45 Batch 6800/7662 eta: 15:19:06.126735	Training Loss 0.4366 (0.4351)	Training Prec@1 93.164 (91.785)	Training Prec@5 96.094 (94.879)	
2022-03-28 12:26:43,641: ============================================================
2022-03-28 12:27:27,694: time cost, forward:0.11806634519908511, backward:0.0343368239706194, data cost:0.2777105224082428 
2022-03-28 12:27:27,694: ============================================================
2022-03-28 12:27:27,695: Epoch 28/45 Batch 6900/7662 eta: 16:01:57.415329	Training Loss 0.4407 (0.4351)	Training Prec@1 89.062 (91.782)	Training Prec@5 93.750 (94.875)	
2022-03-28 12:27:27,695: ============================================================
2022-03-28 12:28:10,576: time cost, forward:0.11813750745977976, backward:0.034361762342358985, data cost:0.2775956843134028 
2022-03-28 12:28:10,576: ============================================================
2022-03-28 12:28:10,587: Epoch 28/45 Batch 7000/7662 eta: 15:35:52.985135	Training Loss 0.4350 (0.4351)	Training Prec@1 93.555 (91.782)	Training Prec@5 96.484 (94.875)	
2022-03-28 12:28:10,587: ============================================================
2022-03-28 12:28:55,239: time cost, forward:0.11818198479973341, backward:0.03438709067331097, data cost:0.2777445572230963 
2022-03-28 12:28:55,240: ============================================================
2022-03-28 12:28:55,240: Epoch 28/45 Batch 7100/7662 eta: 16:13:34.508616	Training Loss 0.4410 (0.4351)	Training Prec@1 90.039 (91.780)	Training Prec@5 94.922 (94.875)	
2022-03-28 12:28:55,240: ============================================================
2022-03-28 12:29:37,216: time cost, forward:0.1180476469370305, backward:0.034320446265573415, data cost:0.2778021147026256 
2022-03-28 12:29:37,217: ============================================================
2022-03-28 12:29:37,217: Epoch 28/45 Batch 7200/7662 eta: 15:14:30.699233	Training Loss 0.4472 (0.4351)	Training Prec@1 88.672 (91.781)	Training Prec@5 92.773 (94.876)	
2022-03-28 12:29:37,217: ============================================================
2022-03-28 12:30:19,557: time cost, forward:0.11810047643807575, backward:0.034310231749072995, data cost:0.27765339602344835 
2022-03-28 12:30:19,558: ============================================================
2022-03-28 12:30:19,558: Epoch 28/45 Batch 7300/7662 eta: 15:21:44.741200	Training Loss 0.4463 (0.4351)	Training Prec@1 90.820 (91.781)	Training Prec@5 94.141 (94.878)	
2022-03-28 12:30:19,558: ============================================================
2022-03-28 12:31:03,211: time cost, forward:0.11827056796861059, backward:0.0342943501449917, data cost:0.27759131487518857 
2022-03-28 12:31:03,212: ============================================================
2022-03-28 12:31:03,212: Epoch 28/45 Batch 7400/7662 eta: 15:49:35.637006	Training Loss 0.4358 (0.4351)	Training Prec@1 94.141 (91.781)	Training Prec@5 96.680 (94.877)	
2022-03-28 12:31:03,212: ============================================================
2022-03-28 12:31:45,138: time cost, forward:0.11818828152281266, backward:0.03429236706455066, data cost:0.2775191648719565 
2022-03-28 12:31:45,138: ============================================================
2022-03-28 12:31:45,139: Epoch 28/45 Batch 7500/7662 eta: 15:11:19.588021	Training Loss 0.4430 (0.4351)	Training Prec@1 91.211 (91.780)	Training Prec@5 94.727 (94.877)	
2022-03-28 12:31:45,139: ============================================================
2022-03-28 12:32:28,509: time cost, forward:0.11806346504511622, backward:0.03430002814045798, data cost:0.2776796207535909 
2022-03-28 12:32:28,509: ============================================================
2022-03-28 12:32:28,510: Epoch 28/45 Batch 7600/7662 eta: 15:41:59.905757	Training Loss 0.4332 (0.4351)	Training Prec@1 91.602 (91.780)	Training Prec@5 94.922 (94.876)	
2022-03-28 12:32:28,510: ============================================================
2022-03-28 12:32:56,575: Epoch: 28/45 eta: 15:41:32.581969	Training Loss 0.4311 (0.4352)	Training Prec@1 92.188 (91.779)	Training Prec@5 95.117 (94.876)
2022-03-28 12:32:56,575: ============================================================
2022-03-28 12:33:40,158: time cost, forward:0.1080533109530054, backward:0.03306966116934112, data cost:0.2940708001454671 
2022-03-28 12:33:40,159: ============================================================
2022-03-28 12:33:40,159: Epoch 29/45 Batch 100/7662 eta: 15:42:45.720531	Training Loss 0.4356 (0.4333)	Training Prec@1 93.359 (91.907)	Training Prec@5 96.680 (94.944)	
2022-03-28 12:33:40,159: ============================================================
2022-03-28 12:34:22,576: time cost, forward:0.10760124005264972, backward:0.032920312641853064, data cost:0.28952649490318105 
2022-03-28 12:34:22,576: ============================================================
2022-03-28 12:34:22,577: Epoch 29/45 Batch 200/7662 eta: 15:19:26.267555	Training Loss 0.4334 (0.4334)	Training Prec@1 92.188 (91.954)	Training Prec@5 95.312 (94.989)	
2022-03-28 12:34:22,577: ============================================================
2022-03-28 12:35:05,428: time cost, forward:0.10786514537390259, backward:0.03289851536320205, data cost:0.2885759722030282 
2022-03-28 12:35:05,429: ============================================================
2022-03-28 12:35:05,429: Epoch 29/45 Batch 300/7662 eta: 15:28:08.876783	Training Loss 0.4350 (0.4333)	Training Prec@1 92.969 (92.005)	Training Prec@5 94.727 (95.015)	
2022-03-28 12:35:05,429: ============================================================
2022-03-28 12:35:48,034: time cost, forward:0.10804964306958038, backward:0.03278132727869172, data cost:0.287574463320854 
2022-03-28 12:35:48,035: ============================================================
2022-03-28 12:35:48,035: Epoch 29/45 Batch 400/7662 eta: 15:22:06.221850	Training Loss 0.4333 (0.4337)	Training Prec@1 91.797 (91.963)	Training Prec@5 94.141 (94.970)	
2022-03-28 12:35:48,035: ============================================================
2022-03-28 12:36:29,747: time cost, forward:0.10818696690943533, backward:0.03253920379287016, data cost:0.2845163823129658 
2022-03-28 12:36:29,747: ============================================================
2022-03-28 12:36:29,748: Epoch 29/45 Batch 500/7662 eta: 15:02:04.004692	Training Loss 0.4278 (0.4337)	Training Prec@1 92.578 (91.918)	Training Prec@5 96.094 (94.930)	
2022-03-28 12:36:29,748: ============================================================
2022-03-28 12:37:13,497: time cost, forward:0.10828103804230092, backward:0.03259751991755974, data cost:0.28690975058655904 
2022-03-28 12:37:13,497: ============================================================
2022-03-28 12:37:13,498: Epoch 29/45 Batch 600/7662 eta: 15:45:24.256628	Training Loss 0.4417 (0.4337)	Training Prec@1 90.430 (91.901)	Training Prec@5 93.555 (94.930)	
2022-03-28 12:37:13,498: ============================================================
2022-03-28 12:37:55,717: time cost, forward:0.10835890947322818, backward:0.0331568963537912, data cost:0.2855092050009361 
2022-03-28 12:37:55,717: ============================================================
2022-03-28 12:37:55,717: Epoch 29/45 Batch 700/7662 eta: 15:11:37.540337	Training Loss 0.4271 (0.4336)	Training Prec@1 92.773 (91.914)	Training Prec@5 95.703 (94.948)	
2022-03-28 12:37:55,717: ============================================================
2022-03-28 12:38:38,598: time cost, forward:0.1084603344245309, backward:0.033591769066859546, data cost:0.2849801010423071 
2022-03-28 12:38:38,598: ============================================================
2022-03-28 12:38:38,599: Epoch 29/45 Batch 800/7662 eta: 15:25:11.997065	Training Loss 0.4447 (0.4336)	Training Prec@1 90.039 (91.918)	Training Prec@5 93.164 (94.943)	
2022-03-28 12:38:38,599: ============================================================
2022-03-28 12:39:21,551: time cost, forward:0.10847415701300735, backward:0.033372016319045766, data cost:0.28556379327784653 
2022-03-28 12:39:21,551: ============================================================
2022-03-28 12:39:21,552: Epoch 29/45 Batch 900/7662 eta: 15:26:01.874519	Training Loss 0.4370 (0.4335)	Training Prec@1 93.750 (91.928)	Training Prec@5 96.094 (94.963)	
2022-03-28 12:39:21,552: ============================================================
2022-03-28 12:40:03,534: time cost, forward:0.10849562254515258, backward:0.03329100002636303, data cost:0.28481857101241864 
2022-03-28 12:40:03,534: ============================================================
2022-03-28 12:40:03,535: Epoch 29/45 Batch 1000/7662 eta: 15:04:25.265994	Training Loss 0.4420 (0.4335)	Training Prec@1 91.797 (91.932)	Training Prec@5 95.508 (94.964)	
2022-03-28 12:40:03,535: ============================================================
2022-03-28 12:40:46,091: time cost, forward:0.10849392034445599, backward:0.03323880925408486, data cost:0.284741138739408 
2022-03-28 12:40:46,092: ============================================================
2022-03-28 12:40:46,092: Epoch 29/45 Batch 1100/7662 eta: 15:16:04.925582	Training Loss 0.4198 (0.4335)	Training Prec@1 93.359 (91.935)	Training Prec@5 96.094 (94.975)	
2022-03-28 12:40:46,092: ============================================================
2022-03-28 12:41:28,220: time cost, forward:0.10850196067644617, backward:0.03310355770279707, data cost:0.284420739222408 
2022-03-28 12:41:28,221: ============================================================
2022-03-28 12:41:28,221: Epoch 29/45 Batch 1200/7662 eta: 15:06:09.236577	Training Loss 0.4369 (0.4335)	Training Prec@1 91.016 (91.921)	Training Prec@5 94.336 (94.955)	
2022-03-28 12:41:28,221: ============================================================
2022-03-28 12:42:10,525: time cost, forward:0.1085037762977418, backward:0.0330035242325898, data cost:0.2843050035354814 
2022-03-28 12:42:10,525: ============================================================
2022-03-28 12:42:10,525: Epoch 29/45 Batch 1300/7662 eta: 15:09:13.971641	Training Loss 0.4321 (0.4336)	Training Prec@1 92.773 (91.915)	Training Prec@5 96.094 (94.955)	
2022-03-28 12:42:10,526: ============================================================
2022-03-28 12:42:52,564: time cost, forward:0.10851484370964437, backward:0.032970963417418606, data cost:0.28387639980984214 
2022-03-28 12:42:52,565: ============================================================
2022-03-28 12:42:52,565: Epoch 29/45 Batch 1400/7662 eta: 15:02:49.815237	Training Loss 0.4316 (0.4336)	Training Prec@1 91.992 (91.910)	Training Prec@5 94.531 (94.947)	
2022-03-28 12:42:52,565: ============================================================
2022-03-28 12:43:35,095: time cost, forward:0.1085394578746671, backward:0.032973605366529345, data cost:0.2838207841953331 
2022-03-28 12:43:35,095: ============================================================
2022-03-28 12:43:35,095: Epoch 29/45 Batch 1500/7662 eta: 15:12:40.022094	Training Loss 0.4461 (0.4336)	Training Prec@1 91.797 (91.906)	Training Prec@5 95.898 (94.953)	
2022-03-28 12:43:35,095: ============================================================
2022-03-28 12:44:16,817: time cost, forward:0.10853634557551037, backward:0.03291038187538705, data cost:0.2833847468759657 
2022-03-28 12:44:16,817: ============================================================
2022-03-28 12:44:16,817: Epoch 29/45 Batch 1600/7662 eta: 14:54:37.404289	Training Loss 0.4187 (0.4336)	Training Prec@1 93.750 (91.915)	Training Prec@5 96.680 (94.957)	
2022-03-28 12:44:16,817: ============================================================
2022-03-28 12:44:59,216: time cost, forward:0.10853406357161784, backward:0.032839648649228045, data cost:0.2833441889517584 
2022-03-28 12:44:59,217: ============================================================
2022-03-28 12:44:59,218: Epoch 29/45 Batch 1700/7662 eta: 15:08:27.666773	Training Loss 0.4404 (0.4336)	Training Prec@1 90.430 (91.911)	Training Prec@5 93.945 (94.955)	
2022-03-28 12:44:59,218: ============================================================
2022-03-28 12:45:43,795: time cost, forward:0.10851113421178779, backward:0.03282525500964959, data cost:0.28454583230583186 
2022-03-28 12:45:43,796: ============================================================
2022-03-28 12:45:43,796: Epoch 29/45 Batch 1800/7662 eta: 15:54:23.561819	Training Loss 0.4354 (0.4335)	Training Prec@1 91.211 (91.917)	Training Prec@5 94.141 (94.966)	
2022-03-28 12:45:43,796: ============================================================
2022-03-28 12:46:26,846: time cost, forward:0.10850621675930254, backward:0.03279879609178781, data cost:0.28479546054279636 
2022-03-28 12:46:26,847: ============================================================
2022-03-28 12:46:26,847: Epoch 29/45 Batch 1900/7662 eta: 15:20:57.772435	Training Loss 0.4469 (0.4336)	Training Prec@1 89.258 (91.907)	Training Prec@5 93.945 (94.960)	
2022-03-28 12:46:26,847: ============================================================
2022-03-28 12:47:08,996: time cost, forward:0.10848090349286124, backward:0.032752698155985645, data cost:0.28462435007691683 
2022-03-28 12:47:08,996: ============================================================
2022-03-28 12:47:08,996: Epoch 29/45 Batch 2000/7662 eta: 15:00:58.494941	Training Loss 0.4426 (0.4336)	Training Prec@1 91.211 (91.915)	Training Prec@5 95.898 (94.964)	
2022-03-28 12:47:08,996: ============================================================
2022-03-28 12:47:52,388: time cost, forward:0.108455074918219, backward:0.032720521269894144, data cost:0.2850387809729792 
2022-03-28 12:47:52,389: ============================================================
2022-03-28 12:47:52,389: Epoch 29/45 Batch 2100/7662 eta: 15:26:49.921046	Training Loss 0.4285 (0.4337)	Training Prec@1 92.773 (91.914)	Training Prec@5 95.117 (94.959)	
2022-03-28 12:47:52,389: ============================================================
2022-03-28 12:48:35,244: time cost, forward:0.10846618502288582, backward:0.03269055920766125, data cost:0.2851188778064098 
2022-03-28 12:48:35,244: ============================================================
2022-03-28 12:48:35,244: Epoch 29/45 Batch 2200/7662 eta: 15:14:38.388545	Training Loss 0.4291 (0.4337)	Training Prec@1 93.750 (91.908)	Training Prec@5 96.289 (94.955)	
2022-03-28 12:48:35,244: ============================================================
2022-03-28 12:49:18,215: time cost, forward:0.10846369999706772, backward:0.032644843785541895, data cost:0.2853389373288148 
2022-03-28 12:49:18,216: ============================================================
2022-03-28 12:49:18,216: Epoch 29/45 Batch 2300/7662 eta: 15:16:25.043321	Training Loss 0.4360 (0.4338)	Training Prec@1 91.992 (91.897)	Training Prec@5 95.117 (94.947)	
2022-03-28 12:49:18,217: ============================================================
2022-03-28 12:50:01,689: time cost, forward:0.10846256931506877, backward:0.032318980657045616, data cost:0.28597750838670494 
2022-03-28 12:50:01,689: ============================================================
2022-03-28 12:50:01,690: Epoch 29/45 Batch 2400/7662 eta: 15:26:22.673683	Training Loss 0.4240 (0.4338)	Training Prec@1 92.383 (91.894)	Training Prec@5 94.922 (94.946)	
2022-03-28 12:50:01,690: ============================================================
2022-03-28 12:50:44,540: time cost, forward:0.1084669375715374, backward:0.03225171112832951, data cost:0.2860985975734898 
2022-03-28 12:50:44,541: ============================================================
2022-03-28 12:50:44,541: Epoch 29/45 Batch 2500/7662 eta: 15:12:24.805331	Training Loss 0.4371 (0.4338)	Training Prec@1 91.406 (91.892)	Training Prec@5 94.727 (94.945)	
2022-03-28 12:50:44,541: ============================================================
2022-03-28 12:51:29,244: time cost, forward:0.10845958787507853, backward:0.03241089216880314, data cost:0.28672816690090486 
2022-03-28 12:51:29,245: ============================================================
2022-03-28 12:51:29,245: Epoch 29/45 Batch 2600/7662 eta: 15:51:06.688800	Training Loss 0.4300 (0.4338)	Training Prec@1 91.602 (91.895)	Training Prec@5 93.555 (94.947)	
2022-03-28 12:51:29,245: ============================================================
2022-03-28 12:52:10,557: time cost, forward:0.10847233065237333, backward:0.032379959724089885, data cost:0.2861960487217318 
2022-03-28 12:52:10,557: ============================================================
2022-03-28 12:52:10,557: Epoch 29/45 Batch 2700/7662 eta: 14:38:16.291665	Training Loss 0.4352 (0.4338)	Training Prec@1 91.211 (91.902)	Training Prec@5 95.117 (94.955)	
2022-03-28 12:52:10,558: ============================================================
2022-03-28 12:52:53,478: time cost, forward:0.10847390537050716, backward:0.032430792161506086, data cost:0.28620425732317206 
2022-03-28 12:52:53,479: ============================================================
2022-03-28 12:52:53,479: Epoch 29/45 Batch 2800/7662 eta: 15:11:45.559799	Training Loss 0.4266 (0.4338)	Training Prec@1 94.141 (91.896)	Training Prec@5 97.266 (94.951)	
2022-03-28 12:52:53,479: ============================================================
2022-03-28 12:53:35,152: time cost, forward:0.10848654060784847, backward:0.03251000115196225, data cost:0.28574735018745623 
2022-03-28 12:53:35,153: ============================================================
2022-03-28 12:53:35,153: Epoch 29/45 Batch 2900/7662 eta: 14:44:33.736298	Training Loss 0.4221 (0.4338)	Training Prec@1 93.945 (91.898)	Training Prec@5 96.875 (94.953)	
2022-03-28 12:53:35,153: ============================================================
2022-03-28 12:54:18,267: time cost, forward:0.10848879925447052, backward:0.03253608045676582, data cost:0.2858681470483651 
2022-03-28 12:54:18,268: ============================================================
2022-03-28 12:54:18,268: Epoch 29/45 Batch 3000/7662 eta: 15:14:26.160739	Training Loss 0.4240 (0.4339)	Training Prec@1 93.164 (91.888)	Training Prec@5 95.898 (94.945)	
2022-03-28 12:54:18,268: ============================================================
2022-03-28 12:55:01,271: time cost, forward:0.10849685190415606, backward:0.03253739239439421, data cost:0.2859351291853445 
2022-03-28 12:55:01,272: ============================================================
2022-03-28 12:55:01,272: Epoch 29/45 Batch 3100/7662 eta: 15:11:21.711578	Training Loss 0.4298 (0.4339)	Training Prec@1 93.750 (91.886)	Training Prec@5 96.289 (94.944)	
2022-03-28 12:55:01,272: ============================================================
2022-03-28 12:55:43,655: time cost, forward:0.10849573270124582, backward:0.03256031378018331, data cost:0.28580583755431155 
2022-03-28 12:55:43,655: ============================================================
2022-03-28 12:55:43,655: Epoch 29/45 Batch 3200/7662 eta: 14:57:29.971036	Training Loss 0.4345 (0.4339)	Training Prec@1 92.773 (91.898)	Training Prec@5 96.094 (94.950)	
2022-03-28 12:55:43,655: ============================================================
2022-03-28 12:56:27,227: time cost, forward:0.10849315905505941, backward:0.032538190831846094, data cost:0.2860907603480954 
2022-03-28 12:56:27,228: ============================================================
2022-03-28 12:56:27,228: Epoch 29/45 Batch 3300/7662 eta: 15:21:58.034593	Training Loss 0.4388 (0.4339)	Training Prec@1 91.797 (91.900)	Training Prec@5 94.922 (94.953)	
2022-03-28 12:56:27,228: ============================================================
2022-03-28 12:57:10,216: time cost, forward:0.10849399375859413, backward:0.03252532300474925, data cost:0.28618344723319056 
2022-03-28 12:57:10,216: ============================================================
2022-03-28 12:57:10,216: Epoch 29/45 Batch 3400/7662 eta: 15:08:52.804444	Training Loss 0.4328 (0.4339)	Training Prec@1 91.406 (91.896)	Training Prec@5 94.141 (94.950)	
2022-03-28 12:57:10,217: ============================================================
2022-03-28 12:57:53,538: time cost, forward:0.10850089282504352, backward:0.03253285344514414, data cost:0.28630147766610425 
2022-03-28 12:57:53,539: ============================================================
2022-03-28 12:57:53,539: Epoch 29/45 Batch 3500/7662 eta: 15:15:13.680558	Training Loss 0.4484 (0.4339)	Training Prec@1 91.211 (91.895)	Training Prec@5 94.141 (94.950)	
2022-03-28 12:57:53,539: ============================================================
2022-03-28 12:58:36,340: time cost, forward:0.10852112933045197, backward:0.03251889468100575, data cost:0.286312037555666 
2022-03-28 12:58:36,340: ============================================================
2022-03-28 12:58:36,340: Epoch 29/45 Batch 3600/7662 eta: 15:03:29.985671	Training Loss 0.4313 (0.4339)	Training Prec@1 91.797 (91.896)	Training Prec@5 94.531 (94.949)	
2022-03-28 12:58:36,341: ============================================================
2022-03-28 12:59:18,963: time cost, forward:0.10851705799427636, backward:0.032504981000476926, data cost:0.28629298408665055 
2022-03-28 12:59:18,964: ============================================================
2022-03-28 12:59:18,964: Epoch 29/45 Batch 3700/7662 eta: 14:59:01.989524	Training Loss 0.4432 (0.4339)	Training Prec@1 88.477 (91.900)	Training Prec@5 92.578 (94.952)	
2022-03-28 12:59:18,964: ============================================================
2022-03-28 13:00:02,075: time cost, forward:0.10851317313069511, backward:0.03248302545570078, data cost:0.2864143250333099 
2022-03-28 13:00:02,076: ============================================================
2022-03-28 13:00:02,076: Epoch 29/45 Batch 3800/7662 eta: 15:08:37.599167	Training Loss 0.4269 (0.4339)	Training Prec@1 92.383 (91.900)	Training Prec@5 95.117 (94.953)	
2022-03-28 13:00:02,076: ============================================================
2022-03-28 13:00:45,174: time cost, forward:0.10851248914567468, backward:0.03246337959600675, data cost:0.2865047799100873 
2022-03-28 13:00:45,175: ============================================================
2022-03-28 13:00:45,175: Epoch 29/45 Batch 3900/7662 eta: 15:07:37.574526	Training Loss 0.4366 (0.4340)	Training Prec@1 91.016 (91.897)	Training Prec@5 94.727 (94.951)	
2022-03-28 13:00:45,175: ============================================================
2022-03-28 13:01:28,090: time cost, forward:0.10851231227072515, backward:0.032456840745029705, data cost:0.28656281063216243 
2022-03-28 13:01:28,091: ============================================================
2022-03-28 13:01:28,091: Epoch 29/45 Batch 4000/7662 eta: 15:03:03.627568	Training Loss 0.4301 (0.4340)	Training Prec@1 92.773 (91.896)	Training Prec@5 96.289 (94.951)	
2022-03-28 13:01:28,091: ============================================================
2022-03-28 13:02:11,851: time cost, forward:0.10851173838280154, backward:0.03245251392207223, data cost:0.2867960696047066 
2022-03-28 13:02:11,852: ============================================================
2022-03-28 13:02:11,852: Epoch 29/45 Batch 4100/7662 eta: 15:20:06.752910	Training Loss 0.4433 (0.4340)	Training Prec@1 92.188 (91.897)	Training Prec@5 96.289 (94.952)	
2022-03-28 13:02:11,852: ============================================================
2022-03-28 13:02:55,859: time cost, forward:0.10851288517477059, backward:0.032459826320658185, data cost:0.2870742294668555 
2022-03-28 13:02:55,859: ============================================================
2022-03-28 13:02:55,860: Epoch 29/45 Batch 4200/7662 eta: 15:24:33.778163	Training Loss 0.4536 (0.4340)	Training Prec@1 89.258 (91.897)	Training Prec@5 93.359 (94.954)	
2022-03-28 13:02:55,860: ============================================================
2022-03-28 13:03:38,648: time cost, forward:0.1085233457644725, backward:0.032504659720591876, data cost:0.28700540492245696 
2022-03-28 13:03:38,648: ============================================================
2022-03-28 13:03:38,648: Epoch 29/45 Batch 4300/7662 eta: 14:58:14.665774	Training Loss 0.4356 (0.4340)	Training Prec@1 93.164 (91.897)	Training Prec@5 95.117 (94.956)	
2022-03-28 13:03:38,648: ============================================================
2022-03-28 13:04:20,431: time cost, forward:0.10852413893992534, backward:0.032588041541413246, data cost:0.28669353630575817 
2022-03-28 13:04:20,431: ============================================================
2022-03-28 13:04:20,431: Epoch 29/45 Batch 4400/7662 eta: 14:36:25.944530	Training Loss 0.4343 (0.4340)	Training Prec@1 92.773 (91.894)	Training Prec@5 95.312 (94.955)	
2022-03-28 13:04:20,432: ============================================================
2022-03-28 13:05:03,058: time cost, forward:0.10852995648016318, backward:0.032653796034247166, data cost:0.28656857345972786 
2022-03-28 13:05:03,058: ============================================================
2022-03-28 13:05:03,058: Epoch 29/45 Batch 4500/7662 eta: 14:53:25.461939	Training Loss 0.4348 (0.4340)	Training Prec@1 92.188 (91.893)	Training Prec@5 95.508 (94.953)	
2022-03-28 13:05:03,058: ============================================================
2022-03-28 13:05:46,059: time cost, forward:0.10853312274222011, backward:0.032719268205762556, data cost:0.28653344918292717 
2022-03-28 13:05:46,059: ============================================================
2022-03-28 13:05:46,059: Epoch 29/45 Batch 4600/7662 eta: 15:00:32.913050	Training Loss 0.4345 (0.4340)	Training Prec@1 91.797 (91.892)	Training Prec@5 94.141 (94.954)	
2022-03-28 13:05:46,059: ============================================================
2022-03-28 13:06:28,188: time cost, forward:0.10853652868861466, backward:0.03279920663445979, data cost:0.28632568815815423 
2022-03-28 13:06:28,189: ============================================================
2022-03-28 13:06:28,189: Epoch 29/45 Batch 4700/7662 eta: 14:41:35.884598	Training Loss 0.4435 (0.4340)	Training Prec@1 91.406 (91.890)	Training Prec@5 94.727 (94.955)	
2022-03-28 13:06:28,189: ============================================================
2022-03-28 13:07:09,535: time cost, forward:0.10854534880273863, backward:0.03289084450407161, data cost:0.28591235927503095 
2022-03-28 13:07:09,536: ============================================================
2022-03-28 13:07:09,536: Epoch 29/45 Batch 4800/7662 eta: 14:24:32.333049	Training Loss 0.4249 (0.4341)	Training Prec@1 92.383 (91.890)	Training Prec@5 94.336 (94.954)	
2022-03-28 13:07:09,537: ============================================================
2022-03-28 13:07:52,210: time cost, forward:0.10855468435417903, backward:0.03284715890738399, data cost:0.28593159860726597 
2022-03-28 13:07:52,210: ============================================================
2022-03-28 13:07:52,210: Epoch 29/45 Batch 4900/7662 eta: 14:51:33.927279	Training Loss 0.4379 (0.4341)	Training Prec@1 92.188 (91.891)	Training Prec@5 94.922 (94.955)	
2022-03-28 13:07:52,210: ============================================================
2022-03-28 13:08:34,949: time cost, forward:0.10877393765267336, backward:0.032839208537279355, data cost:0.28571626214319096 
2022-03-28 13:08:34,949: ============================================================
2022-03-28 13:08:34,950: Epoch 29/45 Batch 5000/7662 eta: 14:52:13.177721	Training Loss 0.4301 (0.4341)	Training Prec@1 92.773 (91.888)	Training Prec@5 95.312 (94.951)	
2022-03-28 13:08:34,950: ============================================================
2022-03-28 13:09:18,368: time cost, forward:0.10876896082595133, backward:0.03283588880182458, data cost:0.285847823323865 
2022-03-28 13:09:18,369: ============================================================
2022-03-28 13:09:18,369: Epoch 29/45 Batch 5100/7662 eta: 15:05:41.248963	Training Loss 0.4476 (0.4341)	Training Prec@1 91.016 (91.885)	Training Prec@5 95.508 (94.950)	
2022-03-28 13:09:18,369: ============================================================
2022-03-28 13:10:01,729: time cost, forward:0.10876220060004756, backward:0.0328857660706306, data cost:0.2859053666785626 
2022-03-28 13:10:01,730: ============================================================
2022-03-28 13:10:01,730: Epoch 29/45 Batch 5200/7662 eta: 15:03:45.397675	Training Loss 0.4425 (0.4341)	Training Prec@1 90.039 (91.885)	Training Prec@5 93.164 (94.948)	
2022-03-28 13:10:01,730: ============================================================
2022-03-28 13:10:44,446: time cost, forward:0.10875882164760228, backward:0.03292650536650373, data cost:0.28585084507343433 
2022-03-28 13:10:44,446: ============================================================
2022-03-28 13:10:44,446: Epoch 29/45 Batch 5300/7662 eta: 14:49:36.270653	Training Loss 0.4283 (0.4341)	Training Prec@1 93.164 (91.880)	Training Prec@5 95.117 (94.944)	
2022-03-28 13:10:44,447: ============================================================
2022-03-28 13:11:27,641: time cost, forward:0.10875567623279032, backward:0.03291671212944063, data cost:0.28595545053879495 
2022-03-28 13:11:27,642: ============================================================
2022-03-28 13:11:27,642: Epoch 29/45 Batch 5400/7662 eta: 14:58:51.619051	Training Loss 0.4359 (0.4342)	Training Prec@1 90.820 (91.877)	Training Prec@5 93.164 (94.941)	
2022-03-28 13:11:27,642: ============================================================
2022-03-28 13:12:10,706: time cost, forward:0.10875223445597508, backward:0.03290114239749745, data cost:0.2860226265233871 
2022-03-28 13:12:10,706: ============================================================
2022-03-28 13:12:10,706: Epoch 29/45 Batch 5500/7662 eta: 14:55:24.894012	Training Loss 0.4370 (0.4342)	Training Prec@1 91.211 (91.875)	Training Prec@5 94.531 (94.940)	
2022-03-28 13:12:10,706: ============================================================
2022-03-28 13:12:52,373: time cost, forward:0.1087471980626167, backward:0.03288028887029758, data cost:0.2858356173325062 
2022-03-28 13:12:52,374: ============================================================
2022-03-28 13:12:52,374: Epoch 29/45 Batch 5600/7662 eta: 14:25:40.747270	Training Loss 0.4477 (0.4342)	Training Prec@1 91.406 (91.873)	Training Prec@5 95.508 (94.940)	
2022-03-28 13:12:52,374: ============================================================
2022-03-28 13:13:35,923: time cost, forward:0.10874616666768053, backward:0.0328723189914032, data cost:0.28597941413681266 
2022-03-28 13:13:35,923: ============================================================
2022-03-28 13:13:35,924: Epoch 29/45 Batch 5700/7662 eta: 15:04:03.474968	Training Loss 0.4370 (0.4342)	Training Prec@1 91.992 (91.871)	Training Prec@5 95.703 (94.938)	
2022-03-28 13:13:35,924: ============================================================
2022-03-28 13:14:18,407: time cost, forward:0.108742802158309, backward:0.03286187606426041, data cost:0.2859481796146899 
2022-03-28 13:14:18,408: ============================================================
2022-03-28 13:14:18,408: Epoch 29/45 Batch 5800/7662 eta: 14:41:14.006351	Training Loss 0.4259 (0.4342)	Training Prec@1 92.773 (91.868)	Training Prec@5 96.094 (94.936)	
2022-03-28 13:14:18,408: ============================================================
2022-03-28 13:15:01,012: time cost, forward:0.10875737693354647, backward:0.032845651806360746, data cost:0.28591699098243334 
2022-03-28 13:15:01,012: ============================================================
2022-03-28 13:15:01,013: Epoch 29/45 Batch 5900/7662 eta: 14:43:01.024689	Training Loss 0.4501 (0.4342)	Training Prec@1 90.039 (91.867)	Training Prec@5 94.141 (94.934)	
2022-03-28 13:15:01,013: ============================================================
2022-03-28 13:15:43,463: time cost, forward:0.1088441126464625, backward:0.03283728562030897, data cost:0.2857736411303714 
2022-03-28 13:15:43,464: ============================================================
2022-03-28 13:15:43,464: Epoch 29/45 Batch 6000/7662 eta: 14:39:07.790414	Training Loss 0.4332 (0.4342)	Training Prec@1 93.359 (91.866)	Training Prec@5 96.094 (94.933)	
2022-03-28 13:15:43,464: ============================================================
2022-03-28 13:16:26,252: time cost, forward:0.10897283464948708, backward:0.032795766041423084, data cost:0.2857075005325065 
2022-03-28 13:16:26,252: ============================================================
2022-03-28 13:16:26,253: Epoch 29/45 Batch 6100/7662 eta: 14:45:24.391720	Training Loss 0.4162 (0.4343)	Training Prec@1 93.945 (91.865)	Training Prec@5 96.680 (94.933)	
2022-03-28 13:16:26,253: ============================================================
2022-03-28 13:17:09,457: time cost, forward:0.10912016165373806, backward:0.03281391226566498, data cost:0.2855575881594322 
2022-03-28 13:17:09,457: ============================================================
2022-03-28 13:17:09,457: Epoch 29/45 Batch 6200/7662 eta: 14:53:17.389453	Training Loss 0.4242 (0.4343)	Training Prec@1 93.945 (91.866)	Training Prec@5 96.289 (94.933)	
2022-03-28 13:17:09,457: ============================================================
2022-03-28 13:17:53,350: time cost, forward:0.10911332783121441, backward:0.0328234565657346, data cost:0.28573209937744093 
2022-03-28 13:17:53,351: ============================================================
2022-03-28 13:17:53,351: Epoch 29/45 Batch 6300/7662 eta: 15:06:48.582571	Training Loss 0.4423 (0.4343)	Training Prec@1 91.992 (91.865)	Training Prec@5 95.898 (94.933)	
2022-03-28 13:17:53,351: ============================================================
2022-03-28 13:18:36,460: time cost, forward:0.10944638909502204, backward:0.032843728981310175, data cost:0.28546042400592003 
2022-03-28 13:18:36,460: ============================================================
2022-03-28 13:18:36,460: Epoch 29/45 Batch 6400/7662 eta: 14:49:52.891393	Training Loss 0.4275 (0.4343)	Training Prec@1 92.188 (91.868)	Training Prec@5 94.531 (94.933)	
2022-03-28 13:18:36,460: ============================================================
2022-03-28 13:19:19,570: time cost, forward:0.10971949430296359, backward:0.032884708164765075, data cost:0.28518522249806716 
2022-03-28 13:19:19,571: ============================================================
2022-03-28 13:19:19,571: Epoch 29/45 Batch 6500/7662 eta: 14:49:12.035829	Training Loss 0.4277 (0.4343)	Training Prec@1 91.992 (91.867)	Training Prec@5 94.531 (94.931)	
2022-03-28 13:19:19,572: ============================================================
2022-03-28 13:20:02,823: time cost, forward:0.10970423163419638, backward:0.03293684938311992, data cost:0.285193809073411 
2022-03-28 13:20:02,823: ============================================================
2022-03-28 13:20:02,823: Epoch 29/45 Batch 6600/7662 eta: 14:51:23.317910	Training Loss 0.4386 (0.4343)	Training Prec@1 91.211 (91.864)	Training Prec@5 95.312 (94.929)	
2022-03-28 13:20:02,824: ============================================================
2022-03-28 13:20:46,157: time cost, forward:0.10968763943233852, backward:0.032979663867953285, data cost:0.2852653609334328 
2022-03-28 13:20:46,157: ============================================================
2022-03-28 13:20:46,157: Epoch 29/45 Batch 6700/7662 eta: 14:52:21.522382	Training Loss 0.4355 (0.4343)	Training Prec@1 90.820 (91.863)	Training Prec@5 93.945 (94.929)	
2022-03-28 13:20:46,158: ============================================================
2022-03-28 13:21:29,346: time cost, forward:0.10966723304335589, backward:0.03297993866445949, data cost:0.28534588012156686 
2022-03-28 13:21:29,347: ============================================================
2022-03-28 13:21:29,347: Epoch 29/45 Batch 6800/7662 eta: 14:48:39.434865	Training Loss 0.4356 (0.4343)	Training Prec@1 91.797 (91.863)	Training Prec@5 94.727 (94.928)	
2022-03-28 13:21:29,347: ============================================================
2022-03-28 13:22:13,408: time cost, forward:0.10965268562489411, backward:0.032966827997212345, data cost:0.2855463751261952 
2022-03-28 13:22:13,409: ============================================================
2022-03-28 13:22:13,409: Epoch 29/45 Batch 6900/7662 eta: 15:05:52.893368	Training Loss 0.4415 (0.4343)	Training Prec@1 91.406 (91.864)	Training Prec@5 94.531 (94.929)	
2022-03-28 13:22:13,409: ============================================================
2022-03-28 13:22:56,979: time cost, forward:0.10963928312859479, backward:0.03296670533534646, data cost:0.28565975992590004 
2022-03-28 13:22:56,980: ============================================================
2022-03-28 13:22:56,980: Epoch 29/45 Batch 7000/7662 eta: 14:55:03.352715	Training Loss 0.4343 (0.4344)	Training Prec@1 90.234 (91.861)	Training Prec@5 93.555 (94.929)	
2022-03-28 13:22:56,980: ============================================================
2022-03-28 13:23:40,240: time cost, forward:0.10963127733637101, backward:0.03296886082853561, data cost:0.28573029873186073 
2022-03-28 13:23:40,240: ============================================================
2022-03-28 13:23:40,241: Epoch 29/45 Batch 7100/7662 eta: 14:47:57.677779	Training Loss 0.4411 (0.4344)	Training Prec@1 92.773 (91.859)	Training Prec@5 96.289 (94.929)	
2022-03-28 13:23:40,241: ============================================================
2022-03-28 13:24:24,494: time cost, forward:0.10995747602918211, backward:0.032985084271129596, data cost:0.285571292996953 
2022-03-28 13:24:24,495: ============================================================
2022-03-28 13:24:24,495: Epoch 29/45 Batch 7200/7662 eta: 15:07:36.935081	Training Loss 0.4302 (0.4344)	Training Prec@1 92.188 (91.855)	Training Prec@5 94.922 (94.926)	
2022-03-28 13:24:24,495: ============================================================
2022-03-28 13:25:07,473: time cost, forward:0.11009875962727363, backward:0.03298873414012043, data cost:0.28543730715657445 
2022-03-28 13:25:07,473: ============================================================
2022-03-28 13:25:07,474: Epoch 29/45 Batch 7300/7662 eta: 14:40:44.564557	Training Loss 0.4351 (0.4344)	Training Prec@1 94.141 (91.853)	Training Prec@5 96.094 (94.926)	
2022-03-28 13:25:07,474: ============================================================
2022-03-28 13:25:51,163: time cost, forward:0.11008014297433795, backward:0.03298862032961855, data cost:0.2855652490327899 
2022-03-28 13:25:51,163: ============================================================
2022-03-28 13:25:51,163: Epoch 29/45 Batch 7400/7662 eta: 14:54:35.103234	Training Loss 0.4335 (0.4344)	Training Prec@1 94.531 (91.853)	Training Prec@5 97.266 (94.926)	
2022-03-28 13:25:51,164: ============================================================
2022-03-28 13:26:34,545: time cost, forward:0.11046741364526501, backward:0.03299302240263098, data cost:0.28524171757624933 
2022-03-28 13:26:34,546: ============================================================
2022-03-28 13:26:34,547: Epoch 29/45 Batch 7500/7662 eta: 14:47:34.637628	Training Loss 0.4326 (0.4344)	Training Prec@1 89.648 (91.852)	Training Prec@5 93.555 (94.926)	
2022-03-28 13:26:34,547: ============================================================
2022-03-28 13:27:18,933: time cost, forward:0.11051597634747336, backward:0.03298136654269116, data cost:0.28539578060301246 
2022-03-28 13:27:18,934: ============================================================
2022-03-28 13:27:18,934: Epoch 29/45 Batch 7600/7662 eta: 15:07:23.556320	Training Loss 0.4314 (0.4344)	Training Prec@1 91.797 (91.853)	Training Prec@5 94.727 (94.927)	
2022-03-28 13:27:18,934: ============================================================
2022-03-28 13:27:46,378: Epoch: 29/45 eta: 15:06:55.592160	Training Loss 0.4321 (0.4345)	Training Prec@1 91.016 (91.854)	Training Prec@5 94.531 (94.927)
2022-03-28 13:27:46,378: ============================================================
2022-03-28 13:28:30,877: time cost, forward:0.10866207305831138, backward:0.03289824302750404, data cost:0.3037720208216195 
2022-03-28 13:28:30,878: ============================================================
2022-03-28 13:28:30,878: Epoch 30/45 Batch 100/7662 eta: 15:06:09.384539	Training Loss 0.4382 (0.4336)	Training Prec@1 91.016 (91.712)	Training Prec@5 94.727 (94.965)	
2022-03-28 13:28:30,879: ============================================================
2022-03-28 13:29:12,873: time cost, forward:0.11414175656572659, backward:0.032783951591606714, data cost:0.2854971693987822 
2022-03-28 13:29:12,873: ============================================================
2022-03-28 13:29:12,874: Epoch 30/45 Batch 200/7662 eta: 14:16:39.428696	Training Loss 0.4373 (0.4330)	Training Prec@1 92.383 (91.889)	Training Prec@5 94.922 (95.032)	
2022-03-28 13:29:12,874: ============================================================
2022-03-28 13:29:57,155: time cost, forward:0.11207548670944163, backward:0.032478787827252546, data cost:0.291217366189861 
2022-03-28 13:29:57,156: ============================================================
2022-03-28 13:29:57,156: Epoch 30/45 Batch 300/7662 eta: 15:02:34.041754	Training Loss 0.4379 (0.4327)	Training Prec@1 91.406 (91.935)	Training Prec@5 94.141 (95.102)	
2022-03-28 13:29:57,156: ============================================================
2022-03-28 13:30:39,019: time cost, forward:0.11243519149627303, backward:0.032680941703624296, data cost:0.2862642767435327 
2022-03-28 13:30:39,019: ============================================================
2022-03-28 13:30:39,019: Epoch 30/45 Batch 400/7662 eta: 14:12:34.186001	Training Loss 0.4320 (0.4330)	Training Prec@1 91.602 (91.926)	Training Prec@5 94.336 (95.072)	
2022-03-28 13:30:39,019: ============================================================
2022-03-28 13:31:21,232: time cost, forward:0.11195662313090536, backward:0.03272308089690122, data cost:0.284783225737975 
2022-03-28 13:31:21,233: ============================================================
2022-03-28 13:31:21,233: Epoch 30/45 Batch 500/7662 eta: 14:18:59.820277	Training Loss 0.4282 (0.4329)	Training Prec@1 91.406 (91.917)	Training Prec@5 96.094 (95.058)	
2022-03-28 13:31:21,233: ============================================================
2022-03-28 13:32:04,066: time cost, forward:0.11246592293995648, backward:0.032800186456543375, data cost:0.28399914652357916 
2022-03-28 13:32:04,066: ============================================================
2022-03-28 13:32:04,066: Epoch 30/45 Batch 600/7662 eta: 14:30:53.674635	Training Loss 0.4338 (0.4326)	Training Prec@1 92.188 (91.981)	Training Prec@5 94.141 (95.089)	
2022-03-28 13:32:04,066: ============================================================
2022-03-28 13:32:46,864: time cost, forward:0.11201154554691778, backward:0.0327346106626104, data cost:0.2842408260050761 
2022-03-28 13:32:46,864: ============================================================
2022-03-28 13:32:46,865: Epoch 30/45 Batch 700/7662 eta: 14:29:28.215550	Training Loss 0.4428 (0.4327)	Training Prec@1 92.578 (91.972)	Training Prec@5 95.117 (95.064)	
2022-03-28 13:32:46,865: ============================================================
2022-03-28 13:33:30,806: time cost, forward:0.11167809124732943, backward:0.033255389396180494, data cost:0.2852469481276034 
2022-03-28 13:33:30,807: ============================================================
2022-03-28 13:33:30,807: Epoch 30/45 Batch 800/7662 eta: 14:51:58.566846	Training Loss 0.4327 (0.4327)	Training Prec@1 90.625 (91.970)	Training Prec@5 93.750 (95.057)	
2022-03-28 13:33:30,807: ============================================================
2022-03-28 13:34:15,560: time cost, forward:0.11132900017387212, backward:0.03365244457003007, data cost:0.2871472769239721 
2022-03-28 13:34:15,561: ============================================================
2022-03-28 13:34:15,561: Epoch 30/45 Batch 900/7662 eta: 15:07:42.575681	Training Loss 0.4392 (0.4325)	Training Prec@1 92.969 (91.987)	Training Prec@5 94.922 (95.069)	
2022-03-28 13:34:15,561: ============================================================
2022-03-28 13:34:59,005: time cost, forward:0.11103556559489176, backward:0.03381961244004625, data cost:0.28741473430866477 
2022-03-28 13:34:59,006: ============================================================
2022-03-28 13:34:59,006: Epoch 30/45 Batch 1000/7662 eta: 14:40:26.116746	Training Loss 0.4301 (0.4324)	Training Prec@1 91.797 (91.985)	Training Prec@5 94.922 (95.069)	
2022-03-28 13:34:59,006: ============================================================
2022-03-28 13:35:41,756: time cost, forward:0.11080884694836159, backward:0.03369206813815727, data cost:0.28729983825267064 
2022-03-28 13:35:41,756: ============================================================
2022-03-28 13:35:41,756: Epoch 30/45 Batch 1100/7662 eta: 14:25:38.296894	Training Loss 0.4347 (0.4323)	Training Prec@1 90.820 (91.996)	Training Prec@5 93.945 (95.069)	
2022-03-28 13:35:41,756: ============================================================
2022-03-28 13:36:25,641: time cost, forward:0.11059989424125664, backward:0.03355564963728115, data cost:0.2883028097208387 
2022-03-28 13:36:25,641: ============================================================
2022-03-28 13:36:25,641: Epoch 30/45 Batch 1200/7662 eta: 14:47:53.460435	Training Loss 0.4339 (0.4324)	Training Prec@1 93.555 (91.990)	Training Prec@5 95.508 (95.059)	
2022-03-28 13:36:25,641: ============================================================
2022-03-28 13:37:09,085: time cost, forward:0.1121290293540103, backward:0.03357793901221765, data cost:0.28683030669555193 
2022-03-28 13:37:09,085: ============================================================
2022-03-28 13:37:09,085: Epoch 30/45 Batch 1300/7662 eta: 14:38:14.771529	Training Loss 0.4267 (0.4324)	Training Prec@1 91.406 (91.984)	Training Prec@5 94.531 (95.056)	
2022-03-28 13:37:09,086: ============================================================
2022-03-28 13:37:51,194: time cost, forward:0.11182765026787846, backward:0.03352915448235818, data cost:0.2863484160059942 
2022-03-28 13:37:51,194: ============================================================
2022-03-28 13:37:51,194: Epoch 30/45 Batch 1400/7662 eta: 14:10:33.041863	Training Loss 0.4242 (0.4323)	Training Prec@1 93.555 (92.006)	Training Prec@5 96.680 (95.065)	
2022-03-28 13:37:51,194: ============================================================
2022-03-28 13:38:34,700: time cost, forward:0.1115995218469112, backward:0.03348401564610171, data cost:0.2868084319040249 
2022-03-28 13:38:34,700: ============================================================
2022-03-28 13:38:34,701: Epoch 30/45 Batch 1500/7662 eta: 14:38:03.201041	Training Loss 0.4387 (0.4322)	Training Prec@1 90.625 (92.018)	Training Prec@5 93.945 (95.066)	
2022-03-28 13:38:34,701: ============================================================
2022-03-28 13:39:18,645: time cost, forward:0.11138981875216238, backward:0.03347271140327, data cost:0.2874242290248716 
2022-03-28 13:39:18,646: ============================================================
2022-03-28 13:39:18,646: Epoch 30/45 Batch 1600/7662 eta: 14:46:10.974652	Training Loss 0.4322 (0.4322)	Training Prec@1 92.383 (92.027)	Training Prec@5 94.141 (95.065)	
2022-03-28 13:39:18,646: ============================================================
2022-03-28 13:40:02,382: time cost, forward:0.11119553157341627, backward:0.033447470505003234, data cost:0.28798880473244115 
2022-03-28 13:40:02,382: ============================================================
2022-03-28 13:40:02,383: Epoch 30/45 Batch 1700/7662 eta: 14:41:14.399545	Training Loss 0.4323 (0.4322)	Training Prec@1 90.039 (92.035)	Training Prec@5 93.164 (95.069)	
2022-03-28 13:40:02,383: ============================================================
2022-03-28 13:40:43,957: time cost, forward:0.11105628739866434, backward:0.03346523806543864, data cost:0.2871576370697276 
2022-03-28 13:40:43,958: ============================================================
2022-03-28 13:40:43,958: Epoch 30/45 Batch 1800/7662 eta: 13:56:59.946697	Training Loss 0.4324 (0.4322)	Training Prec@1 92.188 (92.039)	Training Prec@5 94.922 (95.062)	
2022-03-28 13:40:43,958: ============================================================
2022-03-28 13:41:26,177: time cost, forward:0.11090086836008853, backward:0.03345717424590566, data cost:0.28678339126550756 
2022-03-28 13:41:26,177: ============================================================
2022-03-28 13:41:26,177: Epoch 30/45 Batch 1900/7662 eta: 14:09:15.893448	Training Loss 0.4393 (0.4322)	Training Prec@1 91.602 (92.037)	Training Prec@5 94.141 (95.059)	
2022-03-28 13:41:26,178: ============================================================
2022-03-28 13:42:07,819: time cost, forward:0.1107615978494771, backward:0.033448418001343815, data cost:0.2862203075863112 
2022-03-28 13:42:07,819: ============================================================
2022-03-28 13:42:07,819: Epoch 30/45 Batch 2000/7662 eta: 13:56:57.237531	Training Loss 0.4238 (0.4322)	Training Prec@1 93.555 (92.042)	Training Prec@5 95.117 (95.062)	
2022-03-28 13:42:07,819: ============================================================
2022-03-28 13:42:50,607: time cost, forward:0.11076088471887452, backward:0.03346213458435146, data cost:0.28604921640357045 
2022-03-28 13:42:50,608: ============================================================
2022-03-28 13:42:50,608: Epoch 30/45 Batch 2100/7662 eta: 14:19:17.347910	Training Loss 0.4334 (0.4322)	Training Prec@1 91.406 (92.048)	Training Prec@5 95.508 (95.066)	
2022-03-28 13:42:50,608: ============================================================
2022-03-28 13:43:34,791: time cost, forward:0.11202995199244258, backward:0.0336319674899113, data cost:0.2850968644748009 
2022-03-28 13:43:34,791: ============================================================
2022-03-28 13:43:34,791: Epoch 30/45 Batch 2200/7662 eta: 14:46:33.795004	Training Loss 0.4288 (0.4322)	Training Prec@1 92.188 (92.040)	Training Prec@5 95.312 (95.058)	
2022-03-28 13:43:34,792: ============================================================
2022-03-28 13:44:16,381: time cost, forward:0.11197281754913928, backward:0.03355777745871193, data cost:0.2844720863684098 
2022-03-28 13:44:16,381: ============================================================
2022-03-28 13:44:16,381: Epoch 30/45 Batch 2300/7662 eta: 13:53:49.626862	Training Loss 0.4182 (0.4322)	Training Prec@1 95.312 (92.033)	Training Prec@5 97.656 (95.052)	
2022-03-28 13:44:16,381: ============================================================
2022-03-28 13:44:59,166: time cost, forward:0.11173508712876683, backward:0.03351145110661013, data cost:0.2847486431770595 
2022-03-28 13:44:59,167: ============================================================
2022-03-28 13:44:59,167: Epoch 30/45 Batch 2400/7662 eta: 14:17:05.687875	Training Loss 0.4368 (0.4322)	Training Prec@1 89.648 (92.030)	Training Prec@5 94.141 (95.049)	
2022-03-28 13:44:59,167: ============================================================
2022-03-28 13:45:42,339: time cost, forward:0.11151087641859111, backward:0.033461950263198544, data cost:0.285057662629566 
2022-03-28 13:45:42,339: ============================================================
2022-03-28 13:45:42,339: Epoch 30/45 Batch 2500/7662 eta: 14:24:06.743433	Training Loss 0.4279 (0.4322)	Training Prec@1 92.773 (92.026)	Training Prec@5 95.508 (95.051)	
2022-03-28 13:45:42,340: ============================================================
2022-03-28 13:46:26,722: time cost, forward:0.11133808867662583, backward:0.03341001746928798, data cost:0.2858394715087879 
2022-03-28 13:46:26,723: ============================================================
2022-03-28 13:46:26,723: Epoch 30/45 Batch 2600/7662 eta: 14:47:37.038059	Training Loss 0.4152 (0.4322)	Training Prec@1 93.750 (92.027)	Training Prec@5 95.703 (95.047)	
2022-03-28 13:46:26,723: ============================================================
2022-03-28 13:47:09,268: time cost, forward:0.1112101350991361, backward:0.033376549561406205, data cost:0.285784111061817 
2022-03-28 13:47:09,268: ============================================================
2022-03-28 13:47:09,268: Epoch 30/45 Batch 2700/7662 eta: 14:10:09.361639	Training Loss 0.4388 (0.4322)	Training Prec@1 90.625 (92.032)	Training Prec@5 92.969 (95.052)	
2022-03-28 13:47:09,269: ============================================================
2022-03-28 13:47:52,819: time cost, forward:0.11109156410964142, backward:0.033333865094500044, data cost:0.2861245419562566 
2022-03-28 13:47:52,820: ============================================================
2022-03-28 13:47:52,820: Epoch 30/45 Batch 2800/7662 eta: 14:29:31.731034	Training Loss 0.4131 (0.4322)	Training Prec@1 94.336 (92.033)	Training Prec@5 97.070 (95.053)	
2022-03-28 13:47:52,820: ============================================================
2022-03-28 13:48:37,443: time cost, forward:0.1109979146922527, backward:0.033291203682074756, data cost:0.2867830916493874 
2022-03-28 13:48:37,444: ============================================================
2022-03-28 13:48:37,444: Epoch 30/45 Batch 2900/7662 eta: 14:50:11.596086	Training Loss 0.4259 (0.4321)	Training Prec@1 92.969 (92.038)	Training Prec@5 95.703 (95.058)	
2022-03-28 13:48:37,444: ============================================================
2022-03-28 13:49:21,259: time cost, forward:0.1108966030332, backward:0.03326519738439323, data cost:0.28713906196881706 
2022-03-28 13:49:21,260: ============================================================
2022-03-28 13:49:21,260: Epoch 30/45 Batch 3000/7662 eta: 14:33:21.015990	Training Loss 0.4287 (0.4322)	Training Prec@1 92.383 (92.040)	Training Prec@5 96.484 (95.061)	
2022-03-28 13:49:21,260: ============================================================
2022-03-28 13:50:04,128: time cost, forward:0.11080518819778802, backward:0.03320383110058696, data cost:0.28719104809621027 
2022-03-28 13:50:04,128: ============================================================
2022-03-28 13:50:04,128: Epoch 30/45 Batch 3100/7662 eta: 14:13:44.480069	Training Loss 0.4383 (0.4322)	Training Prec@1 90.039 (92.046)	Training Prec@5 92.969 (95.067)	
2022-03-28 13:50:04,128: ============================================================
2022-03-28 13:50:47,366: time cost, forward:0.11073326386299384, backward:0.03318204325264266, data cost:0.2873234276325861 
2022-03-28 13:50:47,366: ============================================================
2022-03-28 13:50:47,367: Epoch 30/45 Batch 3200/7662 eta: 14:20:23.642817	Training Loss 0.4274 (0.4322)	Training Prec@1 93.750 (92.046)	Training Prec@5 96.289 (95.069)	
2022-03-28 13:50:47,367: ============================================================
2022-03-28 13:51:30,809: time cost, forward:0.11065127951479493, backward:0.033134364691529354, data cost:0.28752045849663954 
2022-03-28 13:51:30,809: ============================================================
2022-03-28 13:51:30,809: Epoch 30/45 Batch 3300/7662 eta: 14:23:43.943574	Training Loss 0.4387 (0.4322)	Training Prec@1 88.672 (92.044)	Training Prec@5 92.188 (95.067)	
2022-03-28 13:51:30,809: ============================================================
2022-03-28 13:52:14,035: time cost, forward:0.11058797335477392, backward:0.03313373130502053, data cost:0.28761829583565607 
2022-03-28 13:52:14,035: ============================================================
2022-03-28 13:52:14,035: Epoch 30/45 Batch 3400/7662 eta: 14:18:42.502101	Training Loss 0.4312 (0.4322)	Training Prec@1 91.797 (92.043)	Training Prec@5 95.703 (95.065)	
2022-03-28 13:52:14,036: ============================================================
2022-03-28 13:52:58,200: time cost, forward:0.1105273261210618, backward:0.033133663699162895, data cost:0.2879651651003593 
2022-03-28 13:52:58,200: ============================================================
2022-03-28 13:52:58,201: Epoch 30/45 Batch 3500/7662 eta: 14:36:37.947558	Training Loss 0.4439 (0.4323)	Training Prec@1 91.992 (92.032)	Training Prec@5 94.727 (95.059)	
2022-03-28 13:52:58,201: ============================================================
2022-03-28 13:53:40,162: time cost, forward:0.11047924038568249, backward:0.03310867335008694, data cost:0.2876964050652286 
2022-03-28 13:53:40,163: ============================================================
2022-03-28 13:53:40,163: Epoch 30/45 Batch 3600/7662 eta: 13:52:12.098999	Training Loss 0.4314 (0.4322)	Training Prec@1 91.797 (92.032)	Training Prec@5 94.727 (95.059)	
2022-03-28 13:53:40,163: ============================================================
2022-03-28 13:54:23,056: time cost, forward:0.11042044181442158, backward:0.03299417164429486, data cost:0.2877902787387226 
2022-03-28 13:54:23,056: ============================================================
2022-03-28 13:54:23,056: Epoch 30/45 Batch 3700/7662 eta: 14:09:57.084181	Training Loss 0.4348 (0.4322)	Training Prec@1 91.602 (92.032)	Training Prec@5 94.727 (95.058)	
2022-03-28 13:54:23,056: ============================================================
2022-03-28 13:55:05,699: time cost, forward:0.11035953919866832, backward:0.03293144109845946, data cost:0.2877838495500027 
2022-03-28 13:55:05,700: ============================================================
2022-03-28 13:55:05,700: Epoch 30/45 Batch 3800/7662 eta: 14:04:17.658181	Training Loss 0.4223 (0.4322)	Training Prec@1 93.750 (92.027)	Training Prec@5 95.312 (95.054)	
2022-03-28 13:55:05,700: ============================================================
2022-03-28 13:55:50,323: time cost, forward:0.11030987067906237, backward:0.032857118065034954, data cost:0.2882854976419242 
2022-03-28 13:55:50,324: ============================================================
2022-03-28 13:55:50,324: Epoch 30/45 Batch 3900/7662 eta: 14:42:45.552804	Training Loss 0.4391 (0.4322)	Training Prec@1 89.258 (92.026)	Training Prec@5 92.773 (95.051)	
2022-03-28 13:55:50,324: ============================================================
2022-03-28 13:56:33,899: time cost, forward:0.11026436151579398, backward:0.03289223945447879, data cost:0.28834898175523116 
2022-03-28 13:56:33,900: ============================================================
2022-03-28 13:56:33,900: Epoch 30/45 Batch 4000/7662 eta: 14:21:18.279333	Training Loss 0.4280 (0.4322)	Training Prec@1 94.922 (92.026)	Training Prec@5 97.656 (95.050)	
2022-03-28 13:56:33,900: ============================================================
2022-03-28 13:57:16,053: time cost, forward:0.1102215674889614, backward:0.03292262888617794, data cost:0.2881467898434678 
2022-03-28 13:57:16,053: ============================================================
2022-03-28 13:57:16,054: Epoch 30/45 Batch 4100/7662 eta: 13:52:29.035275	Training Loss 0.4356 (0.4323)	Training Prec@1 90.625 (92.028)	Training Prec@5 93.555 (95.049)	
2022-03-28 13:57:16,054: ============================================================
2022-03-28 13:57:59,559: time cost, forward:0.11016633187512267, backward:0.03295030052192326, data cost:0.28826760428097736 
2022-03-28 13:57:59,560: ============================================================
2022-03-28 13:57:59,560: Epoch 30/45 Batch 4200/7662 eta: 14:18:28.626841	Training Loss 0.4401 (0.4322)	Training Prec@1 91.211 (92.030)	Training Prec@5 94.141 (95.048)	
2022-03-28 13:57:59,560: ============================================================
2022-03-28 13:58:41,874: time cost, forward:0.110103003283827, backward:0.032927544678774345, data cost:0.288157002264246 
2022-03-28 13:58:41,875: ============================================================
2022-03-28 13:58:41,875: Epoch 30/45 Batch 4300/7662 eta: 13:54:15.447239	Training Loss 0.4274 (0.4322)	Training Prec@1 91.602 (92.032)	Training Prec@5 94.141 (95.048)	
2022-03-28 13:58:41,875: ============================================================
2022-03-28 13:59:25,728: time cost, forward:0.11005580666445147, backward:0.033006270871050764, data cost:0.28827598691664114 
2022-03-28 13:59:25,728: ============================================================
2022-03-28 13:59:25,728: Epoch 30/45 Batch 4400/7662 eta: 14:23:51.668778	Training Loss 0.4307 (0.4322)	Training Prec@1 90.039 (92.033)	Training Prec@5 94.336 (95.049)	
2022-03-28 13:59:25,729: ============================================================
2022-03-28 14:00:08,689: time cost, forward:0.11001635133014517, backward:0.033070197076791125, data cost:0.2882026290066853 
2022-03-28 14:00:08,689: ============================================================
2022-03-28 14:00:08,689: Epoch 30/45 Batch 4500/7662 eta: 14:05:33.858233	Training Loss 0.4293 (0.4322)	Training Prec@1 92.969 (92.031)	Training Prec@5 94.727 (95.048)	
2022-03-28 14:00:08,689: ============================================================
2022-03-28 14:00:52,282: time cost, forward:0.10997874064402986, backward:0.033105993872234216, data cost:0.28830465551925033 
2022-03-28 14:00:52,282: ============================================================
2022-03-28 14:00:52,282: Epoch 30/45 Batch 4600/7662 eta: 14:17:16.795595	Training Loss 0.4281 (0.4322)	Training Prec@1 90.625 (92.033)	Training Prec@5 94.336 (95.050)	
2022-03-28 14:00:52,283: ============================================================
2022-03-28 14:01:35,535: time cost, forward:0.10995099158407805, backward:0.03308655105009564, data cost:0.28835786426541754 
2022-03-28 14:01:35,535: ============================================================
2022-03-28 14:01:35,535: Epoch 30/45 Batch 4700/7662 eta: 14:09:52.132548	Training Loss 0.4268 (0.4322)	Training Prec@1 91.016 (92.033)	Training Prec@5 95.117 (95.050)	
2022-03-28 14:01:35,535: ============================================================
2022-03-28 14:02:20,202: time cost, forward:0.10992326943122688, backward:0.03305328212745191, data cost:0.28873795642284433 
2022-03-28 14:02:20,202: ============================================================
2022-03-28 14:02:20,202: Epoch 30/45 Batch 4800/7662 eta: 14:36:54.838370	Training Loss 0.4263 (0.4322)	Training Prec@1 92.773 (92.029)	Training Prec@5 95.117 (95.048)	
2022-03-28 14:02:20,203: ============================================================
2022-03-28 14:03:03,006: time cost, forward:0.10989682960081987, backward:0.033025782895248795, data cost:0.2887093239254941 
2022-03-28 14:03:03,007: ============================================================
2022-03-28 14:03:03,007: Epoch 30/45 Batch 4900/7662 eta: 13:59:38.040888	Training Loss 0.4209 (0.4322)	Training Prec@1 92.578 (92.032)	Training Prec@5 96.289 (95.050)	
2022-03-28 14:03:03,007: ============================================================
2022-03-28 14:03:45,949: time cost, forward:0.10986814848969854, backward:0.033009607903025914, data cost:0.2887032718795804 
2022-03-28 14:03:45,949: ============================================================
2022-03-28 14:03:45,949: Epoch 30/45 Batch 5000/7662 eta: 14:01:37.171737	Training Loss 0.4277 (0.4322)	Training Prec@1 93.359 (92.035)	Training Prec@5 96.875 (95.053)	
2022-03-28 14:03:45,950: ============================================================
2022-03-28 14:04:29,313: time cost, forward:0.10983074878005099, backward:0.032996805398271094, data cost:0.28878507344810184 
2022-03-28 14:04:29,314: ============================================================
2022-03-28 14:04:29,314: Epoch 30/45 Batch 5100/7662 eta: 14:09:10.452328	Training Loss 0.4156 (0.4322)	Training Prec@1 93.359 (92.037)	Training Prec@5 95.117 (95.056)	
2022-03-28 14:04:29,314: ============================================================
2022-03-28 14:05:14,157: time cost, forward:0.10980274063230684, backward:0.032971092131670265, data cost:0.28917132287925196 
2022-03-28 14:05:14,157: ============================================================
2022-03-28 14:05:14,158: Epoch 30/45 Batch 5200/7662 eta: 14:37:23.096250	Training Loss 0.4338 (0.4322)	Training Prec@1 91.797 (92.037)	Training Prec@5 93.750 (95.055)	
2022-03-28 14:05:14,158: ============================================================
2022-03-28 14:05:56,753: time cost, forward:0.1097793512961666, backward:0.03295796960001735, data cost:0.2890825236061875 
2022-03-28 14:05:56,754: ============================================================
2022-03-28 14:05:56,754: Epoch 30/45 Batch 5300/7662 eta: 13:52:42.365607	Training Loss 0.4347 (0.4322)	Training Prec@1 90.039 (92.039)	Training Prec@5 93.945 (95.055)	
2022-03-28 14:05:56,754: ============================================================
2022-03-28 14:06:41,038: time cost, forward:0.10975527317353058, backward:0.03294134038447186, data cost:0.28932059307985114 
2022-03-28 14:06:41,038: ============================================================
2022-03-28 14:06:41,038: Epoch 30/45 Batch 5400/7662 eta: 14:24:58.190891	Training Loss 0.4402 (0.4322)	Training Prec@1 91.602 (92.043)	Training Prec@5 94.727 (95.056)	
2022-03-28 14:06:41,038: ============================================================
2022-03-28 14:07:24,340: time cost, forward:0.10973343824902368, backward:0.0329213032702529, data cost:0.28937168978933725 
2022-03-28 14:07:24,341: ============================================================
2022-03-28 14:07:24,341: Epoch 30/45 Batch 5500/7662 eta: 14:05:04.282600	Training Loss 0.4304 (0.4322)	Training Prec@1 92.969 (92.043)	Training Prec@5 96.680 (95.058)	
2022-03-28 14:07:24,341: ============================================================
2022-03-28 14:08:08,571: time cost, forward:0.10971203551247283, backward:0.03290888172619086, data cost:0.2895777377513375 
2022-03-28 14:08:08,572: ============================================================
2022-03-28 14:08:08,572: Epoch 30/45 Batch 5600/7662 eta: 14:22:27.675568	Training Loss 0.4323 (0.4322)	Training Prec@1 92.383 (92.043)	Training Prec@5 94.531 (95.058)	
2022-03-28 14:08:08,572: ============================================================
2022-03-28 14:08:50,851: time cost, forward:0.1096943604525693, backward:0.03289649540509105, data cost:0.2894443444859293 
2022-03-28 14:08:50,851: ============================================================
2022-03-28 14:08:50,851: Epoch 30/45 Batch 5700/7662 eta: 13:43:41.430641	Training Loss 0.4312 (0.4322)	Training Prec@1 92.383 (92.045)	Training Prec@5 95.898 (95.058)	
2022-03-28 14:08:50,852: ============================================================
2022-03-28 14:09:34,514: time cost, forward:0.10967057638074595, backward:0.032880170198530345, data cost:0.2895512128209303 
2022-03-28 14:09:34,514: ============================================================
2022-03-28 14:09:34,514: Epoch 30/45 Batch 5800/7662 eta: 14:09:55.118316	Training Loss 0.4325 (0.4322)	Training Prec@1 92.578 (92.045)	Training Prec@5 95.508 (95.058)	
2022-03-28 14:09:34,514: ============================================================
2022-03-28 14:10:17,693: time cost, forward:0.1096549520736671, backward:0.032917260274904867, data cost:0.28952131155770316 
2022-03-28 14:10:17,694: ============================================================
2022-03-28 14:10:17,694: Epoch 30/45 Batch 5900/7662 eta: 13:59:47.866096	Training Loss 0.4273 (0.4322)	Training Prec@1 92.773 (92.043)	Training Prec@5 94.531 (95.057)	
2022-03-28 14:10:17,694: ============================================================
2022-03-28 14:11:00,337: time cost, forward:0.10963598623496729, backward:0.03290830276115355, data cost:0.28944143121849397 
2022-03-28 14:11:00,338: ============================================================
2022-03-28 14:11:00,338: Epoch 30/45 Batch 6000/7662 eta: 13:48:39.895076	Training Loss 0.4281 (0.4322)	Training Prec@1 93.945 (92.041)	Training Prec@5 96.875 (95.056)	
2022-03-28 14:11:00,338: ============================================================
2022-03-28 14:11:42,582: time cost, forward:0.10961457154148425, backward:0.03289199770620796, data cost:0.2893203286425679 
2022-03-28 14:11:42,582: ============================================================
2022-03-28 14:11:42,582: Epoch 30/45 Batch 6100/7662 eta: 13:40:11.800583	Training Loss 0.4263 (0.4322)	Training Prec@1 93.359 (92.041)	Training Prec@5 96.289 (95.056)	
2022-03-28 14:11:42,583: ============================================================
2022-03-28 14:12:24,263: time cost, forward:0.10959181706354222, backward:0.0328889435040295, data cost:0.2891085320931324 
2022-03-28 14:12:24,263: ============================================================
2022-03-28 14:12:24,263: Epoch 30/45 Batch 6200/7662 eta: 13:28:33.519069	Training Loss 0.4450 (0.4322)	Training Prec@1 92.578 (92.039)	Training Prec@5 94.922 (95.054)	
2022-03-28 14:12:24,263: ============================================================
2022-03-28 14:13:06,783: time cost, forward:0.10958061390101749, backward:0.03286029221728068, data cost:0.28904155675031434 
2022-03-28 14:13:06,783: ============================================================
2022-03-28 14:13:06,783: Epoch 30/45 Batch 6300/7662 eta: 13:44:07.847903	Training Loss 0.4283 (0.4322)	Training Prec@1 92.969 (92.042)	Training Prec@5 95.703 (95.056)	
2022-03-28 14:13:06,783: ============================================================
2022-03-28 14:13:50,608: time cost, forward:0.1097765657413302, backward:0.03286964965250403, data cost:0.2889381562718974 
2022-03-28 14:13:50,609: ============================================================
2022-03-28 14:13:50,609: Epoch 30/45 Batch 6400/7662 eta: 14:08:42.198134	Training Loss 0.4306 (0.4322)	Training Prec@1 92.578 (92.044)	Training Prec@5 95.117 (95.056)	
2022-03-28 14:13:50,609: ============================================================
2022-03-28 14:14:32,953: time cost, forward:0.1097525838962572, backward:0.03288560134481514, data cost:0.28881326252211975 
2022-03-28 14:14:32,953: ============================================================
2022-03-28 14:14:32,954: Epoch 30/45 Batch 6500/7662 eta: 13:39:19.333989	Training Loss 0.4291 (0.4322)	Training Prec@1 92.383 (92.044)	Training Prec@5 95.898 (95.056)	
2022-03-28 14:14:32,954: ============================================================
2022-03-28 14:15:16,999: time cost, forward:0.10972771682166967, backward:0.032899663563586846, data cost:0.28895553517908845 
2022-03-28 14:15:16,999: ============================================================
2022-03-28 14:15:16,999: Epoch 30/45 Batch 6600/7662 eta: 14:11:30.052503	Training Loss 0.4230 (0.4322)	Training Prec@1 92.578 (92.044)	Training Prec@5 95.703 (95.057)	
2022-03-28 14:15:17,000: ============================================================
2022-03-28 14:16:00,440: time cost, forward:0.1097078532634484, backward:0.03291083681243518, data cost:0.2889994213626996 
2022-03-28 14:16:00,441: ============================================================
2022-03-28 14:16:00,441: Epoch 30/45 Batch 6700/7662 eta: 13:59:05.379291	Training Loss 0.4174 (0.4322)	Training Prec@1 92.773 (92.043)	Training Prec@5 94.727 (95.056)	
2022-03-28 14:16:00,441: ============================================================
2022-03-28 14:16:43,547: time cost, forward:0.10968914486447018, backward:0.032922047379262694, data cost:0.2889921386060618 
2022-03-28 14:16:43,547: ============================================================
2022-03-28 14:16:43,547: Epoch 30/45 Batch 6800/7662 eta: 13:51:54.398060	Training Loss 0.4357 (0.4322)	Training Prec@1 91.406 (92.042)	Training Prec@5 95.117 (95.054)	
2022-03-28 14:16:43,547: ============================================================
2022-03-28 14:17:25,636: time cost, forward:0.10966884473006368, backward:0.032929447703230884, data cost:0.2888434836960889 
2022-03-28 14:17:25,637: ============================================================
2022-03-28 14:17:25,637: Epoch 30/45 Batch 6900/7662 eta: 13:31:35.032720	Training Loss 0.4256 (0.4322)	Training Prec@1 92.383 (92.042)	Training Prec@5 95.703 (95.054)	
2022-03-28 14:17:25,637: ============================================================
2022-03-28 14:18:09,354: time cost, forward:0.10965268304304049, backward:0.03291912306409373, data cost:0.2889494108360041 
2022-03-28 14:18:09,354: ============================================================
2022-03-28 14:18:09,354: Epoch 30/45 Batch 7000/7662 eta: 14:02:14.056941	Training Loss 0.4294 (0.4322)	Training Prec@1 92.383 (92.041)	Training Prec@5 94.922 (95.053)	
2022-03-28 14:18:09,355: ============================================================
2022-03-28 14:18:51,216: time cost, forward:0.10962966251144915, backward:0.032898296485704207, data cost:0.28880878830815826 
2022-03-28 14:18:51,216: ============================================================
2022-03-28 14:18:51,217: Epoch 30/45 Batch 7100/7662 eta: 13:25:47.822832	Training Loss 0.4328 (0.4322)	Training Prec@1 91.406 (92.040)	Training Prec@5 94.727 (95.051)	
2022-03-28 14:18:51,217: ============================================================
2022-03-28 14:19:33,833: time cost, forward:0.10996529588700665, backward:0.03289210804635774, data cost:0.28840055997578 
2022-03-28 14:19:33,833: ============================================================
2022-03-28 14:19:33,833: Epoch 30/45 Batch 7200/7662 eta: 13:39:36.799152	Training Loss 0.4303 (0.4322)	Training Prec@1 90.430 (92.040)	Training Prec@5 94.531 (95.053)	
2022-03-28 14:19:33,834: ============================================================
2022-03-28 14:20:16,524: time cost, forward:0.11004438320567043, backward:0.032928076749436576, data cost:0.2882162930132804 
2022-03-28 14:20:16,524: ============================================================
2022-03-28 14:20:16,606: Epoch 30/45 Batch 7300/7662 eta: 13:41:53.599512	Training Loss 0.4287 (0.4322)	Training Prec@1 95.312 (92.041)	Training Prec@5 98.242 (95.053)	
2022-03-28 14:20:16,606: ============================================================
2022-03-28 14:21:00,034: time cost, forward:0.11001986083927662, backward:0.03297440714732362, data cost:0.2882331672722076 
2022-03-28 14:21:00,034: ============================================================
2022-03-28 14:21:00,035: Epoch 30/45 Batch 7400/7662 eta: 13:53:46.878384	Training Loss 0.4342 (0.4322)	Training Prec@1 91.992 (92.042)	Training Prec@5 95.508 (95.052)	
2022-03-28 14:21:00,035: ============================================================
2022-03-28 14:21:42,988: time cost, forward:0.1099978534519426, backward:0.03297152289041981, data cost:0.2882418043375937 
2022-03-28 14:21:42,989: ============================================================
2022-03-28 14:21:42,989: Epoch 30/45 Batch 7500/7662 eta: 13:43:57.467052	Training Loss 0.4273 (0.4322)	Training Prec@1 93.359 (92.043)	Training Prec@5 96.289 (95.054)	
2022-03-28 14:21:42,989: ============================================================
2022-03-28 14:22:26,961: time cost, forward:0.10997738143553434, backward:0.032964811621253184, data cost:0.2883793014117739 
2022-03-28 14:22:26,961: ============================================================
2022-03-28 14:22:26,962: Epoch 30/45 Batch 7600/7662 eta: 14:02:45.493051	Training Loss 0.4352 (0.4322)	Training Prec@1 91.016 (92.042)	Training Prec@5 95.117 (95.053)	
2022-03-28 14:22:26,962: ============================================================
2022-03-28 14:22:55,595: Epoch: 30/45 eta: 14:02:17.790269	Training Loss 0.4177 (0.4323)	Training Prec@1 93.555 (92.042)	Training Prec@5 95.703 (95.053)
2022-03-28 14:22:55,595: ============================================================
2022-03-28 14:22:55,814: Save Checkpoint...
2022-03-28 14:22:55,814: ============================================================
2022-03-28 14:22:58,211: Save done!
2022-03-28 14:22:58,211: ============================================================
2022-03-28 14:23:40,669: time cost, forward:0.10829735283899788, backward:0.0331929067168573, data cost:0.28389004745868723 
2022-03-28 14:23:40,670: ============================================================
2022-03-28 14:23:40,670: Epoch 31/45 Batch 100/7662 eta: 13:31:54.385367	Training Loss 0.4319 (0.4318)	Training Prec@1 91.992 (92.018)	Training Prec@5 94.531 (95.064)	
2022-03-28 14:23:40,670: ============================================================
2022-03-28 14:24:22,692: time cost, forward:0.1085851468033527, backward:0.033233915741120154, data cost:0.28094833220668775 
2022-03-28 14:24:22,693: ============================================================
2022-03-28 14:24:22,693: Epoch 31/45 Batch 200/7662 eta: 13:23:33.793277	Training Loss 0.4239 (0.4316)	Training Prec@1 92.773 (92.068)	Training Prec@5 94.922 (95.128)	
2022-03-28 14:24:22,693: ============================================================
2022-03-28 14:25:06,903: time cost, forward:0.10864513454628628, backward:0.03314135624812199, data cost:0.2874324257955902 
2022-03-28 14:25:06,903: ============================================================
2022-03-28 14:25:06,903: Epoch 31/45 Batch 300/7662 eta: 14:04:38.642272	Training Loss 0.4332 (0.4322)	Training Prec@1 91.211 (92.037)	Training Prec@5 94.531 (95.066)	
2022-03-28 14:25:06,904: ============================================================
2022-03-28 14:25:48,850: time cost, forward:0.10878531257610273, backward:0.03302854165098721, data cost:0.2847614838067153 
2022-03-28 14:25:48,851: ============================================================
2022-03-28 14:25:48,851: Epoch 31/45 Batch 400/7662 eta: 13:20:43.067886	Training Loss 0.4261 (0.4322)	Training Prec@1 90.820 (92.025)	Training Prec@5 94.922 (95.066)	
2022-03-28 14:25:48,851: ============================================================
2022-03-28 14:26:31,907: time cost, forward:0.10878250880805189, backward:0.03301284165086154, data cost:0.2855770435027465 
2022-03-28 14:26:31,908: ============================================================
2022-03-28 14:26:31,908: Epoch 31/45 Batch 500/7662 eta: 13:41:10.212913	Training Loss 0.4389 (0.4322)	Training Prec@1 91.797 (92.014)	Training Prec@5 96.094 (95.057)	
2022-03-28 14:26:31,908: ============================================================
2022-03-28 14:27:14,616: time cost, forward:0.1087797770715118, backward:0.032956374905543255, data cost:0.28549046309444065 
2022-03-28 14:27:14,616: ============================================================
2022-03-28 14:27:14,617: Epoch 31/45 Batch 600/7662 eta: 13:33:49.417025	Training Loss 0.4294 (0.4323)	Training Prec@1 93.555 (92.022)	Training Prec@5 95.703 (95.065)	
2022-03-28 14:27:14,617: ============================================================
2022-03-28 14:27:57,158: time cost, forward:0.10880357137906534, backward:0.03272949063215133, data cost:0.28537051394603113 
2022-03-28 14:27:57,158: ============================================================
2022-03-28 14:27:57,158: Epoch 31/45 Batch 700/7662 eta: 13:29:55.651235	Training Loss 0.4343 (0.4322)	Training Prec@1 91.602 (92.041)	Training Prec@5 94.531 (95.060)	
2022-03-28 14:27:57,158: ============================================================
2022-03-28 14:28:41,081: time cost, forward:0.10879492401629128, backward:0.032739739841752415, data cost:0.28662291157976705 
2022-03-28 14:28:41,082: ============================================================
2022-03-28 14:28:41,082: Epoch 31/45 Batch 800/7662 eta: 13:55:30.701012	Training Loss 0.4240 (0.4321)	Training Prec@1 93.164 (92.050)	Training Prec@5 95.508 (95.070)	
2022-03-28 14:28:41,082: ============================================================
2022-03-28 14:29:24,126: time cost, forward:0.10876948151890772, backward:0.032821608067089246, data cost:0.28703833342394125 
2022-03-28 14:29:24,126: ============================================================
2022-03-28 14:29:24,126: Epoch 31/45 Batch 900/7662 eta: 13:38:03.998615	Training Loss 0.4386 (0.4320)	Training Prec@1 92.188 (92.061)	Training Prec@5 94.922 (95.087)	
2022-03-28 14:29:24,127: ============================================================
2022-03-28 14:30:07,581: time cost, forward:0.10873360581345505, backward:0.03218859857744402, data cost:0.28802509708805485 
2022-03-28 14:30:07,582: ============================================================
2022-03-28 14:30:07,582: Epoch 31/45 Batch 1000/7662 eta: 13:45:09.355930	Training Loss 0.4346 (0.4321)	Training Prec@1 91.992 (92.049)	Training Prec@5 94.922 (95.083)	
2022-03-28 14:30:07,582: ============================================================
2022-03-28 14:30:50,580: time cost, forward:0.10867532304029665, backward:0.032234954009607124, data cost:0.28832812630338384 
2022-03-28 14:30:50,580: ============================================================
2022-03-28 14:30:50,580: Epoch 31/45 Batch 1100/7662 eta: 13:35:45.253020	Training Loss 0.4292 (0.4321)	Training Prec@1 90.625 (92.037)	Training Prec@5 93.750 (95.072)	
2022-03-28 14:30:50,580: ============================================================
2022-03-28 14:31:35,497: time cost, forward:0.10860067292786917, backward:0.03227728659953546, data cost:0.28994841193834675 
2022-03-28 14:31:35,497: ============================================================
2022-03-28 14:31:35,497: Epoch 31/45 Batch 1200/7662 eta: 14:11:24.549266	Training Loss 0.4346 (0.4321)	Training Prec@1 92.188 (92.033)	Training Prec@5 95.703 (95.057)	
2022-03-28 14:31:35,497: ============================================================
2022-03-28 14:32:20,076: time cost, forward:0.10854310104716641, backward:0.03197369968276285, data cost:0.2914437370359026 
2022-03-28 14:32:20,076: ============================================================
2022-03-28 14:32:20,077: Epoch 31/45 Batch 1300/7662 eta: 14:04:16.238612	Training Loss 0.4414 (0.4320)	Training Prec@1 90.430 (92.048)	Training Prec@5 93.750 (95.070)	
2022-03-28 14:32:20,077: ============================================================
2022-03-28 14:33:02,351: time cost, forward:0.10853159777005286, backward:0.031869047109019, data cost:0.2908292890361925 
2022-03-28 14:33:02,352: ============================================================
2022-03-28 14:33:02,352: Epoch 31/45 Batch 1400/7662 eta: 13:19:55.617700	Training Loss 0.4347 (0.4320)	Training Prec@1 92.383 (92.046)	Training Prec@5 94.727 (95.067)	
2022-03-28 14:33:02,352: ============================================================
2022-03-28 14:33:44,477: time cost, forward:0.10849573009406988, backward:0.031988615350297005, data cost:0.29008297621209755 
2022-03-28 14:33:44,478: ============================================================
2022-03-28 14:33:44,478: Epoch 31/45 Batch 1500/7662 eta: 13:16:23.837506	Training Loss 0.4261 (0.4321)	Training Prec@1 91.602 (92.044)	Training Prec@5 93.555 (95.062)	
2022-03-28 14:33:44,478: ============================================================
2022-03-28 14:34:28,840: time cost, forward:0.10843529322506712, backward:0.03206845400406466, data cost:0.2908688620375871 
2022-03-28 14:34:28,841: ============================================================
2022-03-28 14:34:28,866: Epoch 31/45 Batch 1600/7662 eta: 13:58:24.783899	Training Loss 0.4290 (0.4321)	Training Prec@1 91.211 (92.049)	Training Prec@5 94.336 (95.065)	
2022-03-28 14:34:28,866: ============================================================
2022-03-28 14:35:12,722: time cost, forward:0.10838435719194238, backward:0.03215869585860119, data cost:0.29127065485684295 
2022-03-28 14:35:12,723: ============================================================
2022-03-28 14:35:12,723: Epoch 31/45 Batch 1700/7662 eta: 13:47:40.243797	Training Loss 0.4225 (0.4321)	Training Prec@1 93.555 (92.058)	Training Prec@5 96.094 (95.071)	
2022-03-28 14:35:12,723: ============================================================
2022-03-28 14:35:56,215: time cost, forward:0.10833162079260308, backward:0.03215958212003766, data cost:0.29147443909191834 
2022-03-28 14:35:56,216: ============================================================
2022-03-28 14:35:56,216: Epoch 31/45 Batch 1800/7662 eta: 13:40:03.952040	Training Loss 0.4308 (0.4321)	Training Prec@1 92.773 (92.047)	Training Prec@5 95.312 (95.062)	
2022-03-28 14:35:56,216: ============================================================
2022-03-28 14:36:39,678: time cost, forward:0.1082879939538796, backward:0.032150161460928695, data cost:0.2916431062908283 
2022-03-28 14:36:39,679: ============================================================
2022-03-28 14:36:39,679: Epoch 31/45 Batch 1900/7662 eta: 13:38:46.822055	Training Loss 0.4256 (0.4321)	Training Prec@1 93.164 (92.046)	Training Prec@5 95.508 (95.062)	
2022-03-28 14:36:39,679: ============================================================
2022-03-28 14:37:22,330: time cost, forward:0.10824892805957269, backward:0.03214627483476693, data cost:0.29140074459417514 
2022-03-28 14:37:22,331: ============================================================
2022-03-28 14:37:22,331: Epoch 31/45 Batch 2000/7662 eta: 13:22:47.217518	Training Loss 0.4467 (0.4322)	Training Prec@1 90.625 (92.044)	Training Prec@5 95.117 (95.060)	
2022-03-28 14:37:22,331: ============================================================
2022-03-28 14:38:04,815: time cost, forward:0.10822526189814073, backward:0.03214876148574633, data cost:0.29083356066736055 
2022-03-28 14:38:04,816: ============================================================
2022-03-28 14:38:04,816: Epoch 31/45 Batch 2100/7662 eta: 13:18:56.530540	Training Loss 0.4242 (0.4321)	Training Prec@1 92.773 (92.049)	Training Prec@5 95.117 (95.062)	
2022-03-28 14:38:04,816: ============================================================
2022-03-28 14:38:48,524: time cost, forward:0.10819366608169091, backward:0.032354962993828264, data cost:0.2911030966458184 
2022-03-28 14:38:48,524: ============================================================
2022-03-28 14:38:48,524: Epoch 31/45 Batch 2200/7662 eta: 13:41:12.624262	Training Loss 0.4317 (0.4321)	Training Prec@1 91.406 (92.057)	Training Prec@5 93.945 (95.070)	
2022-03-28 14:38:48,524: ============================================================
2022-03-28 14:39:30,879: time cost, forward:0.1081622686008621, backward:0.032555563235189565, data cost:0.2906135452887762 
2022-03-28 14:39:30,879: ============================================================
2022-03-28 14:39:30,880: Epoch 31/45 Batch 2300/7662 eta: 13:15:05.131290	Training Loss 0.4310 (0.4321)	Training Prec@1 92.383 (92.054)	Training Prec@5 95.703 (95.069)	
2022-03-28 14:39:30,880: ============================================================
2022-03-28 14:40:12,808: time cost, forward:0.10813323156492767, backward:0.03266946580719083, data cost:0.2899856032705049 
2022-03-28 14:40:12,809: ============================================================
2022-03-28 14:40:12,809: Epoch 31/45 Batch 2400/7662 eta: 13:06:23.593736	Training Loss 0.4479 (0.4322)	Training Prec@1 89.453 (92.046)	Training Prec@5 93.945 (95.067)	
2022-03-28 14:40:12,809: ============================================================
2022-03-28 14:40:56,752: time cost, forward:0.10810626740930747, backward:0.03263133303934977, data cost:0.2904098145529574 
2022-03-28 14:40:56,752: ============================================================
2022-03-28 14:40:56,752: Epoch 31/45 Batch 2500/7662 eta: 13:43:25.874745	Training Loss 0.4308 (0.4321)	Training Prec@1 91.797 (92.048)	Training Prec@5 95.703 (95.071)	
2022-03-28 14:40:56,752: ============================================================
2022-03-28 14:41:40,042: time cost, forward:0.1081357020605248, backward:0.03263982005557816, data cost:0.29040859689525383 
2022-03-28 14:41:40,043: ============================================================
2022-03-28 14:41:40,043: Epoch 31/45 Batch 2600/7662 eta: 13:30:28.862244	Training Loss 0.4247 (0.4321)	Training Prec@1 92.578 (92.047)	Training Prec@5 95.703 (95.071)	
2022-03-28 14:41:40,043: ============================================================
2022-03-28 14:42:23,933: time cost, forward:0.10814702506416946, backward:0.03258588041098483, data cost:0.29073338819548306 
2022-03-28 14:42:23,933: ============================================================
2022-03-28 14:42:23,933: Epoch 31/45 Batch 2700/7662 eta: 13:40:58.724120	Training Loss 0.4391 (0.4321)	Training Prec@1 92.773 (92.051)	Training Prec@5 95.703 (95.072)	
2022-03-28 14:42:23,934: ============================================================
2022-03-28 14:43:06,658: time cost, forward:0.10816368676799244, backward:0.03253978650541806, data cost:0.2905900092498027 
2022-03-28 14:43:06,659: ============================================================
2022-03-28 14:43:06,659: Epoch 31/45 Batch 2800/7662 eta: 13:18:28.628906	Training Loss 0.4289 (0.4321)	Training Prec@1 92.969 (92.050)	Training Prec@5 95.117 (95.072)	
2022-03-28 14:43:06,659: ============================================================
2022-03-28 14:43:49,555: time cost, forward:0.10817251381441494, backward:0.0325697008680993, data cost:0.2904663527575227 
2022-03-28 14:43:49,555: ============================================================
2022-03-28 14:43:49,555: Epoch 31/45 Batch 2900/7662 eta: 13:20:57.299511	Training Loss 0.4512 (0.4321)	Training Prec@1 87.305 (92.046)	Training Prec@5 91.602 (95.064)	
2022-03-28 14:43:49,556: ============================================================
2022-03-28 14:44:33,299: time cost, forward:0.10816036585610324, backward:0.032546764534686, data cost:0.2906781665163463 
2022-03-28 14:44:33,300: ============================================================
2022-03-28 14:44:33,300: Epoch 31/45 Batch 3000/7662 eta: 13:36:03.747208	Training Loss 0.4411 (0.4322)	Training Prec@1 91.211 (92.041)	Training Prec@5 95.898 (95.059)	
2022-03-28 14:44:33,300: ============================================================
2022-03-28 14:45:16,247: time cost, forward:0.10814408765142293, backward:0.032531943925929556, data cost:0.29058038984509815 
2022-03-28 14:45:16,247: ============================================================
2022-03-28 14:45:16,247: Epoch 31/45 Batch 3100/7662 eta: 13:20:28.193446	Training Loss 0.4280 (0.4322)	Training Prec@1 91.406 (92.045)	Training Prec@5 94.531 (95.061)	
2022-03-28 14:45:16,247: ============================================================
2022-03-28 14:46:00,683: time cost, forward:0.10811617554035288, backward:0.03255840255603749, data cost:0.29103868243917147 
2022-03-28 14:46:00,683: ============================================================
2022-03-28 14:46:00,684: Epoch 31/45 Batch 3200/7662 eta: 13:47:29.463106	Training Loss 0.4328 (0.4322)	Training Prec@1 90.430 (92.049)	Training Prec@5 93.359 (95.060)	
2022-03-28 14:46:00,684: ============================================================
2022-03-28 14:46:44,092: time cost, forward:0.10810353951802504, backward:0.03255157356516597, data cost:0.29113359059590793 
2022-03-28 14:46:44,092: ============================================================
2022-03-28 14:46:44,092: Epoch 31/45 Batch 3300/7662 eta: 13:27:37.270992	Training Loss 0.4288 (0.4322)	Training Prec@1 93.164 (92.049)	Training Prec@5 95.898 (95.061)	
2022-03-28 14:46:44,092: ============================================================
2022-03-28 14:47:26,596: time cost, forward:0.10809128822456847, backward:0.032551524638428204, data cost:0.29093208513599383 
2022-03-28 14:47:26,596: ============================================================
2022-03-28 14:47:26,626: Epoch 31/45 Batch 3400/7662 eta: 13:10:38.016297	Training Loss 0.4130 (0.4322)	Training Prec@1 93.945 (92.046)	Training Prec@5 96.289 (95.059)	
2022-03-28 14:47:26,626: ============================================================
2022-03-28 14:48:09,165: time cost, forward:0.10808656705587583, backward:0.032569543945343025, data cost:0.2907266971825804 
2022-03-28 14:48:09,166: ============================================================
2022-03-28 14:48:09,167: Epoch 31/45 Batch 3500/7662 eta: 13:10:03.687306	Training Loss 0.4173 (0.4322)	Training Prec@1 92.969 (92.047)	Training Prec@5 95.117 (95.060)	
2022-03-28 14:48:09,167: ============================================================
2022-03-28 14:48:53,059: time cost, forward:0.10807151819606992, backward:0.03257026576969351, data cost:0.29089873027192054 
2022-03-28 14:48:53,059: ============================================================
2022-03-28 14:48:53,059: Epoch 31/45 Batch 3600/7662 eta: 13:34:26.337864	Training Loss 0.4304 (0.4321)	Training Prec@1 91.992 (92.051)	Training Prec@5 95.508 (95.060)	
2022-03-28 14:48:53,060: ============================================================
2022-03-28 14:49:35,651: time cost, forward:0.10804288417592375, backward:0.032594343816051294, data cost:0.2907911463343282 
2022-03-28 14:49:35,651: ============================================================
2022-03-28 14:49:35,651: Epoch 31/45 Batch 3700/7662 eta: 13:09:35.589209	Training Loss 0.4367 (0.4322)	Training Prec@1 91.797 (92.054)	Training Prec@5 94.531 (95.062)	
2022-03-28 14:49:35,652: ============================================================
2022-03-28 14:50:19,647: time cost, forward:0.10858049296052495, backward:0.03267278744064214, data cost:0.29038408148379724 
2022-03-28 14:50:19,647: ============================================================
2022-03-28 14:50:19,647: Epoch 31/45 Batch 3800/7662 eta: 13:34:53.210980	Training Loss 0.4423 (0.4322)	Training Prec@1 93.359 (92.054)	Training Prec@5 96.094 (95.062)	
2022-03-28 14:50:19,648: ============================================================
2022-03-28 14:51:06,061: time cost, forward:0.10959986852786271, backward:0.0327583390891659, data cost:0.29010229668393445 
2022-03-28 14:51:06,062: ============================================================
2022-03-28 14:51:06,062: Epoch 31/45 Batch 3900/7662 eta: 14:18:54.414931	Training Loss 0.4295 (0.4322)	Training Prec@1 94.141 (92.054)	Training Prec@5 95.898 (95.062)	
2022-03-28 14:51:06,062: ============================================================
2022-03-28 14:51:50,117: time cost, forward:0.11075009259917672, backward:0.03284789276409221, data cost:0.2890491766403782 
2022-03-28 14:51:50,118: ============================================================
2022-03-28 14:51:50,118: Epoch 31/45 Batch 4000/7662 eta: 13:34:32.039587	Training Loss 0.4395 (0.4321)	Training Prec@1 91.016 (92.058)	Training Prec@5 94.141 (95.067)	
2022-03-28 14:51:50,119: ============================================================
2022-03-28 14:52:35,284: time cost, forward:0.11120284502318382, backward:0.03292710654879233, data cost:0.28896396897077037 
2022-03-28 14:52:35,285: ============================================================
2022-03-28 14:52:35,285: Epoch 31/45 Batch 4100/7662 eta: 13:54:18.825909	Training Loss 0.4388 (0.4321)	Training Prec@1 91.602 (92.062)	Training Prec@5 94.531 (95.069)	
2022-03-28 14:52:35,285: ============================================================
2022-03-28 14:53:18,326: time cost, forward:0.11115961956052332, backward:0.032930844566543264, data cost:0.2888888180553984 
2022-03-28 14:53:18,327: ============================================================
2022-03-28 14:53:18,327: Epoch 31/45 Batch 4200/7662 eta: 13:14:20.905722	Training Loss 0.4263 (0.4321)	Training Prec@1 92.383 (92.059)	Training Prec@5 96.094 (95.069)	
2022-03-28 14:53:18,327: ============================================================
2022-03-28 14:54:06,223: time cost, forward:0.11203120813505071, backward:0.03302201017941894, data cost:0.2890416959685818 
2022-03-28 14:54:06,234: ============================================================
2022-03-28 14:54:06,234: Epoch 31/45 Batch 4300/7662 eta: 14:43:20.423178	Training Loss 0.4333 (0.4321)	Training Prec@1 89.648 (92.060)	Training Prec@5 92.773 (95.071)	
2022-03-28 14:54:06,235: ============================================================
2022-03-28 14:54:48,819: time cost, forward:0.11246887145461915, backward:0.03308382975185262, data cost:0.2883269273034278 
2022-03-28 14:54:48,820: ============================================================
2022-03-28 14:54:48,820: Epoch 31/45 Batch 4400/7662 eta: 13:04:30.523963	Training Loss 0.4395 (0.4322)	Training Prec@1 90.625 (92.060)	Training Prec@5 93.164 (95.072)	
2022-03-28 14:54:48,821: ============================================================
2022-03-28 14:55:32,056: time cost, forward:0.11239024680782674, backward:0.03310301786635976, data cost:0.2883479774832911 
2022-03-28 14:55:32,057: ============================================================
2022-03-28 14:55:32,057: Epoch 31/45 Batch 4500/7662 eta: 13:15:46.626774	Training Loss 0.4474 (0.4322)	Training Prec@1 90.430 (92.060)	Training Prec@5 93.945 (95.067)	
2022-03-28 14:55:32,057: ============================================================
2022-03-28 14:56:16,472: time cost, forward:0.1127231347091718, backward:0.03311450845859807, data cost:0.2882136239982683 
2022-03-28 14:56:16,472: ============================================================
2022-03-28 14:56:16,472: Epoch 31/45 Batch 4600/7662 eta: 13:36:44.069432	Training Loss 0.4333 (0.4322)	Training Prec@1 90.234 (92.058)	Training Prec@5 94.336 (95.067)	
2022-03-28 14:56:16,473: ============================================================
2022-03-28 14:57:02,229: time cost, forward:0.11324086314085875, backward:0.03317584791546757, data cost:0.2881257816131228 
2022-03-28 14:57:02,229: ============================================================
2022-03-28 14:57:02,229: Epoch 31/45 Batch 4700/7662 eta: 14:00:38.291935	Training Loss 0.4360 (0.4322)	Training Prec@1 90.625 (92.051)	Training Prec@5 93.945 (95.063)	
2022-03-28 14:57:02,230: ============================================================
2022-03-28 14:57:45,663: time cost, forward:0.11356152144787982, backward:0.0332273306710493, data cost:0.28775299671019683 
2022-03-28 14:57:45,663: ============================================================
2022-03-28 14:57:45,663: Epoch 31/45 Batch 4800/7662 eta: 13:17:14.521481	Training Loss 0.4323 (0.4322)	Training Prec@1 92.773 (92.044)	Training Prec@5 96.094 (95.061)	
2022-03-28 14:57:45,664: ============================================================
2022-03-28 14:58:31,075: time cost, forward:0.11415357759082091, backward:0.03330069757233008, data cost:0.28747337792547023 
2022-03-28 14:58:31,075: ============================================================
2022-03-28 14:58:31,075: Epoch 31/45 Batch 4900/7662 eta: 13:52:46.997220	Training Loss 0.4285 (0.4322)	Training Prec@1 92.383 (92.045)	Training Prec@5 94.531 (95.061)	
2022-03-28 14:58:31,075: ============================================================
2022-03-28 14:59:14,777: time cost, forward:0.11456569859351509, backward:0.03331331263353501, data cost:0.2870731726244083 
2022-03-28 14:59:14,777: ============================================================
2022-03-28 14:59:14,777: Epoch 31/45 Batch 5000/7662 eta: 13:20:42.200412	Training Loss 0.4318 (0.4322)	Training Prec@1 93.359 (92.048)	Training Prec@5 95.117 (95.060)	
2022-03-28 14:59:14,778: ============================================================
2022-03-28 14:59:58,297: time cost, forward:0.11442582951875079, backward:0.03333054853201053, data cost:0.2871830975876109 
2022-03-28 14:59:58,298: ============================================================
2022-03-28 14:59:58,298: Epoch 31/45 Batch 5100/7662 eta: 13:16:38.894504	Training Loss 0.4405 (0.4322)	Training Prec@1 90.625 (92.049)	Training Prec@5 92.969 (95.060)	
2022-03-28 14:59:58,298: ============================================================
2022-03-28 15:00:43,859: time cost, forward:0.11479288615912606, backward:0.03337498255981164, data cost:0.28717382703063166 
2022-03-28 15:00:43,860: ============================================================
2022-03-28 15:00:43,860: Epoch 31/45 Batch 5200/7662 eta: 13:53:15.699740	Training Loss 0.4317 (0.4322)	Training Prec@1 90.234 (92.048)	Training Prec@5 94.531 (95.057)	
2022-03-28 15:00:43,860: ============================================================
2022-03-28 15:01:27,782: time cost, forward:0.11466040123541595, backward:0.033353770627325766, data cost:0.28738927508237383 
2022-03-28 15:01:27,782: ============================================================
2022-03-28 15:01:27,783: Epoch 31/45 Batch 5300/7662 eta: 13:22:33.033167	Training Loss 0.4363 (0.4322)	Training Prec@1 93.359 (92.047)	Training Prec@5 96.484 (95.057)	
2022-03-28 15:01:27,783: ============================================================
2022-03-28 15:02:11,943: time cost, forward:0.114711041317138, backward:0.033380890625630245, data cost:0.2874268011862226 
2022-03-28 15:02:11,943: ============================================================
2022-03-28 15:02:11,944: Epoch 31/45 Batch 5400/7662 eta: 13:26:09.879162	Training Loss 0.4268 (0.4322)	Training Prec@1 92.969 (92.046)	Training Prec@5 94.531 (95.055)	
2022-03-28 15:02:11,944: ============================================================
2022-03-28 15:02:57,767: time cost, forward:0.1152498471908167, backward:0.033479329520386986, data cost:0.2871872094527746 
2022-03-28 15:02:57,768: ============================================================
2022-03-28 15:02:57,769: Epoch 31/45 Batch 5500/7662 eta: 13:55:46.698142	Training Loss 0.4322 (0.4322)	Training Prec@1 91.016 (92.048)	Training Prec@5 94.727 (95.057)	
2022-03-28 15:02:57,769: ============================================================
2022-03-28 15:03:40,810: time cost, forward:0.11514818644945186, backward:0.033488023577044744, data cost:0.28716735346740135 
2022-03-28 15:03:40,811: ============================================================
2022-03-28 15:03:40,811: Epoch 31/45 Batch 5600/7662 eta: 13:04:18.562922	Training Loss 0.4308 (0.4322)	Training Prec@1 93.164 (92.052)	Training Prec@5 95.508 (95.059)	
2022-03-28 15:03:40,811: ============================================================
2022-03-28 15:04:26,995: time cost, forward:0.11530621065593934, backward:0.03350841411269952, data cost:0.2874400283939317 
2022-03-28 15:04:26,996: ============================================================
2022-03-28 15:04:26,996: Epoch 31/45 Batch 5700/7662 eta: 14:00:48.596673	Training Loss 0.4342 (0.4322)	Training Prec@1 93.945 (92.053)	Training Prec@5 95.703 (95.060)	
2022-03-28 15:04:26,996: ============================================================
2022-03-28 15:05:09,589: time cost, forward:0.11517648364700393, backward:0.033526809529243494, data cost:0.28737598555851523 
2022-03-28 15:05:09,589: ============================================================
2022-03-28 15:05:09,590: Epoch 31/45 Batch 5800/7662 eta: 12:54:42.869008	Training Loss 0.4410 (0.4322)	Training Prec@1 89.844 (92.051)	Training Prec@5 94.141 (95.061)	
2022-03-28 15:05:09,590: ============================================================
2022-03-28 15:05:53,124: time cost, forward:0.11505976061150874, backward:0.03352637726647467, data cost:0.28746862434940756 
2022-03-28 15:05:53,124: ============================================================
2022-03-28 15:05:53,124: Epoch 31/45 Batch 5900/7662 eta: 13:11:06.087831	Training Loss 0.4287 (0.4322)	Training Prec@1 91.992 (92.050)	Training Prec@5 93.945 (95.061)	
2022-03-28 15:05:53,124: ============================================================
2022-03-28 15:06:37,579: time cost, forward:0.1151872091361693, backward:0.033559042566079736, data cost:0.28741797150562437 
2022-03-28 15:06:37,580: ============================================================
2022-03-28 15:06:37,580: Epoch 31/45 Batch 6000/7662 eta: 13:27:06.062374	Training Loss 0.4280 (0.4322)	Training Prec@1 93.164 (92.050)	Training Prec@5 95.508 (95.063)	
2022-03-28 15:06:37,580: ============================================================
2022-03-28 15:07:22,729: time cost, forward:0.11556771665855517, backward:0.03357122327132506, data cost:0.2872940197325589 
2022-03-28 15:07:22,730: ============================================================
2022-03-28 15:07:22,730: Epoch 31/45 Batch 6100/7662 eta: 13:38:57.398474	Training Loss 0.4441 (0.4322)	Training Prec@1 90.820 (92.050)	Training Prec@5 93.555 (95.063)	
2022-03-28 15:07:22,730: ============================================================
2022-03-28 15:08:07,734: time cost, forward:0.1160025944689009, backward:0.033592794187262395, data cost:0.2870580869675606 
2022-03-28 15:08:07,734: ============================================================
2022-03-28 15:08:07,735: Epoch 31/45 Batch 6200/7662 eta: 13:35:33.702035	Training Loss 0.4304 (0.4322)	Training Prec@1 93.555 (92.046)	Training Prec@5 96.094 (95.061)	
2022-03-28 15:08:07,735: ============================================================
2022-03-28 15:08:51,040: time cost, forward:0.11597505886037306, backward:0.033583062700779404, data cost:0.2870145507191151 
2022-03-28 15:08:51,040: ============================================================
2022-03-28 15:08:51,041: Epoch 31/45 Batch 6300/7662 eta: 13:04:03.910496	Training Loss 0.4422 (0.4322)	Training Prec@1 91.797 (92.050)	Training Prec@5 94.727 (95.064)	
2022-03-28 15:08:51,041: ============================================================
2022-03-28 15:09:34,739: time cost, forward:0.11584759716094145, backward:0.03354364269356147, data cost:0.2871838139451878 
2022-03-28 15:09:34,739: ============================================================
2022-03-28 15:09:34,740: Epoch 31/45 Batch 6400/7662 eta: 13:10:26.867632	Training Loss 0.4216 (0.4322)	Training Prec@1 92.773 (92.050)	Training Prec@5 95.703 (95.062)	
2022-03-28 15:09:34,740: ============================================================
2022-03-28 15:10:17,882: time cost, forward:0.11575071832586718, backward:0.0335593894181352, data cost:0.2871862930010825 
2022-03-28 15:10:17,883: ============================================================
2022-03-28 15:10:17,883: Epoch 31/45 Batch 6500/7662 eta: 12:59:40.786138	Training Loss 0.4410 (0.4322)	Training Prec@1 91.797 (92.052)	Training Prec@5 95.117 (95.064)	
2022-03-28 15:10:17,883: ============================================================
2022-03-28 15:11:01,551: time cost, forward:0.1156335175082546, backward:0.03352090192032612, data cost:0.2873294486080377 
2022-03-28 15:11:01,551: ============================================================
2022-03-28 15:11:01,552: Epoch 31/45 Batch 6600/7662 eta: 13:08:26.608943	Training Loss 0.4364 (0.4322)	Training Prec@1 92.383 (92.049)	Training Prec@5 95.312 (95.063)	
2022-03-28 15:11:01,552: ============================================================
2022-03-28 15:11:46,770: time cost, forward:0.11594888014052408, backward:0.033519235684420036, data cost:0.28725586689876287 
2022-03-28 15:11:46,770: ============================================================
2022-03-28 15:11:46,771: Epoch 31/45 Batch 6700/7662 eta: 13:35:41.117923	Training Loss 0.4188 (0.4322)	Training Prec@1 95.703 (92.050)	Training Prec@5 97.461 (95.062)	
2022-03-28 15:11:46,771: ============================================================
2022-03-28 15:12:30,794: time cost, forward:0.11627147362186271, backward:0.033564746230818766, data cost:0.28693904093318623 
2022-03-28 15:12:30,808: ============================================================
2022-03-28 15:12:30,809: Epoch 31/45 Batch 6800/7662 eta: 13:13:38.431874	Training Loss 0.4240 (0.4322)	Training Prec@1 92.578 (92.050)	Training Prec@5 95.898 (95.061)	
2022-03-28 15:12:30,809: ============================================================
2022-03-28 15:13:15,221: time cost, forward:0.1165643920240445, backward:0.03359917202346963, data cost:0.2867114060096835 
2022-03-28 15:13:15,222: ============================================================
2022-03-28 15:13:15,222: Epoch 31/45 Batch 6900/7662 eta: 13:19:40.360624	Training Loss 0.4388 (0.4322)	Training Prec@1 91.211 (92.048)	Training Prec@5 94.336 (95.061)	
2022-03-28 15:13:15,222: ============================================================
2022-03-28 15:13:58,513: time cost, forward:0.11664209554562961, backward:0.033651161745695206, data cost:0.28651098422211124 
2022-03-28 15:13:58,514: ============================================================
2022-03-28 15:13:58,515: Epoch 31/45 Batch 7000/7662 eta: 12:58:46.199576	Training Loss 0.4381 (0.4322)	Training Prec@1 91.406 (92.048)	Training Prec@5 93.555 (95.059)	
2022-03-28 15:13:58,515: ============================================================
2022-03-28 15:14:41,556: time cost, forward:0.11665446796221102, backward:0.03367261467458228, data cost:0.28639088638870963 
2022-03-28 15:14:41,568: ============================================================
2022-03-28 15:14:41,568: Epoch 31/45 Batch 7100/7662 eta: 12:53:45.126980	Training Loss 0.4356 (0.4322)	Training Prec@1 91.797 (92.048)	Training Prec@5 95.508 (95.060)	
2022-03-28 15:14:41,568: ============================================================
2022-03-28 15:15:26,918: time cost, forward:0.11740397025949674, backward:0.03372027563012164, data cost:0.2858218078332438 
2022-03-28 15:15:26,929: ============================================================
2022-03-28 15:15:26,929: Epoch 31/45 Batch 7200/7662 eta: 13:34:27.546132	Training Loss 0.4383 (0.4322)	Training Prec@1 91.992 (92.048)	Training Prec@5 94.922 (95.060)	
2022-03-28 15:15:26,929: ============================================================
2022-03-28 15:16:09,847: time cost, forward:0.11771343887497389, backward:0.03373301706994816, data cost:0.2853874877995539 
2022-03-28 15:16:09,847: ============================================================
2022-03-28 15:16:09,859: Epoch 31/45 Batch 7300/7662 eta: 12:50:06.118474	Training Loss 0.4302 (0.4322)	Training Prec@1 91.992 (92.051)	Training Prec@5 95.312 (95.062)	
2022-03-28 15:16:09,859: ============================================================
2022-03-28 15:16:54,085: time cost, forward:0.11810379689537942, backward:0.033811737708359706, data cost:0.28498351817228357 
2022-03-28 15:16:54,086: ============================================================
2022-03-28 15:16:54,086: Epoch 31/45 Batch 7400/7662 eta: 13:12:37.575903	Training Loss 0.4310 (0.4322)	Training Prec@1 91.602 (92.050)	Training Prec@5 95.508 (95.063)	
2022-03-28 15:16:54,086: ============================================================
2022-03-28 15:17:38,466: time cost, forward:0.11831703836527772, backward:0.03386501032982783, data cost:0.28479411789919157 
2022-03-28 15:17:38,466: ============================================================
2022-03-28 15:17:38,466: Epoch 31/45 Batch 7500/7662 eta: 13:14:38.331027	Training Loss 0.4300 (0.4322)	Training Prec@1 92.969 (92.051)	Training Prec@5 96.289 (95.063)	
2022-03-28 15:17:38,466: ============================================================
2022-03-28 15:18:24,695: time cost, forward:0.11867895134500273, backward:0.03390049689662254, data cost:0.284721903180367 
2022-03-28 15:18:24,695: ============================================================
2022-03-28 15:18:24,695: Epoch 31/45 Batch 7600/7662 eta: 13:46:58.167080	Training Loss 0.4280 (0.4322)	Training Prec@1 92.578 (92.052)	Training Prec@5 95.117 (95.063)	
2022-03-28 15:18:24,696: ============================================================
2022-03-28 15:18:54,328: Epoch: 31/45 eta: 13:46:29.042740	Training Loss 0.4339 (0.4322)	Training Prec@1 91.406 (92.053)	Training Prec@5 94.922 (95.064)
2022-03-28 15:18:54,328: ============================================================
2022-03-28 15:18:54,330: Save Checkpoint...
2022-03-28 15:18:54,331: ============================================================
2022-03-28 15:18:56,506: Save done!
2022-03-28 15:18:56,506: ============================================================
2022-03-28 15:19:40,231: time cost, forward:0.10647314726704299, backward:0.03342939868117824, data cost:0.2983017714336665 
2022-03-28 15:19:40,232: ============================================================
2022-03-28 15:19:40,232: Epoch 32/45 Batch 100/7662 eta: 13:00:59.479321	Training Loss 0.4438 (0.4320)	Training Prec@1 91.211 (92.051)	Training Prec@5 94.531 (95.119)	
2022-03-28 15:19:40,232: ============================================================
2022-03-28 15:20:23,213: time cost, forward:0.10758388102354116, backward:0.03347946411401183, data cost:0.292746189251617 
2022-03-28 15:20:23,214: ============================================================
2022-03-28 15:20:23,214: Epoch 32/45 Batch 200/7662 eta: 12:47:00.394637	Training Loss 0.4262 (0.4324)	Training Prec@1 92.578 (91.923)	Training Prec@5 95.703 (95.014)	
2022-03-28 15:20:23,214: ============================================================
2022-03-28 15:21:05,928: time cost, forward:0.10793181087659753, backward:0.03301014389880126, data cost:0.29024498518494063 
2022-03-28 15:21:05,929: ============================================================
2022-03-28 15:21:05,929: Epoch 32/45 Batch 300/7662 eta: 12:41:31.892674	Training Loss 0.4335 (0.4320)	Training Prec@1 92.773 (92.041)	Training Prec@5 95.312 (95.080)	
2022-03-28 15:21:05,929: ============================================================
2022-03-28 15:21:49,267: time cost, forward:0.1128663962945006, backward:0.03412965903604837, data cost:0.28481894268427876 
2022-03-28 15:21:49,268: ============================================================
2022-03-28 15:21:49,268: Epoch 32/45 Batch 400/7662 eta: 12:51:55.840728	Training Loss 0.4379 (0.4319)	Training Prec@1 92.383 (92.080)	Training Prec@5 94.336 (95.091)	
2022-03-28 15:21:49,268: ============================================================
2022-03-28 15:22:32,397: time cost, forward:0.1120529256029454, backward:0.034208334997326194, data cost:0.28533501328829536 
2022-03-28 15:22:32,397: ============================================================
2022-03-28 15:22:32,397: Epoch 32/45 Batch 500/7662 eta: 12:47:28.849466	Training Loss 0.4229 (0.4318)	Training Prec@1 92.773 (92.076)	Training Prec@5 95.508 (95.088)	
2022-03-28 15:22:32,398: ============================================================
2022-03-28 15:23:16,818: time cost, forward:0.11354316057068278, backward:0.034334060942788354, data cost:0.28552542743778386 
2022-03-28 15:23:16,818: ============================================================
2022-03-28 15:23:16,819: Epoch 32/45 Batch 600/7662 eta: 13:09:43.619349	Training Loss 0.4262 (0.4317)	Training Prec@1 92.188 (92.074)	Training Prec@5 95.703 (95.095)	
2022-03-28 15:23:16,819: ============================================================
2022-03-28 15:24:00,109: time cost, forward:0.11282738527344362, backward:0.03432200599637666, data cost:0.2863166704027779 
2022-03-28 15:24:00,110: ============================================================
2022-03-28 15:24:00,110: Epoch 32/45 Batch 700/7662 eta: 12:48:55.197536	Training Loss 0.4423 (0.4315)	Training Prec@1 87.891 (92.106)	Training Prec@5 93.359 (95.108)	
2022-03-28 15:24:00,110: ============================================================
2022-03-28 15:24:44,069: time cost, forward:0.11228996463054709, backward:0.03440235046033418, data cost:0.2874771602759522 
2022-03-28 15:24:44,070: ============================================================
2022-03-28 15:24:44,070: Epoch 32/45 Batch 800/7662 eta: 13:00:03.779041	Training Loss 0.4258 (0.4315)	Training Prec@1 94.336 (92.131)	Training Prec@5 96.289 (95.114)	
2022-03-28 15:24:44,070: ============================================================
2022-03-28 15:25:27,019: time cost, forward:0.11210131326957593, backward:0.034340852890184376, data cost:0.2872452051674033 
2022-03-28 15:25:27,019: ============================================================
2022-03-28 15:25:27,020: Epoch 32/45 Batch 900/7662 eta: 12:41:24.969975	Training Loss 0.4308 (0.4317)	Training Prec@1 93.359 (92.106)	Training Prec@5 95.898 (95.110)	
2022-03-28 15:25:27,020: ============================================================
2022-03-28 15:26:11,427: time cost, forward:0.11278042325505742, backward:0.034287980846217925, data cost:0.2875751859552271 
2022-03-28 15:26:11,427: ============================================================
2022-03-28 15:26:11,427: Epoch 32/45 Batch 1000/7662 eta: 13:06:31.860996	Training Loss 0.4339 (0.4318)	Training Prec@1 91.992 (92.094)	Training Prec@5 94.727 (95.103)	
2022-03-28 15:26:11,428: ============================================================
2022-03-28 15:26:55,115: time cost, forward:0.11243485037688238, backward:0.034210235016035755, data cost:0.2880177011915073 
2022-03-28 15:26:55,116: ============================================================
2022-03-28 15:26:55,116: Epoch 32/45 Batch 1100/7662 eta: 12:53:03.666776	Training Loss 0.4409 (0.4318)	Training Prec@1 88.672 (92.092)	Training Prec@5 91.211 (95.104)	
2022-03-28 15:26:55,116: ============================================================
2022-03-28 15:27:39,362: time cost, forward:0.11359833696666015, backward:0.03424194616710673, data cost:0.2876049048508874 
2022-03-28 15:27:39,363: ============================================================
2022-03-28 15:27:39,363: Epoch 32/45 Batch 1200/7662 eta: 13:02:12.128625	Training Loss 0.4250 (0.4318)	Training Prec@1 93.750 (92.093)	Training Prec@5 95.312 (95.100)	
2022-03-28 15:27:39,363: ============================================================
2022-03-28 15:28:23,245: time cost, forward:0.11325090293062018, backward:0.034139698151903396, data cost:0.28827954990483873 
2022-03-28 15:28:23,246: ============================================================
2022-03-28 15:28:23,246: Epoch 32/45 Batch 1300/7662 eta: 12:55:02.571206	Training Loss 0.4277 (0.4317)	Training Prec@1 92.578 (92.103)	Training Prec@5 95.898 (95.111)	
2022-03-28 15:28:23,246: ============================================================
2022-03-28 15:29:08,501: time cost, forward:0.11530718759096376, backward:0.034347120057352104, data cost:0.28713514602039436 
2022-03-28 15:29:08,501: ============================================================
2022-03-28 15:29:08,502: Epoch 32/45 Batch 1400/7662 eta: 13:18:31.727871	Training Loss 0.4329 (0.4318)	Training Prec@1 94.141 (92.095)	Training Prec@5 96.094 (95.109)	
2022-03-28 15:29:08,502: ============================================================
2022-03-28 15:29:50,588: time cost, forward:0.11620307732137383, backward:0.03432055900222544, data cost:0.28518339711240803 
2022-03-28 15:29:50,588: ============================================================
2022-03-28 15:29:50,589: Epoch 32/45 Batch 1500/7662 eta: 12:21:55.026404	Training Loss 0.4179 (0.4317)	Training Prec@1 92.383 (92.101)	Training Prec@5 96.094 (95.115)	
2022-03-28 15:29:50,589: ============================================================
2022-03-28 15:30:34,671: time cost, forward:0.11572573764984723, backward:0.0341767420837326, data cost:0.28610681607769456 
2022-03-28 15:30:34,672: ============================================================
2022-03-28 15:30:34,672: Epoch 32/45 Batch 1600/7662 eta: 12:56:22.294220	Training Loss 0.4222 (0.4318)	Training Prec@1 93.164 (92.107)	Training Prec@5 95.508 (95.120)	
2022-03-28 15:30:34,672: ============================================================
2022-03-28 15:31:18,053: time cost, forward:0.11530047434087778, backward:0.034086285092116385, data cost:0.28645818159115743 
2022-03-28 15:31:18,053: ============================================================
2022-03-28 15:31:18,054: Epoch 32/45 Batch 1700/7662 eta: 12:43:17.873576	Training Loss 0.4294 (0.4319)	Training Prec@1 93.359 (92.091)	Training Prec@5 95.703 (95.101)	
2022-03-28 15:31:18,054: ============================================================
2022-03-28 15:32:01,784: time cost, forward:0.11490048574963962, backward:0.03405760539247302, data cost:0.2869649802796903 
2022-03-28 15:32:01,784: ============================================================
2022-03-28 15:32:01,785: Epoch 32/45 Batch 1800/7662 eta: 12:48:42.378017	Training Loss 0.4230 (0.4319)	Training Prec@1 91.602 (92.090)	Training Prec@5 93.945 (95.102)	
2022-03-28 15:32:01,785: ============================================================
2022-03-28 15:32:46,454: time cost, forward:0.11456150817770654, backward:0.03417826891573182, data cost:0.28772085223968813 
2022-03-28 15:32:46,455: ============================================================
2022-03-28 15:32:46,455: Epoch 32/45 Batch 1900/7662 eta: 13:04:28.868231	Training Loss 0.4364 (0.4319)	Training Prec@1 91.992 (92.095)	Training Prec@5 95.703 (95.101)	
2022-03-28 15:32:46,455: ============================================================
2022-03-28 15:33:32,752: time cost, forward:0.11693106024428687, backward:0.034396035126175145, data cost:0.28643410953657217 
2022-03-28 15:33:32,753: ============================================================
2022-03-28 15:33:32,753: Epoch 32/45 Batch 2000/7662 eta: 13:32:17.388765	Training Loss 0.4323 (0.4318)	Training Prec@1 91.992 (92.097)	Training Prec@5 94.922 (95.105)	
2022-03-28 15:33:32,753: ============================================================
2022-03-28 15:34:15,093: time cost, forward:0.11647956436051818, backward:0.0343251724706598, data cost:0.28627103314392904 
2022-03-28 15:34:15,094: ============================================================
2022-03-28 15:34:15,094: Epoch 32/45 Batch 2100/7662 eta: 12:22:09.313006	Training Loss 0.4384 (0.4319)	Training Prec@1 93.555 (92.093)	Training Prec@5 96.484 (95.097)	
2022-03-28 15:34:15,094: ============================================================
2022-03-28 15:34:59,399: time cost, forward:0.1169647920017841, backward:0.03428290084363548, data cost:0.28610187825857375 
2022-03-28 15:34:59,400: ============================================================
2022-03-28 15:34:59,400: Epoch 32/45 Batch 2200/7662 eta: 12:55:51.941275	Training Loss 0.4290 (0.4318)	Training Prec@1 92.578 (92.104)	Training Prec@5 94.727 (95.099)	
2022-03-28 15:34:59,400: ============================================================
2022-03-28 15:35:43,369: time cost, forward:0.11781270641925906, backward:0.03437839917070506, data cost:0.2852093245268594 
2022-03-28 15:35:43,369: ============================================================
2022-03-28 15:35:43,369: Epoch 32/45 Batch 2300/7662 eta: 12:49:14.449635	Training Loss 0.4235 (0.4319)	Training Prec@1 91.992 (92.092)	Training Prec@5 94.727 (95.092)	
2022-03-28 15:35:43,370: ============================================================
2022-03-28 15:36:27,123: time cost, forward:0.11772526736655002, backward:0.034367503499726344, data cost:0.28529010230474244 
2022-03-28 15:36:27,124: ============================================================
2022-03-28 15:36:27,124: Epoch 32/45 Batch 2400/7662 eta: 12:44:44.965519	Training Loss 0.4351 (0.4319)	Training Prec@1 92.383 (92.089)	Training Prec@5 94.141 (95.092)	
2022-03-28 15:36:27,124: ============================================================
2022-03-28 15:37:10,923: time cost, forward:0.11726791212777225, backward:0.03431439752719936, data cost:0.2858170930649481 
2022-03-28 15:37:10,923: ============================================================
2022-03-28 15:37:10,923: Epoch 32/45 Batch 2500/7662 eta: 12:44:48.209218	Training Loss 0.4280 (0.4319)	Training Prec@1 91.016 (92.084)	Training Prec@5 94.922 (95.084)	
2022-03-28 15:37:10,924: ============================================================
2022-03-28 15:37:54,572: time cost, forward:0.11691515332875503, backward:0.03417036752601732, data cost:0.2862820864732837 
2022-03-28 15:37:54,573: ============================================================
2022-03-28 15:37:54,573: Epoch 32/45 Batch 2600/7662 eta: 12:41:27.463350	Training Loss 0.4369 (0.4319)	Training Prec@1 91.406 (92.083)	Training Prec@5 95.312 (95.084)	
2022-03-28 15:37:54,573: ============================================================
2022-03-28 15:38:42,552: time cost, forward:0.11925125263407743, backward:0.03449226653589148, data cost:0.28519147897305513 
2022-03-28 15:38:42,552: ============================================================
2022-03-28 15:38:42,552: Epoch 32/45 Batch 2700/7662 eta: 13:56:11.603633	Training Loss 0.4303 (0.4319)	Training Prec@1 93.164 (92.079)	Training Prec@5 95.898 (95.080)	
2022-03-28 15:38:42,552: ============================================================
2022-03-28 15:39:26,651: time cost, forward:0.11995534924108159, backward:0.03451594125802877, data cost:0.28449531017179447 
2022-03-28 15:39:26,652: ============================================================
2022-03-28 15:39:26,652: Epoch 32/45 Batch 2800/7662 eta: 12:47:50.375088	Training Loss 0.4298 (0.4320)	Training Prec@1 92.383 (92.080)	Training Prec@5 94.922 (95.084)	
2022-03-28 15:39:26,652: ============================================================
2022-03-28 15:40:11,840: time cost, forward:0.12108052142861384, backward:0.034625318799606064, data cost:0.28370145001301234 
2022-03-28 15:40:11,840: ============================================================
2022-03-28 15:40:11,841: Epoch 32/45 Batch 2900/7662 eta: 13:06:03.049255	Training Loss 0.4291 (0.4320)	Training Prec@1 93.945 (92.079)	Training Prec@5 96.094 (95.080)	
2022-03-28 15:40:11,841: ============================================================
2022-03-28 15:40:54,596: time cost, forward:0.12129659953217539, backward:0.034602877138932175, data cost:0.2830875368267745 
2022-03-28 15:40:54,596: ============================================================
2022-03-28 15:40:54,597: Epoch 32/45 Batch 3000/7662 eta: 12:23:01.342184	Training Loss 0.4183 (0.4319)	Training Prec@1 92.773 (92.078)	Training Prec@5 95.703 (95.078)	
2022-03-28 15:40:54,597: ============================================================
2022-03-28 15:41:38,303: time cost, forward:0.12234762415804375, backward:0.03471986060374858, data cost:0.2818386787828148 
2022-03-28 15:41:38,303: ============================================================
2022-03-28 15:41:38,304: Epoch 32/45 Batch 3100/7662 eta: 12:38:49.039544	Training Loss 0.4313 (0.4320)	Training Prec@1 93.750 (92.074)	Training Prec@5 95.703 (95.075)	
2022-03-28 15:41:38,304: ============================================================
2022-03-28 15:42:21,681: time cost, forward:0.12185127282746325, backward:0.034713628777268156, data cost:0.28217173785334565 
2022-03-28 15:42:21,682: ============================================================
2022-03-28 15:42:21,682: Epoch 32/45 Batch 3200/7662 eta: 12:32:23.295488	Training Loss 0.4371 (0.4320)	Training Prec@1 91.211 (92.071)	Training Prec@5 95.703 (95.073)	
2022-03-28 15:42:21,682: ============================================================
2022-03-28 15:43:05,525: time cost, forward:0.12139733468593703, backward:0.03467851112957758, data cost:0.28263516164469193 
2022-03-28 15:43:05,526: ============================================================
2022-03-28 15:43:05,526: Epoch 32/45 Batch 3300/7662 eta: 12:39:44.506080	Training Loss 0.4402 (0.4320)	Training Prec@1 92.383 (92.075)	Training Prec@5 95.312 (95.073)	
2022-03-28 15:43:05,526: ============================================================
2022-03-28 15:43:49,494: time cost, forward:0.1216399579863227, backward:0.034648790959927504, data cost:0.28245595638525417 
2022-03-28 15:43:49,495: ============================================================
2022-03-28 15:43:49,495: Epoch 32/45 Batch 3400/7662 eta: 12:41:09.867753	Training Loss 0.4403 (0.4320)	Training Prec@1 88.281 (92.065)	Training Prec@5 93.750 (95.066)	
2022-03-28 15:43:49,495: ============================================================
2022-03-28 15:44:33,606: time cost, forward:0.1211932001198384, backward:0.03458851805820776, data cost:0.28289491164749025 
2022-03-28 15:44:33,606: ============================================================
2022-03-28 15:44:33,607: Epoch 32/45 Batch 3500/7662 eta: 12:42:54.238365	Training Loss 0.4260 (0.4320)	Training Prec@1 93.945 (92.071)	Training Prec@5 96.289 (95.068)	
2022-03-28 15:44:33,607: ============================================================
2022-03-28 15:45:17,662: time cost, forward:0.12076446545657332, backward:0.034538403058191176, data cost:0.2835095340790236 
2022-03-28 15:45:17,662: ============================================================
2022-03-28 15:45:17,663: Epoch 32/45 Batch 3600/7662 eta: 12:41:12.324418	Training Loss 0.4332 (0.4320)	Training Prec@1 92.383 (92.069)	Training Prec@5 95.898 (95.065)	
2022-03-28 15:45:17,663: ============================================================
2022-03-28 15:46:00,893: time cost, forward:0.12034195724388302, backward:0.03447839136734689, data cost:0.28380882504760463 
2022-03-28 15:46:00,893: ============================================================
2022-03-28 15:46:00,893: Epoch 32/45 Batch 3700/7662 eta: 12:26:13.777290	Training Loss 0.4284 (0.4320)	Training Prec@1 93.555 (92.067)	Training Prec@5 95.117 (95.065)	
2022-03-28 15:46:00,894: ============================================================
2022-03-28 15:46:44,443: time cost, forward:0.11994739544018221, backward:0.03441357982883519, data cost:0.2841807053132445 
2022-03-28 15:46:44,444: ============================================================
2022-03-28 15:46:44,444: Epoch 32/45 Batch 3800/7662 eta: 12:31:01.437220	Training Loss 0.4318 (0.4320)	Training Prec@1 91.406 (92.069)	Training Prec@5 94.727 (95.068)	
2022-03-28 15:46:44,444: ============================================================
2022-03-28 15:47:28,799: time cost, forward:0.11957429647139936, backward:0.03436887756498327, data cost:0.2847231569336634 
2022-03-28 15:47:28,799: ============================================================
2022-03-28 15:47:28,799: Epoch 32/45 Batch 3900/7662 eta: 12:44:09.633948	Training Loss 0.4345 (0.4320)	Training Prec@1 92.578 (92.066)	Training Prec@5 95.898 (95.064)	
2022-03-28 15:47:28,799: ============================================================
2022-03-28 15:48:12,045: time cost, forward:0.11924688170390833, backward:0.0343337428662204, data cost:0.2849233761582562 
2022-03-28 15:48:12,045: ============================================================
2022-03-28 15:48:12,045: Epoch 32/45 Batch 4000/7662 eta: 12:24:19.678801	Training Loss 0.4225 (0.4320)	Training Prec@1 92.188 (92.065)	Training Prec@5 94.922 (95.063)	
2022-03-28 15:48:12,046: ============================================================
2022-03-28 15:48:57,110: time cost, forward:0.11962951413303738, backward:0.03434854764768513, data cost:0.28479366303653536 
2022-03-28 15:48:57,110: ============================================================
2022-03-28 15:48:57,110: Epoch 32/45 Batch 4100/7662 eta: 12:54:53.288978	Training Loss 0.4281 (0.4320)	Training Prec@1 92.188 (92.062)	Training Prec@5 95.312 (95.062)	
2022-03-28 15:48:57,111: ============================================================
2022-03-28 15:49:40,889: time cost, forward:0.1192946035994266, backward:0.03432216783965534, data cost:0.2851358717127793 
2022-03-28 15:49:40,889: ============================================================
2022-03-28 15:49:40,890: Epoch 32/45 Batch 4200/7662 eta: 12:32:02.616873	Training Loss 0.4358 (0.4321)	Training Prec@1 90.820 (92.059)	Training Prec@5 94.922 (95.063)	
2022-03-28 15:49:40,890: ============================================================
2022-03-28 15:50:24,906: time cost, forward:0.11978891556361132, backward:0.034318162164291025, data cost:0.28466495482637094 
2022-03-28 15:50:24,906: ============================================================
2022-03-28 15:50:24,906: Epoch 32/45 Batch 4300/7662 eta: 12:35:23.470666	Training Loss 0.4403 (0.4320)	Training Prec@1 91.406 (92.063)	Training Prec@5 93.945 (95.065)	
2022-03-28 15:50:24,906: ============================================================
2022-03-28 15:51:08,211: time cost, forward:0.12005339847095556, backward:0.03434279166072248, data cost:0.2842319504351311 
2022-03-28 15:51:08,212: ============================================================
2022-03-28 15:51:08,213: Epoch 32/45 Batch 4400/7662 eta: 12:22:29.064243	Training Loss 0.4409 (0.4320)	Training Prec@1 90.234 (92.062)	Training Prec@5 93.359 (95.064)	
2022-03-28 15:51:08,213: ============================================================
2022-03-28 15:51:51,917: time cost, forward:0.11977090981833853, backward:0.03429307410864863, data cost:0.284517760540703 
2022-03-28 15:51:51,917: ============================================================
2022-03-28 15:51:51,918: Epoch 32/45 Batch 4500/7662 eta: 12:28:35.002216	Training Loss 0.4348 (0.4320)	Training Prec@1 88.672 (92.062)	Training Prec@5 91.992 (95.066)	
2022-03-28 15:51:51,918: ============================================================
2022-03-28 15:52:36,008: time cost, forward:0.11951773009577064, backward:0.03425593416595957, data cost:0.2848459290639036 
2022-03-28 15:52:36,009: ============================================================
2022-03-28 15:52:36,009: Epoch 32/45 Batch 4600/7662 eta: 12:34:28.487725	Training Loss 0.4323 (0.4320)	Training Prec@1 92.969 (92.061)	Training Prec@5 95.898 (95.067)	
2022-03-28 15:52:36,009: ============================================================
2022-03-28 15:53:19,908: time cost, forward:0.11938948138111778, backward:0.034299114760654385, data cost:0.2849478934718893 
2022-03-28 15:53:19,920: ============================================================
2022-03-28 15:53:19,920: Epoch 32/45 Batch 4700/7662 eta: 12:30:39.241116	Training Loss 0.4287 (0.4320)	Training Prec@1 91.992 (92.055)	Training Prec@5 95.117 (95.062)	
2022-03-28 15:53:19,921: ============================================================
2022-03-28 15:54:05,573: time cost, forward:0.11969366568430038, backward:0.034373305543906295, data cost:0.2849208154537251 
2022-03-28 15:54:05,574: ============================================================
2022-03-28 15:54:05,574: Epoch 32/45 Batch 4800/7662 eta: 12:59:40.824495	Training Loss 0.4264 (0.4320)	Training Prec@1 92.773 (92.055)	Training Prec@5 95.117 (95.062)	
2022-03-28 15:54:05,574: ============================================================
2022-03-28 15:54:49,716: time cost, forward:0.11951905358784246, backward:0.03437267334808206, data cost:0.2851340662681173 
2022-03-28 15:54:49,717: ============================================================
2022-03-28 15:54:49,717: Epoch 32/45 Batch 4900/7662 eta: 12:33:08.484939	Training Loss 0.4308 (0.4320)	Training Prec@1 91.016 (92.054)	Training Prec@5 93.945 (95.061)	
2022-03-28 15:54:49,717: ============================================================
2022-03-28 15:55:34,696: time cost, forward:0.11927785380264835, backward:0.034327453984143996, data cost:0.2856282009365893 
2022-03-28 15:55:34,697: ============================================================
2022-03-28 15:55:34,697: Epoch 32/45 Batch 5000/7662 eta: 12:46:40.877110	Training Loss 0.4287 (0.4320)	Training Prec@1 91.602 (92.057)	Training Prec@5 95.117 (95.064)	
2022-03-28 15:55:34,697: ============================================================
2022-03-28 15:56:19,413: time cost, forward:0.11945003756216213, backward:0.03435519331224901, data cost:0.285584996461541 
2022-03-28 15:56:19,414: ============================================================
2022-03-28 15:56:19,414: Epoch 32/45 Batch 5100/7662 eta: 12:41:26.556584	Training Loss 0.4146 (0.4320)	Training Prec@1 93.555 (92.058)	Training Prec@5 96.094 (95.064)	
2022-03-28 15:56:19,414: ============================================================
2022-03-28 15:57:01,234: time cost, forward:0.11927777635383936, backward:0.034339119228268385, data cost:0.28535846963160816 
2022-03-28 15:57:01,234: ============================================================
2022-03-28 15:57:01,235: Epoch 32/45 Batch 5200/7662 eta: 11:51:26.223784	Training Loss 0.4305 (0.4320)	Training Prec@1 94.531 (92.060)	Training Prec@5 96.680 (95.066)	
2022-03-28 15:57:01,235: ============================================================
2022-03-28 15:57:44,908: time cost, forward:0.11905305888162826, backward:0.03432150471095747, data cost:0.28555079900356795 
2022-03-28 15:57:44,908: ============================================================
2022-03-28 15:57:44,908: Epoch 32/45 Batch 5300/7662 eta: 12:22:13.329656	Training Loss 0.4412 (0.4320)	Training Prec@1 91.016 (92.062)	Training Prec@5 94.531 (95.068)	
2022-03-28 15:57:44,908: ============================================================
2022-03-28 15:58:30,297: time cost, forward:0.11957177141680278, backward:0.03436059292918511, data cost:0.28526905276727227 
2022-03-28 15:58:30,298: ============================================================
2022-03-28 15:58:30,298: Epoch 32/45 Batch 5400/7662 eta: 12:50:38.249829	Training Loss 0.4341 (0.4320)	Training Prec@1 92.188 (92.062)	Training Prec@5 94.531 (95.068)	
2022-03-28 15:58:30,298: ============================================================
2022-03-28 15:59:15,362: time cost, forward:0.12036443784206992, backward:0.03444948225026565, data cost:0.2845601043954375 
2022-03-28 15:59:15,363: ============================================================
2022-03-28 15:59:15,363: Epoch 32/45 Batch 5500/7662 eta: 12:44:22.122507	Training Loss 0.4276 (0.4320)	Training Prec@1 92.773 (92.063)	Training Prec@5 94.531 (95.070)	
2022-03-28 15:59:15,363: ============================================================
2022-03-28 15:59:58,098: time cost, forward:0.12013699842746992, backward:0.0343829869244605, data cost:0.2846468974275448 
2022-03-28 15:59:58,098: ============================================================
2022-03-28 15:59:58,099: Epoch 32/45 Batch 5600/7662 eta: 12:04:08.806497	Training Loss 0.4394 (0.4320)	Training Prec@1 91.797 (92.060)	Training Prec@5 93.359 (95.069)	
2022-03-28 15:59:58,099: ============================================================
2022-03-28 16:00:43,697: time cost, forward:0.12020371503674329, backward:0.034398841615266226, data cost:0.28484619155350976 
2022-03-28 16:00:43,698: ============================================================
2022-03-28 16:00:43,698: Epoch 32/45 Batch 5700/7662 eta: 12:51:54.693594	Training Loss 0.4278 (0.4320)	Training Prec@1 91.602 (92.063)	Training Prec@5 94.727 (95.069)	
2022-03-28 16:00:43,698: ============================================================
2022-03-28 16:01:26,803: time cost, forward:0.1199803888890266, backward:0.03436939147736085, data cost:0.2849121307952586 
2022-03-28 16:01:26,803: ============================================================
2022-03-28 16:01:26,803: Epoch 32/45 Batch 5800/7662 eta: 12:08:58.737438	Training Loss 0.4320 (0.4320)	Training Prec@1 92.773 (92.062)	Training Prec@5 95.312 (95.068)	
2022-03-28 16:01:26,804: ============================================================
2022-03-28 16:02:11,917: time cost, forward:0.12019696072372951, backward:0.03437303255970023, data cost:0.28493231020654375 
2022-03-28 16:02:11,918: ============================================================
2022-03-28 16:02:11,918: Epoch 32/45 Batch 5900/7662 eta: 12:42:12.318148	Training Loss 0.4312 (0.4320)	Training Prec@1 91.016 (92.068)	Training Prec@5 94.727 (95.070)	
2022-03-28 16:02:11,918: ============================================================
2022-03-28 16:02:57,646: time cost, forward:0.12092508334162871, backward:0.03441287068689559, data cost:0.2844514435858583 
2022-03-28 16:02:57,653: ============================================================
2022-03-28 16:02:57,653: Epoch 32/45 Batch 6000/7662 eta: 12:51:55.415192	Training Loss 0.4261 (0.4320)	Training Prec@1 91.602 (92.071)	Training Prec@5 94.141 (95.072)	
2022-03-28 16:02:57,653: ============================================================
2022-03-28 16:03:42,453: time cost, forward:0.12160087280692497, backward:0.034447015201601754, data cost:0.28385075078008215 
2022-03-28 16:03:42,454: ============================================================
2022-03-28 16:03:42,454: Epoch 32/45 Batch 6100/7662 eta: 12:35:24.812573	Training Loss 0.4323 (0.4320)	Training Prec@1 92.188 (92.068)	Training Prec@5 94.141 (95.068)	
2022-03-28 16:03:42,454: ============================================================
2022-03-28 16:04:27,382: time cost, forward:0.121781450295606, backward:0.03443696468486807, data cost:0.2838310643076262 
2022-03-28 16:04:27,382: ============================================================
2022-03-28 16:04:27,383: Epoch 32/45 Batch 6200/7662 eta: 12:36:48.939579	Training Loss 0.4283 (0.4320)	Training Prec@1 94.141 (92.067)	Training Prec@5 96.094 (95.067)	
2022-03-28 16:04:27,383: ============================================================
2022-03-28 16:05:10,610: time cost, forward:0.12195835247515194, backward:0.03442158390934798, data cost:0.2835405627249309 
2022-03-28 16:05:10,620: ============================================================
2022-03-28 16:05:10,620: Epoch 32/45 Batch 6300/7662 eta: 12:07:36.173786	Training Loss 0.4401 (0.4320)	Training Prec@1 91.602 (92.066)	Training Prec@5 93.945 (95.069)	
2022-03-28 16:05:10,620: ============================================================
2022-03-28 16:05:56,658: time cost, forward:0.12285859138821416, backward:0.03442276781621362, data cost:0.2829456992923589 
2022-03-28 16:05:56,658: ============================================================
2022-03-28 16:05:56,658: Epoch 32/45 Batch 6400/7662 eta: 12:53:58.475314	Training Loss 0.4399 (0.4320)	Training Prec@1 90.430 (92.062)	Training Prec@5 93.555 (95.066)	
2022-03-28 16:05:56,659: ============================================================
2022-03-28 16:06:41,199: time cost, forward:0.12312965496519306, backward:0.03445159290218045, data cost:0.2827088376855388 
2022-03-28 16:06:41,200: ============================================================
2022-03-28 16:06:41,200: Epoch 32/45 Batch 6500/7662 eta: 12:28:04.190482	Training Loss 0.4311 (0.4320)	Training Prec@1 91.016 (92.063)	Training Prec@5 93.555 (95.067)	
2022-03-28 16:06:41,200: ============================================================
2022-03-28 16:07:26,512: time cost, forward:0.12367926342665454, backward:0.03450061379282524, data cost:0.28230825433732526 
2022-03-28 16:07:26,512: ============================================================
2022-03-28 16:07:26,513: Epoch 32/45 Batch 6600/7662 eta: 12:40:15.692948	Training Loss 0.4272 (0.4321)	Training Prec@1 90.625 (92.064)	Training Prec@5 94.336 (95.069)	
2022-03-28 16:07:26,513: ============================================================
2022-03-28 16:08:13,546: time cost, forward:0.12416103708688885, backward:0.034502285881742895, data cost:0.2822572732615923 
2022-03-28 16:08:13,546: ============================================================
2022-03-28 16:08:13,547: Epoch 32/45 Batch 6700/7662 eta: 13:08:21.612350	Training Loss 0.4305 (0.4321)	Training Prec@1 91.016 (92.061)	Training Prec@5 94.727 (95.067)	
2022-03-28 16:08:13,547: ============================================================
2022-03-28 16:08:57,987: time cost, forward:0.12438030613925741, backward:0.034541866992883676, data cost:0.2820407532825069 
2022-03-28 16:08:57,988: ============================================================
2022-03-28 16:08:57,988: Epoch 32/45 Batch 6800/7662 eta: 12:24:09.642421	Training Loss 0.4376 (0.4321)	Training Prec@1 93.945 (92.062)	Training Prec@5 96.484 (95.068)	
2022-03-28 16:08:57,988: ============================================================
2022-03-28 16:09:43,773: time cost, forward:0.12495481679363171, backward:0.03459428182320761, data cost:0.28164747376807586 
2022-03-28 16:09:43,773: ============================================================
2022-03-28 16:09:43,774: Epoch 32/45 Batch 6900/7662 eta: 12:45:54.846259	Training Loss 0.4270 (0.4321)	Training Prec@1 93.945 (92.062)	Training Prec@5 96.094 (95.066)	
2022-03-28 16:09:43,774: ============================================================
2022-03-28 16:10:27,716: time cost, forward:0.12494413994741296, backward:0.034590025121168606, data cost:0.281631568445957 
2022-03-28 16:10:27,717: ============================================================
2022-03-28 16:10:27,717: Epoch 32/45 Batch 7000/7662 eta: 12:14:21.684868	Training Loss 0.4285 (0.4321)	Training Prec@1 94.141 (92.063)	Training Prec@5 97.266 (95.068)	
2022-03-28 16:10:27,718: ============================================================
2022-03-28 16:11:13,339: time cost, forward:0.12530989508542195, backward:0.03461851363686512, data cost:0.2814449333677763 
2022-03-28 16:11:13,340: ============================================================
2022-03-28 16:11:13,341: Epoch 32/45 Batch 7100/7662 eta: 12:41:40.370256	Training Loss 0.4414 (0.4321)	Training Prec@1 90.039 (92.062)	Training Prec@5 93.164 (95.067)	
2022-03-28 16:11:13,341: ============================================================
2022-03-28 16:11:57,516: time cost, forward:0.12519622686952828, backward:0.03461252349766613, data cost:0.2815480355769996 
2022-03-28 16:11:57,516: ============================================================
2022-03-28 16:11:57,517: Epoch 32/45 Batch 7200/7662 eta: 12:16:46.554565	Training Loss 0.4398 (0.4321)	Training Prec@1 91.406 (92.062)	Training Prec@5 95.117 (95.068)	
2022-03-28 16:11:57,517: ============================================================
2022-03-28 16:12:42,651: time cost, forward:0.12529502804760476, backward:0.03463011546434809, data cost:0.2815659092556762 
2022-03-28 16:12:42,651: ============================================================
2022-03-28 16:12:42,651: Epoch 32/45 Batch 7300/7662 eta: 12:32:00.898740	Training Loss 0.4308 (0.4321)	Training Prec@1 91.211 (92.062)	Training Prec@5 93.555 (95.067)	
2022-03-28 16:12:42,652: ============================================================
2022-03-28 16:13:27,932: time cost, forward:0.12519506119863297, backward:0.034653215379968884, data cost:0.2817888961706408 
2022-03-28 16:13:27,933: ============================================================
2022-03-28 16:13:27,933: Epoch 32/45 Batch 7400/7662 eta: 12:33:42.047520	Training Loss 0.4285 (0.4321)	Training Prec@1 94.922 (92.063)	Training Prec@5 97.461 (95.068)	
2022-03-28 16:13:27,933: ============================================================
2022-03-28 16:14:13,521: time cost, forward:0.12542169344299617, backward:0.03465287185284373, data cost:0.2817481069632221 
2022-03-28 16:14:13,521: ============================================================
2022-03-28 16:14:13,521: Epoch 32/45 Batch 7500/7662 eta: 12:38:03.149437	Training Loss 0.4399 (0.4321)	Training Prec@1 91.797 (92.065)	Training Prec@5 94.336 (95.067)	
2022-03-28 16:14:13,521: ============================================================
2022-03-28 16:15:00,151: time cost, forward:0.12596162050425153, backward:0.03470942725914374, data cost:0.2814697349584484 
2022-03-28 16:15:00,162: ============================================================
2022-03-28 16:15:00,163: Epoch 32/45 Batch 7600/7662 eta: 12:54:46.956707	Training Loss 0.4278 (0.4321)	Training Prec@1 91.602 (92.063)	Training Prec@5 94.922 (95.067)	
2022-03-28 16:15:00,163: ============================================================
2022-03-28 16:15:29,974: Epoch: 32/45 eta: 12:54:17.572663	Training Loss 0.4321 (0.4321)	Training Prec@1 90.820 (92.063)	Training Prec@5 93.750 (95.067)
2022-03-28 16:15:29,974: ============================================================
2022-03-28 16:15:29,975: Save Checkpoint...
2022-03-28 16:15:29,976: ============================================================
2022-03-28 16:15:32,050: Save done!
2022-03-28 16:15:32,050: ============================================================
2022-03-28 16:16:14,964: time cost, forward:0.10808925676827479, backward:0.033680672597403476, data cost:0.2883354196644793 
2022-03-28 16:16:14,964: ============================================================
2022-03-28 16:16:14,965: Epoch 33/45 Batch 100/7662 eta: 11:51:41.816945	Training Loss 0.4382 (0.4310)	Training Prec@1 91.211 (91.969)	Training Prec@5 94.727 (94.995)	
2022-03-28 16:16:14,965: ============================================================
2022-03-28 16:16:58,766: time cost, forward:0.11642823506839312, backward:0.03514358506130813, data cost:0.28258377822799297 
2022-03-28 16:16:58,766: ============================================================
2022-03-28 16:16:58,766: Epoch 33/45 Batch 200/7662 eta: 12:05:42.009360	Training Loss 0.4375 (0.4307)	Training Prec@1 90.625 (92.071)	Training Prec@5 94.336 (95.101)	
2022-03-28 16:16:58,767: ============================================================
2022-03-28 16:17:42,546: time cost, forward:0.1158015680153633, backward:0.034880349468626705, data cost:0.28446972569494344 
2022-03-28 16:17:42,546: ============================================================
2022-03-28 16:17:42,547: Epoch 33/45 Batch 300/7662 eta: 12:04:36.753971	Training Loss 0.4222 (0.4312)	Training Prec@1 92.383 (92.048)	Training Prec@5 95.312 (95.092)	
2022-03-28 16:17:42,547: ============================================================
2022-03-28 16:18:27,148: time cost, forward:0.12069866292757497, backward:0.03544172547514875, data cost:0.28165969095732035 
2022-03-28 16:18:27,148: ============================================================
2022-03-28 16:18:27,149: Epoch 33/45 Batch 400/7662 eta: 12:17:28.284851	Training Loss 0.4380 (0.4313)	Training Prec@1 90.430 (92.094)	Training Prec@5 94.336 (95.125)	
2022-03-28 16:18:27,149: ============================================================
2022-03-28 16:19:10,086: time cost, forward:0.1203548956012917, backward:0.035114723121474886, data cost:0.2804602558961612 
2022-03-28 16:19:10,087: ============================================================
2022-03-28 16:19:10,087: Epoch 33/45 Batch 500/7662 eta: 11:49:14.995561	Training Loss 0.4161 (0.4313)	Training Prec@1 93.164 (92.100)	Training Prec@5 95.117 (95.137)	
2022-03-28 16:19:10,087: ============================================================
2022-03-28 16:19:56,337: time cost, forward:0.12675607343746942, backward:0.03532134989068186, data cost:0.2780908487475973 
2022-03-28 16:19:56,338: ============================================================
2022-03-28 16:19:56,338: Epoch 33/45 Batch 600/7662 eta: 12:43:11.492863	Training Loss 0.4479 (0.4315)	Training Prec@1 90.820 (92.098)	Training Prec@5 94.141 (95.140)	
2022-03-28 16:19:56,338: ============================================================
2022-03-28 16:20:42,398: time cost, forward:0.13225280812199364, backward:0.035663111868163885, data cost:0.2749290773285305 
2022-03-28 16:20:42,398: ============================================================
2022-03-28 16:20:42,398: Epoch 33/45 Batch 700/7662 eta: 12:39:17.174191	Training Loss 0.4270 (0.4315)	Training Prec@1 91.211 (92.095)	Training Prec@5 95.117 (95.126)	
2022-03-28 16:20:42,399: ============================================================
2022-03-28 16:21:26,939: time cost, forward:0.13021812629938423, backward:0.035183849859894145, data cost:0.2778129392631063 
2022-03-28 16:21:26,940: ============================================================
2022-03-28 16:21:26,940: Epoch 33/45 Batch 800/7662 eta: 12:13:30.164755	Training Loss 0.4314 (0.4316)	Training Prec@1 93.359 (92.099)	Training Prec@5 95.508 (95.130)	
2022-03-28 16:21:26,940: ============================================================
2022-03-28 16:22:12,387: time cost, forward:0.13276483697010757, backward:0.03523798938322651, data cost:0.27632058528692227 
2022-03-28 16:22:12,387: ============================================================
2022-03-28 16:22:12,387: Epoch 33/45 Batch 900/7662 eta: 12:27:39.749678	Training Loss 0.4281 (0.4317)	Training Prec@1 93.359 (92.071)	Training Prec@5 96.484 (95.102)	
2022-03-28 16:22:12,387: ============================================================
2022-03-28 16:22:56,475: time cost, forward:0.13187276302753864, backward:0.035104082869337846, data cost:0.2769698356842255 
2022-03-28 16:22:56,475: ============================================================
2022-03-28 16:22:56,475: Epoch 33/45 Batch 1000/7662 eta: 12:04:33.984227	Training Loss 0.4327 (0.4318)	Training Prec@1 91.602 (92.065)	Training Prec@5 94.531 (95.086)	
2022-03-28 16:22:56,476: ============================================================
2022-03-28 16:23:45,023: time cost, forward:0.1349304403577532, backward:0.03548129739492345, data cost:0.2773009227339629 
2022-03-28 16:23:45,024: ============================================================
2022-03-28 16:23:45,024: Epoch 33/45 Batch 1100/7662 eta: 13:17:03.725677	Training Loss 0.4303 (0.4317)	Training Prec@1 91.406 (92.078)	Training Prec@5 94.531 (95.092)	
2022-03-28 16:23:45,024: ============================================================
2022-03-28 16:24:27,738: time cost, forward:0.1348603740148091, backward:0.035441115262410956, data cost:0.27563696070648014 
2022-03-28 16:24:27,739: ============================================================
2022-03-28 16:24:27,739: Epoch 33/45 Batch 1200/7662 eta: 11:40:34.695665	Training Loss 0.4342 (0.4317)	Training Prec@1 91.602 (92.070)	Training Prec@5 94.922 (95.090)	
2022-03-28 16:24:27,739: ============================================================
2022-03-28 16:25:12,279: time cost, forward:0.13463334949132202, backward:0.03545983742529654, data cost:0.27574742014725634 
2022-03-28 16:25:12,280: ============================================================
2022-03-28 16:25:12,280: Epoch 33/45 Batch 1300/7662 eta: 12:09:46.708177	Training Loss 0.4369 (0.4317)	Training Prec@1 92.188 (92.071)	Training Prec@5 95.312 (95.088)	
2022-03-28 16:25:12,280: ============================================================
2022-03-28 16:25:57,295: time cost, forward:0.1357599158556313, backward:0.03548272053116641, data cost:0.2748264819916186 
2022-03-28 16:25:57,295: ============================================================
2022-03-28 16:25:57,295: Epoch 33/45 Batch 1400/7662 eta: 12:16:48.423898	Training Loss 0.4449 (0.4318)	Training Prec@1 91.992 (92.067)	Training Prec@5 94.336 (95.090)	
2022-03-28 16:25:57,296: ============================================================
2022-03-28 16:26:43,259: time cost, forward:0.13696644256876817, backward:0.03580094783444497, data cost:0.27422295672802866 
2022-03-28 16:26:43,259: ============================================================
2022-03-28 16:26:43,260: Epoch 33/45 Batch 1500/7662 eta: 12:31:34.099351	Training Loss 0.4396 (0.4318)	Training Prec@1 92.969 (92.070)	Training Prec@5 96.289 (95.089)	
2022-03-28 16:26:43,260: ============================================================
2022-03-28 16:27:28,477: time cost, forward:0.1383127947014671, backward:0.03605411215228092, data cost:0.2728924110131088 
2022-03-28 16:27:28,477: ============================================================
2022-03-28 16:27:28,478: Epoch 33/45 Batch 1600/7662 eta: 12:18:37.257342	Training Loss 0.4363 (0.4318)	Training Prec@1 92.969 (92.063)	Training Prec@5 95.703 (95.084)	
2022-03-28 16:27:28,479: ============================================================
2022-03-28 16:28:12,104: time cost, forward:0.13728813454289518, backward:0.035986818096931016, data cost:0.2733259934969268 
2022-03-28 16:28:12,105: ============================================================
2022-03-28 16:28:12,106: Epoch 33/45 Batch 1700/7662 eta: 11:51:54.204444	Training Loss 0.4429 (0.4319)	Training Prec@1 90.039 (92.054)	Training Prec@5 94.531 (95.072)	
2022-03-28 16:28:12,106: ============================================================
2022-03-28 16:28:58,534: time cost, forward:0.1392851600784802, backward:0.03616813290708922, data cost:0.27206758222956334 
2022-03-28 16:28:58,534: ============================================================
2022-03-28 16:28:58,535: Epoch 33/45 Batch 1800/7662 eta: 12:36:51.052818	Training Loss 0.4330 (0.4319)	Training Prec@1 93.164 (92.055)	Training Prec@5 95.508 (95.071)	
2022-03-28 16:28:58,535: ============================================================
2022-03-28 16:29:44,795: time cost, forward:0.1403030667448119, backward:0.03620994448850128, data cost:0.2717793938233515 
2022-03-28 16:29:44,796: ============================================================
2022-03-28 16:29:44,796: Epoch 33/45 Batch 1900/7662 eta: 12:33:20.462784	Training Loss 0.4496 (0.4319)	Training Prec@1 92.383 (92.062)	Training Prec@5 95.117 (95.071)	
2022-03-28 16:29:44,796: ============================================================
2022-03-28 16:30:29,977: time cost, forward:0.1401585491375067, backward:0.036146238125700426, data cost:0.2721229644344114 
2022-03-28 16:30:29,977: ============================================================
2022-03-28 16:30:29,977: Epoch 33/45 Batch 2000/7662 eta: 12:15:00.016005	Training Loss 0.4394 (0.4319)	Training Prec@1 92.188 (92.066)	Training Prec@5 94.727 (95.076)	
2022-03-28 16:30:29,977: ============================================================
2022-03-28 16:31:14,456: time cost, forward:0.14009540247996685, backward:0.03602076633138961, data cost:0.2721310178684927 
2022-03-28 16:31:14,457: ============================================================
2022-03-28 16:31:14,457: Epoch 33/45 Batch 2100/7662 eta: 12:02:50.779320	Training Loss 0.4228 (0.4319)	Training Prec@1 92.188 (92.063)	Training Prec@5 95.117 (95.075)	
2022-03-28 16:31:14,457: ============================================================
2022-03-28 16:31:59,124: time cost, forward:0.13974788840546723, backward:0.03593079596012492, data cost:0.27249459647872115 
2022-03-28 16:31:59,125: ============================================================
2022-03-28 16:31:59,125: Epoch 33/45 Batch 2200/7662 eta: 12:05:09.884383	Training Loss 0.4219 (0.4320)	Training Prec@1 92.773 (92.054)	Training Prec@5 96.289 (95.069)	
2022-03-28 16:31:59,125: ============================================================
2022-03-28 16:32:43,261: time cost, forward:0.13867814025447492, backward:0.035840773800239714, data cost:0.27329325572259017 
2022-03-28 16:32:43,261: ============================================================
2022-03-28 16:32:43,261: Epoch 33/45 Batch 2300/7662 eta: 11:55:47.739685	Training Loss 0.4233 (0.4319)	Training Prec@1 92.383 (92.058)	Training Prec@5 95.703 (95.075)	
2022-03-28 16:32:43,262: ============================================================
2022-03-28 16:33:27,429: time cost, forward:0.13744627947407795, backward:0.03569656026616798, data cost:0.27438228763406997 
2022-03-28 16:33:27,429: ============================================================
2022-03-28 16:33:27,429: Epoch 33/45 Batch 2400/7662 eta: 11:55:34.483460	Training Loss 0.4279 (0.4320)	Training Prec@1 91.797 (92.052)	Training Prec@5 95.312 (95.073)	
2022-03-28 16:33:27,430: ============================================================
2022-03-28 16:34:11,524: time cost, forward:0.13619575084519892, backward:0.035309869511311605, data cost:0.2757612603719161 
2022-03-28 16:34:11,524: ============================================================
2022-03-28 16:34:11,525: Epoch 33/45 Batch 2500/7662 eta: 11:53:39.805138	Training Loss 0.4397 (0.4320)	Training Prec@1 91.406 (92.049)	Training Prec@5 95.117 (95.070)	
2022-03-28 16:34:11,525: ============================================================
2022-03-28 16:34:56,058: time cost, forward:0.13505794691002887, backward:0.03525317921552258, data cost:0.2768556812076488 
2022-03-28 16:34:56,058: ============================================================
2022-03-28 16:34:56,058: Epoch 33/45 Batch 2600/7662 eta: 12:00:00.533666	Training Loss 0.4364 (0.4320)	Training Prec@1 92.578 (92.053)	Training Prec@5 96.484 (95.068)	
2022-03-28 16:34:56,059: ============================================================
2022-03-28 16:35:40,115: time cost, forward:0.13397375103631431, backward:0.03514912897854127, data cost:0.2777749032786971 
2022-03-28 16:35:40,115: ============================================================
2022-03-28 16:35:40,116: Epoch 33/45 Batch 2700/7662 eta: 11:51:34.599481	Training Loss 0.4262 (0.4319)	Training Prec@1 91.797 (92.056)	Training Prec@5 95.312 (95.071)	
2022-03-28 16:35:40,116: ============================================================
2022-03-28 16:36:23,409: time cost, forward:0.13298325983274403, backward:0.03505238723482307, data cost:0.2783515521482554 
2022-03-28 16:36:23,409: ============================================================
2022-03-28 16:36:23,410: Epoch 33/45 Batch 2800/7662 eta: 11:38:31.657576	Training Loss 0.4183 (0.4319)	Training Prec@1 93.945 (92.053)	Training Prec@5 97.266 (95.070)	
2022-03-28 16:36:23,410: ============================================================
2022-03-28 16:37:09,084: time cost, forward:0.13353506693884437, backward:0.03501728922385683, data cost:0.2781937560693031 
2022-03-28 16:37:09,096: ============================================================
2022-03-28 16:37:09,096: Epoch 33/45 Batch 2900/7662 eta: 12:16:21.679329	Training Loss 0.4380 (0.4319)	Training Prec@1 92.383 (92.066)	Training Prec@5 96.289 (95.078)	
2022-03-28 16:37:09,096: ============================================================
2022-03-28 16:37:51,701: time cost, forward:0.13283109132271284, backward:0.03487002400407476, data cost:0.27831336210949814 
2022-03-28 16:37:51,702: ============================================================
2022-03-28 16:37:51,702: Epoch 33/45 Batch 3000/7662 eta: 11:26:00.500673	Training Loss 0.4367 (0.4319)	Training Prec@1 92.969 (92.066)	Training Prec@5 96.289 (95.081)	
2022-03-28 16:37:51,702: ============================================================
2022-03-28 16:38:36,482: time cost, forward:0.13254800439842596, backward:0.034813670813556484, data cost:0.2786947184510676 
2022-03-28 16:38:36,482: ============================================================
2022-03-28 16:38:36,483: Epoch 33/45 Batch 3100/7662 eta: 12:00:16.620332	Training Loss 0.4296 (0.4319)	Training Prec@1 93.164 (92.068)	Training Prec@5 95.703 (95.082)	
2022-03-28 16:38:36,483: ============================================================
2022-03-28 16:39:19,367: time cost, forward:0.1317427967741699, backward:0.03477957227074306, data cost:0.2789869529823096 
2022-03-28 16:39:19,368: ============================================================
2022-03-28 16:39:19,368: Epoch 33/45 Batch 3200/7662 eta: 11:29:04.224917	Training Loss 0.4354 (0.4319)	Training Prec@1 91.016 (92.069)	Training Prec@5 93.945 (95.082)	
2022-03-28 16:39:19,368: ============================================================
2022-03-28 16:40:03,777: time cost, forward:0.1309720037488657, backward:0.03468882130579213, data cost:0.27980024889043764 
2022-03-28 16:40:03,778: ============================================================
2022-03-28 16:40:03,778: Epoch 33/45 Batch 3300/7662 eta: 11:52:49.815250	Training Loss 0.4211 (0.4319)	Training Prec@1 92.773 (92.070)	Training Prec@5 95.312 (95.083)	
2022-03-28 16:40:03,778: ============================================================
2022-03-28 16:40:47,759: time cost, forward:0.13069871167078828, backward:0.03473572122170386, data cost:0.2798140969266608 
2022-03-28 16:40:47,770: ============================================================
2022-03-28 16:40:47,770: Epoch 33/45 Batch 3400/7662 eta: 11:45:23.742053	Training Loss 0.4256 (0.4319)	Training Prec@1 93.750 (92.072)	Training Prec@5 95.898 (95.082)	
2022-03-28 16:40:47,770: ============================================================
2022-03-28 16:41:33,700: time cost, forward:0.13139824418893506, backward:0.03485541399562996, data cost:0.27941083144923556 
2022-03-28 16:41:33,700: ============================================================
2022-03-28 16:41:33,700: Epoch 33/45 Batch 3500/7662 eta: 12:15:41.980222	Training Loss 0.4263 (0.4318)	Training Prec@1 91.016 (92.074)	Training Prec@5 94.141 (95.085)	
2022-03-28 16:41:33,700: ============================================================
2022-03-28 16:42:19,391: time cost, forward:0.13186235188046705, backward:0.034883355723383955, data cost:0.2792088807904412 
2022-03-28 16:42:19,392: ============================================================
2022-03-28 16:42:19,392: Epoch 33/45 Batch 3600/7662 eta: 12:11:07.511489	Training Loss 0.4418 (0.4318)	Training Prec@1 91.602 (92.075)	Training Prec@5 95.703 (95.086)	
2022-03-28 16:42:19,392: ============================================================
2022-03-28 16:43:07,268: time cost, forward:0.13312720453200969, backward:0.03506472704377681, data cost:0.27864259196217234 
2022-03-28 16:43:07,269: ============================================================
2022-03-28 16:43:07,269: Epoch 33/45 Batch 3700/7662 eta: 12:45:17.032053	Training Loss 0.4364 (0.4318)	Training Prec@1 91.211 (92.077)	Training Prec@5 95.508 (95.084)	
2022-03-28 16:43:07,269: ============================================================
2022-03-28 16:43:49,905: time cost, forward:0.13254631811394507, backward:0.03500644612795555, data cost:0.27870676667478783 
2022-03-28 16:43:49,906: ============================================================
2022-03-28 16:43:49,906: Epoch 33/45 Batch 3800/7662 eta: 11:20:49.258766	Training Loss 0.4205 (0.4318)	Training Prec@1 93.750 (92.073)	Training Prec@5 95.703 (95.080)	
2022-03-28 16:43:49,906: ============================================================
2022-03-28 16:44:34,758: time cost, forward:0.13267310450217087, backward:0.0349856633716866, data cost:0.27866997343358335 
2022-03-28 16:44:34,769: ============================================================
2022-03-28 16:44:34,770: Epoch 33/45 Batch 3900/7662 eta: 11:55:37.772925	Training Loss 0.4336 (0.4318)	Training Prec@1 92.188 (92.077)	Training Prec@5 94.727 (95.082)	
2022-03-28 16:44:34,770: ============================================================
2022-03-28 16:45:19,574: time cost, forward:0.13286781561437264, backward:0.03501981650331253, data cost:0.27837355907752115 
2022-03-28 16:45:19,574: ============================================================
2022-03-28 16:45:19,574: Epoch 33/45 Batch 4000/7662 eta: 11:53:56.331643	Training Loss 0.4300 (0.4318)	Training Prec@1 91.016 (92.075)	Training Prec@5 95.117 (95.082)	
2022-03-28 16:45:19,574: ============================================================
2022-03-28 16:46:03,127: time cost, forward:0.1326783360664831, backward:0.03500633182744335, data cost:0.27837176804543706 
2022-03-28 16:46:03,127: ============================================================
2022-03-28 16:46:03,128: Epoch 33/45 Batch 4100/7662 eta: 11:33:16.575605	Training Loss 0.4372 (0.4318)	Training Prec@1 92.383 (92.075)	Training Prec@5 95.312 (95.082)	
2022-03-28 16:46:03,128: ============================================================
2022-03-28 16:46:48,022: time cost, forward:0.13259278578143882, backward:0.03509769992732979, data cost:0.27843421423653814 
2022-03-28 16:46:48,023: ============================================================
2022-03-28 16:46:48,023: Epoch 33/45 Batch 4200/7662 eta: 11:53:53.307692	Training Loss 0.4325 (0.4318)	Training Prec@1 91.797 (92.075)	Training Prec@5 95.117 (95.083)	
2022-03-28 16:46:48,023: ============================================================
2022-03-28 16:47:31,877: time cost, forward:0.13196740873638046, backward:0.03505521670250761, data cost:0.27884082767569096 
2022-03-28 16:47:31,878: ============================================================
2022-03-28 16:47:31,878: Epoch 33/45 Batch 4300/7662 eta: 11:36:36.696495	Training Loss 0.4384 (0.4318)	Training Prec@1 91.211 (92.073)	Training Prec@5 95.312 (95.082)	
2022-03-28 16:47:31,878: ============================================================
2022-03-28 16:48:15,798: time cost, forward:0.1315669075796132, backward:0.03503548846946139, data cost:0.279171379513837 
2022-03-28 16:48:15,799: ============================================================
2022-03-28 16:48:15,800: Epoch 33/45 Batch 4400/7662 eta: 11:36:56.766494	Training Loss 0.4288 (0.4319)	Training Prec@1 94.336 (92.077)	Training Prec@5 96.289 (95.086)	
2022-03-28 16:48:15,800: ============================================================
2022-03-28 16:49:01,874: time cost, forward:0.13210408394536485, backward:0.03506327851874798, data cost:0.2789109658336449 
2022-03-28 16:49:01,874: ============================================================
2022-03-28 16:49:01,875: Epoch 33/45 Batch 4500/7662 eta: 12:10:20.355741	Training Loss 0.4383 (0.4319)	Training Prec@1 91.602 (92.079)	Training Prec@5 94.531 (95.087)	
2022-03-28 16:49:01,875: ============================================================
2022-03-28 16:49:45,511: time cost, forward:0.13177992852259315, backward:0.03501984481579274, data cost:0.2790704313685879 
2022-03-28 16:49:45,511: ============================================================
2022-03-28 16:49:45,512: Epoch 33/45 Batch 4600/7662 eta: 11:30:58.166162	Training Loss 0.4311 (0.4319)	Training Prec@1 91.992 (92.078)	Training Prec@5 94.336 (95.085)	
2022-03-28 16:49:45,512: ============================================================
2022-03-28 16:50:30,051: time cost, forward:0.13124495796711808, backward:0.034971948785207506, data cost:0.2796340170858464 
2022-03-28 16:50:30,052: ============================================================
2022-03-28 16:50:30,052: Epoch 33/45 Batch 4700/7662 eta: 11:44:32.011761	Training Loss 0.4250 (0.4319)	Training Prec@1 92.969 (92.079)	Training Prec@5 95.703 (95.086)	
2022-03-28 16:50:30,052: ============================================================
2022-03-28 16:51:16,992: time cost, forward:0.13179802367974877, backward:0.03496952547731934, data cost:0.2795318284365604 
2022-03-28 16:51:16,992: ============================================================
2022-03-28 16:51:16,993: Epoch 33/45 Batch 4800/7662 eta: 12:21:43.227275	Training Loss 0.4346 (0.4319)	Training Prec@1 92.188 (92.076)	Training Prec@5 95.117 (95.084)	
2022-03-28 16:51:16,993: ============================================================
2022-03-28 16:52:03,078: time cost, forward:0.13206849333752804, backward:0.03495471042914156, data cost:0.27958732961416 
2022-03-28 16:52:03,090: ============================================================
2022-03-28 16:52:03,090: Epoch 33/45 Batch 4900/7662 eta: 12:07:37.496634	Training Loss 0.4263 (0.4319)	Training Prec@1 91.211 (92.075)	Training Prec@5 93.750 (95.086)	
2022-03-28 16:52:03,090: ============================================================
2022-03-28 16:52:47,250: time cost, forward:0.13238682210815028, backward:0.03497385167913405, data cost:0.27911324724242026 
2022-03-28 16:52:47,250: ============================================================
2022-03-28 16:52:47,250: Epoch 33/45 Batch 5000/7662 eta: 11:36:18.592031	Training Loss 0.4361 (0.4319)	Training Prec@1 90.625 (92.077)	Training Prec@5 92.773 (95.086)	
2022-03-28 16:52:47,251: ============================================================
2022-03-28 16:53:31,959: time cost, forward:0.13205881538660438, backward:0.03497308776714821, data cost:0.2794524368806829 
2022-03-28 16:53:31,960: ============================================================
2022-03-28 16:53:31,960: Epoch 33/45 Batch 5100/7662 eta: 11:44:13.567804	Training Loss 0.4457 (0.4319)	Training Prec@1 90.625 (92.078)	Training Prec@5 94.922 (95.085)	
2022-03-28 16:53:31,960: ============================================================
2022-03-28 16:54:16,039: time cost, forward:0.13164929074263385, backward:0.034940350168048015, data cost:0.27978012313703365 
2022-03-28 16:54:16,039: ============================================================
2022-03-28 16:54:16,040: Epoch 33/45 Batch 5200/7662 eta: 11:33:34.291761	Training Loss 0.4326 (0.4319)	Training Prec@1 92.969 (92.080)	Training Prec@5 96.484 (95.087)	
2022-03-28 16:54:16,040: ============================================================
2022-03-28 16:55:01,767: time cost, forward:0.13139987374323903, backward:0.034936483906538765, data cost:0.28023293855663156 
2022-03-28 16:55:01,767: ============================================================
2022-03-28 16:55:01,767: Epoch 33/45 Batch 5300/7662 eta: 11:58:44.397076	Training Loss 0.4234 (0.4319)	Training Prec@1 92.969 (92.076)	Training Prec@5 94.922 (95.085)	
2022-03-28 16:55:01,767: ============================================================
2022-03-28 16:55:46,263: time cost, forward:0.13130359058800528, backward:0.03496536146073148, data cost:0.280273736064181 
2022-03-28 16:55:46,264: ============================================================
2022-03-28 16:55:46,264: Epoch 33/45 Batch 5400/7662 eta: 11:38:39.223859	Training Loss 0.4291 (0.4319)	Training Prec@1 92.969 (92.079)	Training Prec@5 95.508 (95.087)	
2022-03-28 16:55:46,264: ============================================================
2022-03-28 16:56:32,493: time cost, forward:0.13183889676926677, backward:0.03503454319020362, data cost:0.279936823330699 
2022-03-28 16:56:32,493: ============================================================
2022-03-28 16:56:32,493: Epoch 33/45 Batch 5500/7662 eta: 12:05:04.865347	Training Loss 0.4270 (0.4319)	Training Prec@1 91.602 (92.080)	Training Prec@5 95.508 (95.088)	
2022-03-28 16:56:32,493: ============================================================
2022-03-28 16:57:15,293: time cost, forward:0.13143144260242295, backward:0.03501231735530635, data cost:0.28000457109266313 
2022-03-28 16:57:15,293: ============================================================
2022-03-28 16:57:15,294: Epoch 33/45 Batch 5600/7662 eta: 11:10:35.271859	Training Loss 0.4461 (0.4319)	Training Prec@1 91.406 (92.083)	Training Prec@5 95.508 (95.090)	
2022-03-28 16:57:15,294: ============================================================
2022-03-28 16:57:59,454: time cost, forward:0.1310146360318689, backward:0.03499983210044067, data cost:0.2803446337473478 
2022-03-28 16:57:59,455: ============================================================
2022-03-28 16:57:59,455: Epoch 33/45 Batch 5700/7662 eta: 11:31:10.587089	Training Loss 0.4249 (0.4319)	Training Prec@1 92.773 (92.079)	Training Prec@5 95.508 (95.085)	
2022-03-28 16:57:59,455: ============================================================
2022-03-28 16:58:45,923: time cost, forward:0.13139804710004016, backward:0.034994779855840305, data cost:0.28028378924083497 
2022-03-28 16:58:45,923: ============================================================
2022-03-28 16:58:45,923: Epoch 33/45 Batch 5800/7662 eta: 12:06:30.672599	Training Loss 0.4254 (0.4319)	Training Prec@1 92.773 (92.073)	Training Prec@5 95.508 (95.081)	
2022-03-28 16:58:45,924: ============================================================
2022-03-28 16:59:30,751: time cost, forward:0.13175726866475565, backward:0.034991526769407526, data cost:0.27995008944575916 
2022-03-28 16:59:30,752: ============================================================
2022-03-28 16:59:30,752: Epoch 33/45 Batch 5900/7662 eta: 11:40:07.600480	Training Loss 0.4284 (0.4319)	Training Prec@1 92.383 (92.070)	Training Prec@5 95.508 (95.078)	
2022-03-28 16:59:30,752: ============================================================
2022-03-28 17:00:14,985: time cost, forward:0.1316855303107311, backward:0.03495777768082451, data cost:0.2799546782186298 
2022-03-28 17:00:14,986: ============================================================
2022-03-28 17:00:14,986: Epoch 33/45 Batch 6000/7662 eta: 11:30:05.932212	Training Loss 0.4255 (0.4319)	Training Prec@1 92.773 (92.073)	Training Prec@5 94.922 (95.079)	
2022-03-28 17:00:14,986: ============================================================
2022-03-28 17:01:01,979: time cost, forward:0.13171924800438106, backward:0.034959811424696945, data cost:0.2802962407770265 
2022-03-28 17:01:01,979: ============================================================
2022-03-28 17:01:01,980: Epoch 33/45 Batch 6100/7662 eta: 12:12:22.424945	Training Loss 0.4417 (0.4319)	Training Prec@1 91.797 (92.070)	Training Prec@5 93.945 (95.077)	
2022-03-28 17:01:01,980: ============================================================
2022-03-28 17:01:47,733: time cost, forward:0.1319271963168429, backward:0.035014202756677104, data cost:0.28020720994170584 
2022-03-28 17:01:47,734: ============================================================
2022-03-28 17:01:47,734: Epoch 33/45 Batch 6200/7662 eta: 11:52:17.845381	Training Loss 0.4386 (0.4319)	Training Prec@1 93.945 (92.071)	Training Prec@5 96.094 (95.076)	
2022-03-28 17:01:47,734: ============================================================
2022-03-28 17:02:33,804: time cost, forward:0.1321351650045455, backward:0.03502076185777237, data cost:0.28019205421318155 
2022-03-28 17:02:33,814: ============================================================
2022-03-28 17:02:33,814: Epoch 33/45 Batch 6300/7662 eta: 11:56:36.106677	Training Loss 0.4223 (0.4320)	Training Prec@1 93.945 (92.072)	Training Prec@5 97.070 (95.077)	
2022-03-28 17:02:33,814: ============================================================
2022-03-28 17:03:22,101: time cost, forward:0.13310777703231713, backward:0.03507735647620922, data cost:0.2797097017437541 
2022-03-28 17:03:22,102: ============================================================
2022-03-28 17:03:22,102: Epoch 33/45 Batch 6400/7662 eta: 12:30:07.412036	Training Loss 0.4320 (0.4320)	Training Prec@1 91.797 (92.072)	Training Prec@5 93.945 (95.076)	
2022-03-28 17:03:22,102: ============================================================
2022-03-28 17:04:04,142: time cost, forward:0.13274802026574034, backward:0.035059892245303086, data cost:0.2796435414103109 
2022-03-28 17:04:04,142: ============================================================
2022-03-28 17:04:04,143: Epoch 33/45 Batch 6500/7662 eta: 10:52:22.746605	Training Loss 0.4327 (0.4320)	Training Prec@1 91.602 (92.074)	Training Prec@5 94.727 (95.076)	
2022-03-28 17:04:04,143: ============================================================
2022-03-28 17:04:49,597: time cost, forward:0.13279858331352243, backward:0.035042300118083465, data cost:0.27970749422499114 
2022-03-28 17:04:49,598: ============================================================
2022-03-28 17:04:49,598: Epoch 33/45 Batch 6600/7662 eta: 11:44:36.516715	Training Loss 0.4359 (0.4320)	Training Prec@1 91.992 (92.072)	Training Prec@5 94.922 (95.074)	
2022-03-28 17:04:49,598: ============================================================
2022-03-28 17:05:33,266: time cost, forward:0.13242763788918271, backward:0.03500032759474398, data cost:0.2799595846918416 
2022-03-28 17:05:33,266: ============================================================
2022-03-28 17:05:33,266: Epoch 33/45 Batch 6700/7662 eta: 11:16:11.157040	Training Loss 0.4321 (0.4320)	Training Prec@1 92.188 (92.074)	Training Prec@5 94.336 (95.077)	
2022-03-28 17:05:33,267: ============================================================
2022-03-28 17:06:19,351: time cost, forward:0.13252219762464082, backward:0.03501131934687607, data cost:0.280038470362088 
2022-03-28 17:06:19,351: ============================================================
2022-03-28 17:06:19,352: Epoch 33/45 Batch 6800/7662 eta: 11:52:50.408903	Training Loss 0.4257 (0.4320)	Training Prec@1 93.164 (92.075)	Training Prec@5 96.875 (95.077)	
2022-03-28 17:06:19,352: ============================================================
2022-03-28 17:07:04,543: time cost, forward:0.13254886679864306, backward:0.035027039985861326, data cost:0.2800502018542165 
2022-03-28 17:07:04,543: ============================================================
2022-03-28 17:07:04,543: Epoch 33/45 Batch 6900/7662 eta: 11:38:15.853609	Training Loss 0.4410 (0.4320)	Training Prec@1 91.992 (92.075)	Training Prec@5 94.531 (95.076)	
2022-03-28 17:07:04,543: ============================================================
2022-03-28 17:07:49,451: time cost, forward:0.13278245121977264, backward:0.03504374702073179, data cost:0.2798201007695858 
2022-03-28 17:07:49,452: ============================================================
2022-03-28 17:07:49,452: Epoch 33/45 Batch 7000/7662 eta: 11:33:08.609072	Training Loss 0.4384 (0.4320)	Training Prec@1 91.406 (92.074)	Training Prec@5 94.336 (95.076)	
2022-03-28 17:07:49,452: ============================================================
2022-03-28 17:08:35,924: time cost, forward:0.13326177430465233, backward:0.03508901209844, data cost:0.27952662406294826 
2022-03-28 17:08:35,925: ============================================================
2022-03-28 17:08:35,925: Epoch 33/45 Batch 7100/7662 eta: 11:56:30.640971	Training Loss 0.4293 (0.4320)	Training Prec@1 92.188 (92.072)	Training Prec@5 94.531 (95.076)	
2022-03-28 17:08:35,925: ============================================================
2022-03-28 17:09:20,211: time cost, forward:0.13312377219631336, backward:0.03508567803434935, data cost:0.27956116697923694 
2022-03-28 17:09:20,211: ============================================================
2022-03-28 17:09:20,212: Epoch 33/45 Batch 7200/7662 eta: 11:22:04.076973	Training Loss 0.4350 (0.4320)	Training Prec@1 92.578 (92.069)	Training Prec@5 94.727 (95.074)	
2022-03-28 17:09:20,212: ============================================================
2022-03-28 17:10:05,949: time cost, forward:0.1330458573697936, backward:0.035073489354953226, data cost:0.2798049601157539 
2022-03-28 17:10:05,950: ============================================================
2022-03-28 17:10:05,950: Epoch 33/45 Batch 7300/7662 eta: 11:43:39.462733	Training Loss 0.4387 (0.4320)	Training Prec@1 90.234 (92.068)	Training Prec@5 94.141 (95.074)	
2022-03-28 17:10:05,950: ============================================================
2022-03-28 17:10:50,745: time cost, forward:0.13324509850868196, backward:0.035098014030606056, data cost:0.2795636000093825 
2022-03-28 17:10:50,746: ============================================================
2022-03-28 17:10:50,746: Epoch 33/45 Batch 7400/7662 eta: 11:28:25.439256	Training Loss 0.4324 (0.4320)	Training Prec@1 94.336 (92.066)	Training Prec@5 96.289 (95.073)	
2022-03-28 17:10:50,746: ============================================================
2022-03-28 17:11:34,849: time cost, forward:0.13305949939061903, backward:0.03508016805296533, data cost:0.27966999800972403 
2022-03-28 17:11:34,849: ============================================================
2022-03-28 17:11:34,849: Epoch 33/45 Batch 7500/7662 eta: 11:17:02.097054	Training Loss 0.4233 (0.4320)	Training Prec@1 93.945 (92.067)	Training Prec@5 95.703 (95.074)	
2022-03-28 17:11:34,849: ============================================================
2022-03-28 17:12:19,154: time cost, forward:0.13277144842452793, backward:0.03505788305618682, data cost:0.27991140806482 
2022-03-28 17:12:19,154: ============================================================
2022-03-28 17:12:19,154: Epoch 33/45 Batch 7600/7662 eta: 11:19:23.714134	Training Loss 0.4293 (0.4320)	Training Prec@1 91.797 (92.068)	Training Prec@5 95.312 (95.074)	
2022-03-28 17:12:19,154: ============================================================
2022-03-28 17:12:48,399: Epoch: 33/45 eta: 11:18:55.801976	Training Loss 0.4302 (0.4320)	Training Prec@1 93.555 (92.070)	Training Prec@5 96.289 (95.075)
2022-03-28 17:12:48,400: ============================================================
2022-03-28 17:12:48,402: Save Checkpoint...
2022-03-28 17:12:48,403: ============================================================
2022-03-28 17:12:50,462: Save done!
2022-03-28 17:12:50,462: ============================================================
2022-03-28 17:13:36,256: time cost, forward:0.10780021879408094, backward:0.03478244820026436, data cost:0.31753458639588017 
2022-03-28 17:13:36,257: ============================================================
2022-03-28 17:13:36,258: Epoch 34/45 Batch 100/7662 eta: 11:40:59.783448	Training Loss 0.4251 (0.4321)	Training Prec@1 92.969 (92.034)	Training Prec@5 95.898 (95.149)	
2022-03-28 17:13:36,258: ============================================================
2022-03-28 17:14:19,789: time cost, forward:0.12111490096279125, backward:0.03594843466677258, data cost:0.2899574239050324 
2022-03-28 17:14:19,790: ============================================================
2022-03-28 17:14:19,790: Epoch 34/45 Batch 200/7662 eta: 11:05:38.791313	Training Loss 0.4348 (0.4319)	Training Prec@1 90.625 (92.072)	Training Prec@5 95.117 (95.156)	
2022-03-28 17:14:19,790: ============================================================
2022-03-28 17:15:04,005: time cost, forward:0.12287263886186989, backward:0.036338756714377515, data cost:0.2860919049751001 
2022-03-28 17:15:04,005: ============================================================
2022-03-28 17:15:04,006: Epoch 34/45 Batch 300/7662 eta: 11:15:21.551381	Training Loss 0.4405 (0.4317)	Training Prec@1 91.211 (92.054)	Training Prec@5 93.359 (95.094)	
2022-03-28 17:15:04,006: ============================================================
2022-03-28 17:15:48,563: time cost, forward:0.12917148080983556, backward:0.03592764942867117, data cost:0.2800752716255666 
2022-03-28 17:15:48,563: ============================================================
2022-03-28 17:15:48,564: Epoch 34/45 Batch 400/7662 eta: 11:19:50.581457	Training Loss 0.4357 (0.4319)	Training Prec@1 91.797 (92.024)	Training Prec@5 94.922 (95.070)	
2022-03-28 17:15:48,564: ============================================================
2022-03-28 17:16:32,465: time cost, forward:0.12520773042896707, backward:0.03571964982516302, data cost:0.2829941594767905 
2022-03-28 17:16:32,465: ============================================================
2022-03-28 17:16:32,465: Epoch 34/45 Batch 500/7662 eta: 11:09:05.902606	Training Loss 0.4376 (0.4319)	Training Prec@1 92.383 (92.044)	Training Prec@5 94.922 (95.080)	
2022-03-28 17:16:32,466: ============================================================
2022-03-28 17:17:18,468: time cost, forward:0.13002616813863457, backward:0.03597133585527067, data cost:0.28051954239159077 
2022-03-28 17:17:18,469: ============================================================
2022-03-28 17:17:18,469: Epoch 34/45 Batch 600/7662 eta: 11:40:22.169290	Training Loss 0.4240 (0.4318)	Training Prec@1 92.188 (92.057)	Training Prec@5 94.727 (95.077)	
2022-03-28 17:17:18,470: ============================================================
2022-03-28 17:18:02,529: time cost, forward:0.1288584182531878, backward:0.03599606801852989, data cost:0.280777223802602 
2022-03-28 17:18:02,529: ============================================================
2022-03-28 17:18:02,529: Epoch 34/45 Batch 700/7662 eta: 11:10:02.669426	Training Loss 0.4315 (0.4317)	Training Prec@1 90.625 (92.083)	Training Prec@5 95.117 (95.098)	
2022-03-28 17:18:02,529: ============================================================
2022-03-28 17:18:49,147: time cost, forward:0.13212559697625037, backward:0.035709672338225515, data cost:0.2801982013692844 
2022-03-28 17:18:49,147: ============================================================
2022-03-28 17:18:49,147: Epoch 34/45 Batch 800/7662 eta: 11:48:09.895724	Training Loss 0.4383 (0.4317)	Training Prec@1 91.211 (92.086)	Training Prec@5 95.898 (95.104)	
2022-03-28 17:18:49,147: ============================================================
2022-03-28 17:19:35,969: time cost, forward:0.13587793094562875, backward:0.03612225843351065, data cost:0.2782716427549504 
2022-03-28 17:19:35,969: ============================================================
2022-03-28 17:19:35,969: Epoch 34/45 Batch 900/7662 eta: 11:50:29.324358	Training Loss 0.4302 (0.4316)	Training Prec@1 92.383 (92.101)	Training Prec@5 95.703 (95.109)	
2022-03-28 17:19:35,970: ============================================================
2022-03-28 17:20:21,547: time cost, forward:0.1372761098710863, backward:0.0362595242184323, data cost:0.27716855577997734 
2022-03-28 17:20:21,548: ============================================================
2022-03-28 17:20:21,548: Epoch 34/45 Batch 1000/7662 eta: 11:30:51.150892	Training Loss 0.4397 (0.4317)	Training Prec@1 90.430 (92.112)	Training Prec@5 92.383 (95.117)	
2022-03-28 17:20:21,548: ============================================================
2022-03-28 17:21:08,085: time cost, forward:0.1377348626495167, backward:0.03633890160655194, data cost:0.2778748999518411 
2022-03-28 17:21:08,085: ============================================================
2022-03-28 17:21:08,085: Epoch 34/45 Batch 1100/7662 eta: 11:44:37.179140	Training Loss 0.4201 (0.4318)	Training Prec@1 92.773 (92.091)	Training Prec@5 97.266 (95.101)	
2022-03-28 17:21:08,086: ============================================================
2022-03-28 17:21:51,490: time cost, forward:0.13528712955089883, backward:0.0364090618836671, data cost:0.27871106663179757 
2022-03-28 17:21:51,491: ============================================================
2022-03-28 17:21:51,491: Epoch 34/45 Batch 1200/7662 eta: 10:56:28.121871	Training Loss 0.4333 (0.4318)	Training Prec@1 93.164 (92.097)	Training Prec@5 96.094 (95.105)	
2022-03-28 17:21:51,491: ============================================================
2022-03-28 17:22:36,127: time cost, forward:0.1340023654529551, backward:0.03644328910263801, data cost:0.27960261296455086 
2022-03-28 17:22:36,127: ============================================================
2022-03-28 17:22:36,128: Epoch 34/45 Batch 1300/7662 eta: 11:14:21.167664	Training Loss 0.4380 (0.4318)	Training Prec@1 90.625 (92.093)	Training Prec@5 93.750 (95.102)	
2022-03-28 17:22:36,128: ============================================================
2022-03-28 17:23:24,121: time cost, forward:0.1352293222098797, backward:0.03650252779864515, data cost:0.2804401571875048 
2022-03-28 17:23:24,122: ============================================================
2022-03-28 17:23:24,122: Epoch 34/45 Batch 1400/7662 eta: 12:04:16.702203	Training Loss 0.4269 (0.4318)	Training Prec@1 92.969 (92.084)	Training Prec@5 95.703 (95.094)	
2022-03-28 17:23:24,122: ============================================================
2022-03-28 17:24:07,782: time cost, forward:0.13426532754904433, backward:0.03631159764595872, data cost:0.2804744622165 
2022-03-28 17:24:07,782: ============================================================
2022-03-28 17:24:07,782: Epoch 34/45 Batch 1500/7662 eta: 10:58:08.250850	Training Loss 0.4298 (0.4318)	Training Prec@1 93.359 (92.080)	Training Prec@5 94.922 (95.094)	
2022-03-28 17:24:07,782: ============================================================
2022-03-28 17:24:53,645: time cost, forward:0.1344651676402232, backward:0.036214394596235835, data cost:0.28083538263570224 
2022-03-28 17:24:53,646: ============================================================
2022-03-28 17:24:53,646: Epoch 34/45 Batch 1600/7662 eta: 11:30:35.574549	Training Loss 0.4367 (0.4318)	Training Prec@1 90.234 (92.083)	Training Prec@5 93.164 (95.095)	
2022-03-28 17:24:53,646: ============================================================
2022-03-28 17:25:37,561: time cost, forward:0.13349652725363423, backward:0.036053938750592034, data cost:0.28124662298816594 
2022-03-28 17:25:37,561: ============================================================
2022-03-28 17:25:37,561: Epoch 34/45 Batch 1700/7662 eta: 11:00:31.451680	Training Loss 0.4236 (0.4318)	Training Prec@1 93.945 (92.083)	Training Prec@5 96.094 (95.094)	
2022-03-28 17:25:37,562: ============================================================
2022-03-28 17:26:22,476: time cost, forward:0.13323091864254555, backward:0.03598653985236074, data cost:0.2813985017752104 
2022-03-28 17:26:22,476: ============================================================
2022-03-28 17:26:22,476: Epoch 34/45 Batch 1800/7662 eta: 11:14:48.538597	Training Loss 0.4372 (0.4318)	Training Prec@1 92.578 (92.082)	Training Prec@5 94.922 (95.087)	
2022-03-28 17:26:22,476: ============================================================
2022-03-28 17:27:07,070: time cost, forward:0.1329219189363131, backward:0.03601316918568463, data cost:0.28145873176229197 
2022-03-28 17:27:07,070: ============================================================
2022-03-28 17:27:07,070: Epoch 34/45 Batch 1900/7662 eta: 11:09:14.683707	Training Loss 0.4388 (0.4318)	Training Prec@1 91.602 (92.083)	Training Prec@5 94.336 (95.087)	
2022-03-28 17:27:07,070: ============================================================
2022-03-28 17:27:52,170: time cost, forward:0.1316514080795662, backward:0.035784922461917605, data cost:0.2829552479181485 
2022-03-28 17:27:52,170: ============================================================
2022-03-28 17:27:52,170: Epoch 34/45 Batch 2000/7662 eta: 11:16:05.365184	Training Loss 0.4445 (0.4318)	Training Prec@1 90.234 (92.086)	Training Prec@5 93.555 (95.089)	
2022-03-28 17:27:52,171: ============================================================
2022-03-28 17:28:38,276: time cost, forward:0.13237404323521543, backward:0.03575380169930715, data cost:0.28277252605269215 
2022-03-28 17:28:38,276: ============================================================
2022-03-28 17:28:38,277: Epoch 34/45 Batch 2100/7662 eta: 11:30:24.315441	Training Loss 0.4406 (0.4318)	Training Prec@1 89.844 (92.090)	Training Prec@5 92.969 (95.090)	
2022-03-28 17:28:38,277: ============================================================
2022-03-28 17:29:22,910: time cost, forward:0.132779792527168, backward:0.03570816255580734, data cost:0.282147163692091 
2022-03-28 17:29:22,910: ============================================================
2022-03-28 17:29:22,911: Epoch 34/45 Batch 2200/7662 eta: 11:07:36.576021	Training Loss 0.4403 (0.4318)	Training Prec@1 91.406 (92.093)	Training Prec@5 95.312 (95.090)	
2022-03-28 17:29:22,911: ============================================================
2022-03-28 17:30:09,426: time cost, forward:0.13327193675014234, backward:0.03565586085110034, data cost:0.2823481327453454 
2022-03-28 17:30:09,426: ============================================================
2022-03-28 17:30:09,426: Epoch 34/45 Batch 2300/7662 eta: 11:34:59.017951	Training Loss 0.4463 (0.4317)	Training Prec@1 91.797 (92.094)	Training Prec@5 94.531 (95.092)	
2022-03-28 17:30:09,427: ============================================================
2022-03-28 17:30:52,314: time cost, forward:0.13232123797115758, backward:0.03554577338491792, data cost:0.28242664965255504 
2022-03-28 17:30:52,314: ============================================================
2022-03-28 17:30:52,314: Epoch 34/45 Batch 2400/7662 eta: 10:40:03.929807	Training Loss 0.4302 (0.4318)	Training Prec@1 92.773 (92.093)	Training Prec@5 95.508 (95.093)	
2022-03-28 17:30:52,314: ============================================================
2022-03-28 17:31:39,834: time cost, forward:0.13271572285530422, backward:0.03567840748665189, data cost:0.28292820910636596 
2022-03-28 17:31:39,834: ============================================================
2022-03-28 17:31:39,834: Epoch 34/45 Batch 2500/7662 eta: 11:48:24.195102	Training Loss 0.4324 (0.4317)	Training Prec@1 92.383 (92.100)	Training Prec@5 95.117 (95.091)	
2022-03-28 17:31:39,834: ============================================================
2022-03-28 17:32:24,543: time cost, forward:0.1332430518466998, backward:0.03567383215032022, data cost:0.28219425435156126 
2022-03-28 17:32:24,544: ============================================================
2022-03-28 17:32:24,544: Epoch 34/45 Batch 2600/7662 eta: 11:05:45.965702	Training Loss 0.4191 (0.4317)	Training Prec@1 94.727 (92.110)	Training Prec@5 96.875 (95.100)	
2022-03-28 17:32:24,544: ============================================================
2022-03-28 17:33:10,287: time cost, forward:0.1337056805708533, backward:0.035759358460835146, data cost:0.2818528089138171 
2022-03-28 17:33:10,287: ============================================================
2022-03-28 17:33:10,288: Epoch 34/45 Batch 2700/7662 eta: 11:20:24.159355	Training Loss 0.4415 (0.4317)	Training Prec@1 91.602 (92.112)	Training Prec@5 95.703 (95.103)	
2022-03-28 17:33:10,288: ============================================================
2022-03-28 17:33:54,753: time cost, forward:0.13346901583901555, backward:0.03572171813635027, data cost:0.2818725966010276 
2022-03-28 17:33:54,754: ============================================================
2022-03-28 17:33:54,754: Epoch 34/45 Batch 2800/7662 eta: 11:00:39.206845	Training Loss 0.4264 (0.4317)	Training Prec@1 91.016 (92.103)	Training Prec@5 94.531 (95.097)	
2022-03-28 17:33:54,754: ============================================================
2022-03-28 17:34:39,205: time cost, forward:0.13251562700801242, backward:0.035654879800612944, data cost:0.2826547182687771 
2022-03-28 17:34:39,205: ============================================================
2022-03-28 17:34:39,205: Epoch 34/45 Batch 2900/7662 eta: 10:59:41.746195	Training Loss 0.4414 (0.4317)	Training Prec@1 90.625 (92.107)	Training Prec@5 94.922 (95.098)	
2022-03-28 17:34:39,206: ============================================================
2022-03-28 17:35:22,515: time cost, forward:0.13273803565294673, backward:0.035620343887873517, data cost:0.2818435083512029 
2022-03-28 17:35:22,515: ============================================================
2022-03-28 17:35:22,515: Epoch 34/45 Batch 3000/7662 eta: 10:42:02.104684	Training Loss 0.4265 (0.4317)	Training Prec@1 91.992 (92.111)	Training Prec@5 94.531 (95.097)	
2022-03-28 17:35:22,516: ============================================================
2022-03-28 17:36:08,153: time cost, forward:0.13224670463086868, backward:0.03554203918804157, data cost:0.28257565553744557 
2022-03-28 17:36:08,154: ============================================================
2022-03-28 17:36:08,154: Epoch 34/45 Batch 3100/7662 eta: 11:15:47.554111	Training Loss 0.4340 (0.4317)	Training Prec@1 91.797 (92.103)	Training Prec@5 93.359 (95.092)	
2022-03-28 17:36:08,154: ============================================================
2022-03-28 17:36:53,074: time cost, forward:0.13143844029127563, backward:0.0354801542723913, data cost:0.28341262651331983 
2022-03-28 17:36:53,075: ============================================================
2022-03-28 17:36:53,075: Epoch 34/45 Batch 3200/7662 eta: 11:04:25.241956	Training Loss 0.4353 (0.4317)	Training Prec@1 92.188 (92.102)	Training Prec@5 94.531 (95.093)	
2022-03-28 17:36:53,075: ============================================================
2022-03-28 17:37:36,955: time cost, forward:0.13066778511088992, backward:0.035445509263333785, data cost:0.28387586837033135 
2022-03-28 17:37:36,956: ============================================================
2022-03-28 17:37:36,956: Epoch 34/45 Batch 3300/7662 eta: 10:48:18.447115	Training Loss 0.4342 (0.4317)	Training Prec@1 93.945 (92.098)	Training Prec@5 96.680 (95.090)	
2022-03-28 17:37:36,956: ============================================================
2022-03-28 17:38:23,550: time cost, forward:0.13140151542086712, backward:0.035513168364701884, data cost:0.2835334518019611 
2022-03-28 17:38:23,551: ============================================================
2022-03-28 17:38:23,551: Epoch 34/45 Batch 3400/7662 eta: 11:27:37.184526	Training Loss 0.4321 (0.4318)	Training Prec@1 91.211 (92.101)	Training Prec@5 94.922 (95.093)	
2022-03-28 17:38:23,551: ============================================================
2022-03-28 17:39:07,095: time cost, forward:0.13166161972170184, backward:0.03550803651125576, data cost:0.28281338584869375 
2022-03-28 17:39:07,095: ============================================================
2022-03-28 17:39:07,095: Epoch 34/45 Batch 3500/7662 eta: 10:41:53.100766	Training Loss 0.4264 (0.4318)	Training Prec@1 92.773 (92.100)	Training Prec@5 96.094 (95.090)	
2022-03-28 17:39:07,096: ============================================================
2022-03-28 17:39:50,962: time cost, forward:0.13118404943302428, backward:0.035455708438802275, data cost:0.28300809012283445 
2022-03-28 17:39:50,963: ============================================================
2022-03-28 17:39:50,963: Epoch 34/45 Batch 3600/7662 eta: 10:45:54.891639	Training Loss 0.4288 (0.4318)	Training Prec@1 91.016 (92.098)	Training Prec@5 94.336 (95.091)	
2022-03-28 17:39:50,963: ============================================================
2022-03-28 17:40:37,110: time cost, forward:0.13163556959797287, backward:0.03548508967925291, data cost:0.28285176214510505 
2022-03-28 17:40:37,110: ============================================================
2022-03-28 17:40:37,110: Epoch 34/45 Batch 3700/7662 eta: 11:18:42.542664	Training Loss 0.4251 (0.4318)	Training Prec@1 91.992 (92.097)	Training Prec@5 94.922 (95.091)	
2022-03-28 17:40:37,111: ============================================================
2022-03-28 17:41:25,440: time cost, forward:0.1329347503659097, backward:0.03559252449511603, data cost:0.28230206744863534 
2022-03-28 17:41:25,441: ============================================================
2022-03-28 17:41:25,441: Epoch 34/45 Batch 3800/7662 eta: 11:50:00.960316	Training Loss 0.4248 (0.4318)	Training Prec@1 92.188 (92.096)	Training Prec@5 95.898 (95.092)	
2022-03-28 17:41:25,441: ============================================================
2022-03-28 17:42:10,731: time cost, forward:0.13322743393574413, backward:0.03557319915059225, data cost:0.282057017477759 
2022-03-28 17:42:10,732: ============================================================
2022-03-28 17:42:10,732: Epoch 34/45 Batch 3900/7662 eta: 11:04:36.564971	Training Loss 0.4359 (0.4318)	Training Prec@1 93.555 (92.091)	Training Prec@5 95.508 (95.088)	
2022-03-28 17:42:10,732: ============================================================
2022-03-28 17:42:58,225: time cost, forward:0.13434647959332133, backward:0.03559026255491943, data cost:0.2815027183280882 
2022-03-28 17:42:58,225: ============================================================
2022-03-28 17:42:58,226: Epoch 34/45 Batch 4000/7662 eta: 11:36:08.518376	Training Loss 0.4365 (0.4318)	Training Prec@1 91.992 (92.091)	Training Prec@5 94.336 (95.089)	
2022-03-28 17:42:58,226: ============================================================
2022-03-28 17:43:44,388: time cost, forward:0.13486502402641332, backward:0.03562929723925287, data cost:0.2811522102262893 
2022-03-28 17:43:44,388: ============================================================
2022-03-28 17:43:44,389: Epoch 34/45 Batch 4100/7662 eta: 11:15:51.590460	Training Loss 0.4227 (0.4318)	Training Prec@1 93.164 (92.093)	Training Prec@5 96.680 (95.093)	
2022-03-28 17:43:44,389: ============================================================
2022-03-28 17:44:28,109: time cost, forward:0.134571166627888, backward:0.035582245290492316, data cost:0.28114311398140274 
2022-03-28 17:44:28,110: ============================================================
2022-03-28 17:44:28,110: Epoch 34/45 Batch 4200/7662 eta: 10:39:23.313998	Training Loss 0.4235 (0.4318)	Training Prec@1 92.773 (92.091)	Training Prec@5 95.117 (95.093)	
2022-03-28 17:44:28,110: ============================================================
2022-03-28 17:45:13,074: time cost, forward:0.13405707620637144, backward:0.03553965425014385, data cost:0.2816472627529296 
2022-03-28 17:45:13,075: ============================================================
2022-03-28 17:45:13,075: Epoch 34/45 Batch 4300/7662 eta: 10:56:49.575011	Training Loss 0.4331 (0.4318)	Training Prec@1 92.188 (92.088)	Training Prec@5 96.094 (95.091)	
2022-03-28 17:45:13,075: ============================================================
2022-03-28 17:45:57,081: time cost, forward:0.13372273183893307, backward:0.035513429377235645, data cost:0.28174314327634553 
2022-03-28 17:45:57,081: ============================================================
2022-03-28 17:45:57,081: Epoch 34/45 Batch 4400/7662 eta: 10:42:05.505876	Training Loss 0.4376 (0.4318)	Training Prec@1 92.383 (92.088)	Training Prec@5 95.703 (95.094)	
2022-03-28 17:45:57,082: ============================================================
2022-03-28 17:46:42,147: time cost, forward:0.13349606079852802, backward:0.03555591266985654, data cost:0.2819214868238169 
2022-03-28 17:46:42,158: ============================================================
2022-03-28 17:46:42,158: Epoch 34/45 Batch 4500/7662 eta: 10:56:57.329464	Training Loss 0.4398 (0.4318)	Training Prec@1 89.062 (92.086)	Training Prec@5 93.164 (95.092)	
2022-03-28 17:46:42,158: ============================================================
2022-03-28 17:47:27,051: time cost, forward:0.13337499231171365, backward:0.03558237160203456, data cost:0.2819555895461754 
2022-03-28 17:47:27,051: ============================================================
2022-03-28 17:47:27,052: Epoch 34/45 Batch 4600/7662 eta: 10:53:32.223697	Training Loss 0.4383 (0.4319)	Training Prec@1 91.211 (92.086)	Training Prec@5 94.727 (95.092)	
2022-03-28 17:47:27,052: ============================================================
2022-03-28 17:48:13,140: time cost, forward:0.13395662864844174, backward:0.03561810265959362, data cost:0.28155310810005496 
2022-03-28 17:48:13,140: ============================================================
2022-03-28 17:48:13,141: Epoch 34/45 Batch 4700/7662 eta: 11:10:10.297076	Training Loss 0.4358 (0.4319)	Training Prec@1 91.406 (92.080)	Training Prec@5 94.531 (95.087)	
2022-03-28 17:48:13,141: ============================================================
2022-03-28 17:48:59,846: time cost, forward:0.13480890445546276, backward:0.035637746132470886, data cost:0.2809836492361588 
2022-03-28 17:48:59,846: ============================================================
2022-03-28 17:48:59,846: Epoch 34/45 Batch 4800/7662 eta: 11:18:21.809974	Training Loss 0.4319 (0.4319)	Training Prec@1 91.992 (92.081)	Training Prec@5 95.508 (95.086)	
2022-03-28 17:48:59,847: ============================================================
2022-03-28 17:49:45,220: time cost, forward:0.13549893436248897, backward:0.03567434252707611, data cost:0.2802983691045571 
2022-03-28 17:49:45,230: ============================================================
2022-03-28 17:49:45,230: Epoch 34/45 Batch 4900/7662 eta: 10:58:24.398466	Training Loss 0.4315 (0.4319)	Training Prec@1 90.039 (92.080)	Training Prec@5 92.969 (95.086)	
2022-03-28 17:49:45,230: ============================================================
2022-03-28 17:50:29,878: time cost, forward:0.13544021770700881, backward:0.035648172725365385, data cost:0.28027490535529287 
2022-03-28 17:50:29,879: ============================================================
2022-03-28 17:50:29,879: Epoch 34/45 Batch 5000/7662 eta: 10:46:59.732199	Training Loss 0.4349 (0.4319)	Training Prec@1 91.406 (92.078)	Training Prec@5 94.141 (95.086)	
2022-03-28 17:50:29,879: ============================================================
2022-03-28 17:51:16,135: time cost, forward:0.13538648890943988, backward:0.035619531479225974, data cost:0.28055050242622825 
2022-03-28 17:51:16,136: ============================================================
2022-03-28 17:51:16,136: Epoch 34/45 Batch 5100/7662 eta: 11:09:32.080664	Training Loss 0.4250 (0.4319)	Training Prec@1 90.430 (92.081)	Training Prec@5 94.727 (95.088)	
2022-03-28 17:51:16,136: ============================================================
2022-03-28 17:52:03,459: time cost, forward:0.13608739870881822, backward:0.03566183257868804, data cost:0.2802140521966487 
2022-03-28 17:52:03,459: ============================================================
2022-03-28 17:52:03,459: Epoch 34/45 Batch 5200/7662 eta: 11:24:10.329279	Training Loss 0.4240 (0.4319)	Training Prec@1 93.359 (92.079)	Training Prec@5 95.703 (95.087)	
2022-03-28 17:52:03,459: ============================================================
2022-03-28 17:52:47,969: time cost, forward:0.13612942606531284, backward:0.03565176762956834, data cost:0.2800437128978037 
2022-03-28 17:52:47,970: ============================================================
2022-03-28 17:52:47,970: Epoch 34/45 Batch 5300/7662 eta: 10:42:46.319967	Training Loss 0.4294 (0.4319)	Training Prec@1 92.773 (92.076)	Training Prec@5 96.680 (95.086)	
2022-03-28 17:52:47,970: ============================================================
2022-03-28 17:53:36,824: time cost, forward:0.13690319792565028, backward:0.035728359778825165, data cost:0.27985019497483676 
2022-03-28 17:53:36,825: ============================================================
2022-03-28 17:53:36,825: Epoch 34/45 Batch 5400/7662 eta: 11:44:41.796025	Training Loss 0.4326 (0.4319)	Training Prec@1 92.383 (92.076)	Training Prec@5 95.508 (95.085)	
2022-03-28 17:53:36,825: ============================================================
2022-03-28 17:54:21,552: time cost, forward:0.1368379752014654, backward:0.03574352993664253, data cost:0.2798052921059305 
2022-03-28 17:54:21,552: ============================================================
2022-03-28 17:54:21,553: Epoch 34/45 Batch 5500/7662 eta: 10:44:24.714960	Training Loss 0.4469 (0.4319)	Training Prec@1 90.234 (92.074)	Training Prec@5 94.336 (95.083)	
2022-03-28 17:54:21,553: ============================================================
2022-03-28 17:55:06,002: time cost, forward:0.13696660389110732, backward:0.0358096684147405, data cost:0.27946410747186734 
2022-03-28 17:55:06,013: ============================================================
2022-03-28 17:55:06,013: Epoch 34/45 Batch 5600/7662 eta: 10:39:49.282446	Training Loss 0.4273 (0.4319)	Training Prec@1 91.992 (92.076)	Training Prec@5 94.531 (95.085)	
2022-03-28 17:55:06,013: ============================================================
2022-03-28 17:55:50,439: time cost, forward:0.13692203926023505, backward:0.03581088366142845, data cost:0.27934000500044626 
2022-03-28 17:55:50,440: ============================================================
2022-03-28 17:55:50,440: Epoch 34/45 Batch 5700/7662 eta: 10:38:35.950012	Training Loss 0.4335 (0.4319)	Training Prec@1 90.039 (92.079)	Training Prec@5 95.117 (95.087)	
2022-03-28 17:55:50,440: ============================================================
2022-03-28 17:56:36,940: time cost, forward:0.13719147163170087, backward:0.0358042297619831, data cost:0.279289697281018 
2022-03-28 17:56:36,940: ============================================================
2022-03-28 17:56:36,941: Epoch 34/45 Batch 5800/7662 eta: 11:07:38.197250	Training Loss 0.4344 (0.4319)	Training Prec@1 91.211 (92.078)	Training Prec@5 95.117 (95.088)	
2022-03-28 17:56:36,941: ============================================================
2022-03-28 17:57:23,971: time cost, forward:0.13776223068217905, backward:0.03581101030672096, data cost:0.27900733008467965 
2022-03-28 17:57:23,971: ============================================================
2022-03-28 17:57:23,972: Epoch 34/45 Batch 5900/7662 eta: 11:14:27.738727	Training Loss 0.4188 (0.4319)	Training Prec@1 92.383 (92.078)	Training Prec@5 95.117 (95.086)	
2022-03-28 17:57:23,972: ============================================================
2022-03-28 17:58:09,459: time cost, forward:0.13804342345727208, backward:0.03582599672163938, data cost:0.2787325929494515 
2022-03-28 17:58:09,460: ============================================================
2022-03-28 17:58:09,461: Epoch 34/45 Batch 6000/7662 eta: 10:51:35.205766	Training Loss 0.4316 (0.4319)	Training Prec@1 92.188 (92.077)	Training Prec@5 95.508 (95.086)	
2022-03-28 17:58:09,461: ============================================================
2022-03-28 17:58:56,807: time cost, forward:0.1384643376665167, backward:0.03584990303601216, data cost:0.2786008320708493 
2022-03-28 17:58:56,807: ============================================================
2022-03-28 17:58:56,808: Epoch 34/45 Batch 6100/7662 eta: 11:17:25.279497	Training Loss 0.4311 (0.4319)	Training Prec@1 91.016 (92.076)	Training Prec@5 93.750 (95.083)	
2022-03-28 17:58:56,808: ============================================================
2022-03-28 17:59:42,752: time cost, forward:0.13812673316884183, backward:0.035820756040863266, data cost:0.2790118728766308 
2022-03-28 17:59:42,752: ============================================================
2022-03-28 17:59:42,752: Epoch 34/45 Batch 6200/7662 eta: 10:56:35.350684	Training Loss 0.4270 (0.4319)	Training Prec@1 91.797 (92.075)	Training Prec@5 96.094 (95.083)	
2022-03-28 17:59:42,753: ============================================================
2022-03-28 18:00:29,677: time cost, forward:0.13835093202544008, backward:0.0358536697262941, data cost:0.27907028590370536 
2022-03-28 18:00:29,677: ============================================================
2022-03-28 18:00:29,677: Epoch 34/45 Batch 6300/7662 eta: 11:09:48.782054	Training Loss 0.4269 (0.4319)	Training Prec@1 92.773 (92.076)	Training Prec@5 96.094 (95.083)	
2022-03-28 18:00:29,677: ============================================================
2022-03-28 18:01:15,436: time cost, forward:0.13850062048832015, backward:0.035855760945437715, data cost:0.27897411723642873 
2022-03-28 18:01:15,437: ============================================================
2022-03-28 18:01:15,437: Epoch 34/45 Batch 6400/7662 eta: 10:52:25.104211	Training Loss 0.4291 (0.4319)	Training Prec@1 92.969 (92.074)	Training Prec@5 95.312 (95.081)	
2022-03-28 18:01:15,437: ============================================================
2022-03-28 18:02:00,333: time cost, forward:0.1381541495507342, backward:0.03581543188128843, data cost:0.2792940286879063 
2022-03-28 18:02:00,333: ============================================================
2022-03-28 18:02:00,334: Epoch 34/45 Batch 6500/7662 eta: 10:39:21.966900	Training Loss 0.4219 (0.4319)	Training Prec@1 92.578 (92.076)	Training Prec@5 96.094 (95.081)	
2022-03-28 18:02:00,334: ============================================================
2022-03-28 18:02:46,942: time cost, forward:0.13828357723124515, backward:0.035837027932282814, data cost:0.27932469626957224 
2022-03-28 18:02:46,942: ============================================================
2022-03-28 18:02:46,943: Epoch 34/45 Batch 6600/7662 eta: 11:02:58.387741	Training Loss 0.4395 (0.4319)	Training Prec@1 90.625 (92.074)	Training Prec@5 94.336 (95.079)	
2022-03-28 18:02:46,943: ============================================================
2022-03-28 18:03:35,149: time cost, forward:0.13909995568334246, backward:0.0359029490514591, data cost:0.27885516911660546 
2022-03-28 18:03:35,149: ============================================================
2022-03-28 18:03:35,149: Epoch 34/45 Batch 6700/7662 eta: 11:24:53.831568	Training Loss 0.4254 (0.4319)	Training Prec@1 93.750 (92.075)	Training Prec@5 96.094 (95.079)	
2022-03-28 18:03:35,149: ============================================================
2022-03-28 18:04:22,779: time cost, forward:0.1395160756684135, backward:0.03593983494511316, data cost:0.2787188524567988 
2022-03-28 18:04:22,779: ============================================================
2022-03-28 18:04:22,779: Epoch 34/45 Batch 6800/7662 eta: 11:15:54.464330	Training Loss 0.4371 (0.4319)	Training Prec@1 91.992 (92.078)	Training Prec@5 94.727 (95.080)	
2022-03-28 18:04:22,779: ============================================================
2022-03-28 18:05:08,445: time cost, forward:0.13960522519520943, backward:0.035931644793575544, data cost:0.278670352444232 
2022-03-28 18:05:08,454: ============================================================
2022-03-28 18:05:08,455: Epoch 34/45 Batch 6900/7662 eta: 10:47:24.592241	Training Loss 0.4211 (0.4319)	Training Prec@1 92.969 (92.077)	Training Prec@5 96.289 (95.081)	
2022-03-28 18:05:08,455: ============================================================
2022-03-28 18:05:52,951: time cost, forward:0.13933960273106075, backward:0.035878132745050875, data cost:0.2788449309692841 
2022-03-28 18:05:52,952: ============================================================
2022-03-28 18:05:52,952: Epoch 34/45 Batch 7000/7662 eta: 10:29:58.431300	Training Loss 0.4274 (0.4319)	Training Prec@1 95.117 (92.077)	Training Prec@5 97.070 (95.082)	
2022-03-28 18:05:52,952: ============================================================
2022-03-28 18:06:38,777: time cost, forward:0.13926532393123822, backward:0.03586463022104434, data cost:0.2789812053420339 
2022-03-28 18:06:38,777: ============================================================
2022-03-28 18:06:38,777: Epoch 34/45 Batch 7100/7662 eta: 10:48:00.464034	Training Loss 0.4250 (0.4319)	Training Prec@1 93.750 (92.077)	Training Prec@5 96.680 (95.082)	
2022-03-28 18:06:38,777: ============================================================
2022-03-28 18:07:23,283: time cost, forward:0.13907612603743022, backward:0.03583609446798336, data cost:0.27906185839801784 
2022-03-28 18:07:23,283: ============================================================
2022-03-28 18:07:23,283: Epoch 34/45 Batch 7200/7662 eta: 10:28:36.643073	Training Loss 0.4301 (0.4319)	Training Prec@1 91.602 (92.082)	Training Prec@5 94.141 (95.084)	
2022-03-28 18:07:23,284: ============================================================
2022-03-28 18:08:10,133: time cost, forward:0.13930023142794645, backward:0.03586872562445554, data cost:0.2790067473214397 
2022-03-28 18:08:10,134: ============================================================
2022-03-28 18:08:10,134: Epoch 34/45 Batch 7300/7662 eta: 11:00:56.685127	Training Loss 0.4420 (0.4319)	Training Prec@1 91.016 (92.082)	Training Prec@5 94.336 (95.085)	
2022-03-28 18:08:10,134: ============================================================
2022-03-28 18:08:57,452: time cost, forward:0.14008593494818974, backward:0.035956934700625734, data cost:0.27838047370183694 
2022-03-28 18:08:57,453: ============================================================
2022-03-28 18:08:57,453: Epoch 34/45 Batch 7400/7662 eta: 11:06:46.201909	Training Loss 0.4400 (0.4319)	Training Prec@1 92.578 (92.082)	Training Prec@5 95.703 (95.084)	
2022-03-28 18:08:57,454: ============================================================
2022-03-28 18:09:43,253: time cost, forward:0.14021883142488864, backward:0.03597446829020969, data cost:0.2782655918593406 
2022-03-28 18:09:43,253: ============================================================
2022-03-28 18:09:43,254: Epoch 34/45 Batch 7500/7662 eta: 10:44:35.890493	Training Loss 0.4375 (0.4319)	Training Prec@1 91.211 (92.082)	Training Prec@5 93.945 (95.084)	
2022-03-28 18:09:43,254: ============================================================
2022-03-28 18:10:29,095: time cost, forward:0.1404345484842892, backward:0.03599680397193327, data cost:0.27806702822913526 
2022-03-28 18:10:29,095: ============================================================
2022-03-28 18:10:29,095: Epoch 34/45 Batch 7600/7662 eta: 10:44:25.211893	Training Loss 0.4339 (0.4319)	Training Prec@1 91.797 (92.079)	Training Prec@5 95.312 (95.081)	
2022-03-28 18:10:29,096: ============================================================
2022-03-28 18:10:58,656: Epoch: 34/45 eta: 10:43:56.331600	Training Loss 0.4369 (0.4319)	Training Prec@1 90.430 (92.077)	Training Prec@5 94.141 (95.079)
2022-03-28 18:10:58,656: ============================================================
2022-03-28 18:10:58,658: Save Checkpoint...
2022-03-28 18:10:58,659: ============================================================
2022-03-28 18:11:01,463: Save done!
2022-03-28 18:11:01,463: ============================================================
2022-03-28 18:11:53,477: time cost, forward:0.11022130166641389, backward:0.034300255052971115, data cost:0.3783758818501174 
2022-03-28 18:11:53,478: ============================================================
2022-03-28 18:11:53,478: Epoch 35/45 Batch 100/7662 eta: 12:09:46.434767	Training Loss 0.4285 (0.4311)	Training Prec@1 92.383 (92.280)	Training Prec@5 94.727 (95.194)	
2022-03-28 18:11:53,478: ============================================================
2022-03-28 18:12:34,883: time cost, forward:0.1105654131827043, backward:0.03505656108185275, data cost:0.32230974681413355 
2022-03-28 18:12:34,884: ============================================================
2022-03-28 18:12:34,884: Epoch 35/45 Batch 200/7662 eta: 9:40:15.345752	Training Loss 0.4354 (0.4313)	Training Prec@1 90.234 (92.227)	Training Prec@5 93.945 (95.191)	
2022-03-28 18:12:34,884: ============================================================
2022-03-28 18:13:18,133: time cost, forward:0.11237347006399097, backward:0.03461057047381449, data cost:0.30883071414603036 
2022-03-28 18:13:18,133: ============================================================
2022-03-28 18:13:18,134: Epoch 35/45 Batch 300/7662 eta: 10:05:22.376187	Training Loss 0.4402 (0.4314)	Training Prec@1 93.945 (92.278)	Training Prec@5 95.898 (95.216)	
2022-03-28 18:13:18,134: ============================================================
2022-03-28 18:14:01,774: time cost, forward:0.11153333886225421, backward:0.034102405820574076, data cost:0.305028774385763 
2022-03-28 18:14:01,775: ============================================================
2022-03-28 18:14:01,775: Epoch 35/45 Batch 400/7662 eta: 10:10:07.663047	Training Loss 0.4287 (0.4316)	Training Prec@1 92.969 (92.238)	Training Prec@5 96.289 (95.178)	
2022-03-28 18:14:01,775: ============================================================
2022-03-28 18:14:45,993: time cost, forward:0.11128628803398423, backward:0.034171340460767724, data cost:0.30351021485720464 
2022-03-28 18:14:45,994: ============================================================
2022-03-28 18:14:45,994: Epoch 35/45 Batch 500/7662 eta: 10:17:28.135893	Training Loss 0.4357 (0.4318)	Training Prec@1 90.625 (92.159)	Training Prec@5 93.750 (95.143)	
2022-03-28 18:14:45,994: ============================================================
2022-03-28 18:15:30,193: time cost, forward:0.11117374359665809, backward:0.03418311571239032, data cost:0.30231589267966347 
2022-03-28 18:15:30,193: ============================================================
2022-03-28 18:15:30,193: Epoch 35/45 Batch 600/7662 eta: 10:16:27.189929	Training Loss 0.4220 (0.4317)	Training Prec@1 94.922 (92.175)	Training Prec@5 96.680 (95.162)	
2022-03-28 18:15:30,193: ============================================================
2022-03-28 18:16:15,061: time cost, forward:0.11191200971262308, backward:0.034026297035817596, data cost:0.3018595493573146 
2022-03-28 18:16:15,062: ============================================================
2022-03-28 18:16:15,063: Epoch 35/45 Batch 700/7662 eta: 10:25:03.219916	Training Loss 0.4186 (0.4317)	Training Prec@1 93.750 (92.175)	Training Prec@5 95.898 (95.159)	
2022-03-28 18:16:15,063: ============================================================
2022-03-28 18:17:01,041: time cost, forward:0.11533765022984434, backward:0.03486449160474412, data cost:0.2990312665812811 
2022-03-28 18:17:01,042: ============================================================
2022-03-28 18:17:01,042: Epoch 35/45 Batch 800/7662 eta: 10:39:44.798921	Training Loss 0.4311 (0.4316)	Training Prec@1 92.383 (92.188)	Training Prec@5 94.727 (95.174)	
2022-03-28 18:17:01,042: ============================================================
2022-03-28 18:17:48,081: time cost, forward:0.11986829335485337, backward:0.035591173490242115, data cost:0.29612222476318495 
2022-03-28 18:17:48,081: ============================================================
2022-03-28 18:17:48,081: Epoch 35/45 Batch 900/7662 eta: 10:53:42.928789	Training Loss 0.4262 (0.4317)	Training Prec@1 93.750 (92.149)	Training Prec@5 96.875 (95.146)	
2022-03-28 18:17:48,082: ============================================================
2022-03-28 18:18:35,044: time cost, forward:0.12420505374759525, backward:0.036090883048805034, data cost:0.2930561808852462 
2022-03-28 18:18:35,044: ============================================================
2022-03-28 18:18:35,044: Epoch 35/45 Batch 1000/7662 eta: 10:51:52.250937	Training Loss 0.4432 (0.4317)	Training Prec@1 89.453 (92.123)	Training Prec@5 92.969 (95.144)	
2022-03-28 18:18:35,045: ============================================================
2022-03-28 18:19:19,228: time cost, forward:0.12396796971044288, backward:0.036086261435136456, data cost:0.2921487471534514 
2022-03-28 18:19:19,229: ============================================================
2022-03-28 18:19:19,229: Epoch 35/45 Batch 1100/7662 eta: 10:12:33.862517	Training Loss 0.4367 (0.4317)	Training Prec@1 92.578 (92.113)	Training Prec@5 94.727 (95.134)	
2022-03-28 18:19:19,229: ============================================================
2022-03-28 18:20:05,191: time cost, forward:0.1248913449580914, backward:0.03622057598963492, data cost:0.2917498651397934 
2022-03-28 18:20:05,191: ============================================================
2022-03-28 18:20:05,192: Epoch 35/45 Batch 1200/7662 eta: 10:36:27.248187	Training Loss 0.4365 (0.4316)	Training Prec@1 91.016 (92.118)	Training Prec@5 95.117 (95.135)	
2022-03-28 18:20:05,192: ============================================================
2022-03-28 18:20:51,479: time cost, forward:0.12563485122809875, backward:0.03630964640381705, data cost:0.29162928909407476 
2022-03-28 18:20:51,480: ============================================================
2022-03-28 18:20:51,480: Epoch 35/45 Batch 1300/7662 eta: 10:40:11.459392	Training Loss 0.4244 (0.4317)	Training Prec@1 91.016 (92.107)	Training Prec@5 94.531 (95.128)	
2022-03-28 18:20:51,480: ============================================================
2022-03-28 18:21:37,059: time cost, forward:0.12789409462258677, backward:0.03676182973205915, data cost:0.289035344992986 
2022-03-28 18:21:37,060: ============================================================
2022-03-28 18:21:37,060: Epoch 35/45 Batch 1400/7662 eta: 10:29:38.451338	Training Loss 0.4361 (0.4317)	Training Prec@1 91.602 (92.092)	Training Prec@5 94.336 (95.119)	
2022-03-28 18:21:37,061: ============================================================
2022-03-28 18:22:23,859: time cost, forward:0.1283854267611513, backward:0.03684781534501598, data cost:0.28938458059691363 
2022-03-28 18:22:23,859: ============================================================
2022-03-28 18:22:23,860: Epoch 35/45 Batch 1500/7662 eta: 10:45:41.818495	Training Loss 0.4311 (0.4317)	Training Prec@1 91.992 (92.093)	Training Prec@5 95.117 (95.115)	
2022-03-28 18:22:23,860: ============================================================
2022-03-28 18:23:08,776: time cost, forward:0.12770812954285354, backward:0.036730890202477544, data cost:0.28977329198683405 
2022-03-28 18:23:08,777: ============================================================
2022-03-28 18:23:08,777: Epoch 35/45 Batch 1600/7662 eta: 10:18:59.092778	Training Loss 0.4410 (0.4316)	Training Prec@1 93.164 (92.108)	Training Prec@5 95.508 (95.126)	
2022-03-28 18:23:08,777: ============================================================
2022-03-28 18:23:54,577: time cost, forward:0.1285212034334359, backward:0.036815605281450385, data cost:0.2891486723890299 
2022-03-28 18:23:54,577: ============================================================
2022-03-28 18:23:54,577: Epoch 35/45 Batch 1700/7662 eta: 10:30:23.280357	Training Loss 0.4318 (0.4317)	Training Prec@1 91.602 (92.101)	Training Prec@5 95.312 (95.128)	
2022-03-28 18:23:54,578: ============================================================
2022-03-28 18:24:40,906: time cost, forward:0.1286730034739657, backward:0.036711236117216134, data cost:0.2895022863278858 
2022-03-28 18:24:40,907: ============================================================
2022-03-28 18:24:40,907: Epoch 35/45 Batch 1800/7662 eta: 10:36:54.108156	Training Loss 0.4233 (0.4317)	Training Prec@1 93.945 (92.098)	Training Prec@5 96.289 (95.127)	
2022-03-28 18:24:40,907: ============================================================
2022-03-28 18:25:29,955: time cost, forward:0.13048425582034015, backward:0.03675474387335112, data cost:0.28948013414641066 
2022-03-28 18:25:29,955: ============================================================
2022-03-28 18:25:29,955: Epoch 35/45 Batch 1900/7662 eta: 11:13:27.474565	Training Loss 0.4354 (0.4316)	Training Prec@1 92.578 (92.101)	Training Prec@5 94.727 (95.130)	
2022-03-28 18:25:29,956: ============================================================
2022-03-28 18:26:13,697: time cost, forward:0.12938613197456425, backward:0.03661280778004206, data cost:0.289708682213383 
2022-03-28 18:26:13,697: ============================================================
2022-03-28 18:26:13,698: Epoch 35/45 Batch 2000/7662 eta: 9:59:52.371389	Training Loss 0.4358 (0.4316)	Training Prec@1 92.188 (92.095)	Training Prec@5 95.703 (95.126)	
2022-03-28 18:26:13,698: ============================================================
2022-03-28 18:27:00,629: time cost, forward:0.13022363918517077, backward:0.03661515963764745, data cost:0.2895277979943002 
2022-03-28 18:27:00,640: ============================================================
2022-03-28 18:27:00,640: Epoch 35/45 Batch 2100/7662 eta: 10:42:58.784848	Training Loss 0.4403 (0.4316)	Training Prec@1 92.578 (92.094)	Training Prec@5 95.703 (95.126)	
2022-03-28 18:27:00,640: ============================================================
2022-03-28 18:27:47,683: time cost, forward:0.13186221797122583, backward:0.03675834988398463, data cost:0.28836318968425506 
2022-03-28 18:27:47,683: ============================================================
2022-03-28 18:27:47,683: Epoch 35/45 Batch 2200/7662 eta: 10:43:34.425836	Training Loss 0.4172 (0.4316)	Training Prec@1 91.992 (92.089)	Training Prec@5 94.922 (95.118)	
2022-03-28 18:27:47,683: ============================================================
2022-03-28 18:28:35,553: time cost, forward:0.1326330566779589, backward:0.03674531522238965, data cost:0.2884626270325716 
2022-03-28 18:28:35,554: ============================================================
2022-03-28 18:28:35,555: Epoch 35/45 Batch 2300/7662 eta: 10:54:06.338810	Training Loss 0.4368 (0.4316)	Training Prec@1 92.773 (92.088)	Training Prec@5 95.117 (95.122)	
2022-03-28 18:28:35,555: ============================================================
2022-03-28 18:29:23,134: time cost, forward:0.13477516681167073, backward:0.03690264233950527, data cost:0.2868915217376938 
2022-03-28 18:29:23,135: ============================================================
2022-03-28 18:29:23,135: Epoch 35/45 Batch 2400/7662 eta: 10:49:20.159164	Training Loss 0.4264 (0.4316)	Training Prec@1 91.211 (92.085)	Training Prec@5 95.117 (95.119)	
2022-03-28 18:29:23,135: ============================================================
2022-03-28 18:30:08,803: time cost, forward:0.13624492527342358, backward:0.036935371415717164, data cost:0.2852876114816654 
2022-03-28 18:30:08,803: ============================================================
2022-03-28 18:30:08,804: Epoch 35/45 Batch 2500/7662 eta: 10:22:29.363935	Training Loss 0.4453 (0.4316)	Training Prec@1 89.844 (92.084)	Training Prec@5 92.578 (95.115)	
2022-03-28 18:30:08,804: ============================================================
2022-03-28 18:30:53,996: time cost, forward:0.13666629378453454, backward:0.0369666187797156, data cost:0.2845178704483778 
2022-03-28 18:30:53,996: ============================================================
2022-03-28 18:30:53,997: Epoch 35/45 Batch 2600/7662 eta: 10:15:14.816165	Training Loss 0.4312 (0.4316)	Training Prec@1 91.797 (92.075)	Training Prec@5 95.117 (95.108)	
2022-03-28 18:30:53,997: ============================================================
2022-03-28 18:31:39,642: time cost, forward:0.1355459800513862, backward:0.03684385947361395, data cost:0.2856773413212576 
2022-03-28 18:31:39,642: ============================================================
2022-03-28 18:31:39,643: Epoch 35/45 Batch 2700/7662 eta: 10:20:39.361517	Training Loss 0.4405 (0.4316)	Training Prec@1 91.211 (92.072)	Training Prec@5 94.141 (95.104)	
2022-03-28 18:31:39,643: ============================================================
2022-03-28 18:32:24,406: time cost, forward:0.13451580543013802, backward:0.03672097725031758, data cost:0.28645866400175923 
2022-03-28 18:32:24,406: ============================================================
2022-03-28 18:32:24,406: Epoch 35/45 Batch 2800/7662 eta: 10:07:54.959929	Training Loss 0.4303 (0.4316)	Training Prec@1 92.383 (92.073)	Training Prec@5 94.727 (95.105)	
2022-03-28 18:32:24,407: ============================================================
2022-03-28 18:33:11,594: time cost, forward:0.13482928021112858, backward:0.03673173189574416, data cost:0.2866215460791428 
2022-03-28 18:33:11,595: ============================================================
2022-03-28 18:33:11,595: Epoch 35/45 Batch 2900/7662 eta: 10:40:03.653227	Training Loss 0.4408 (0.4317)	Training Prec@1 90.625 (92.072)	Training Prec@5 93.750 (95.104)	
2022-03-28 18:33:11,595: ============================================================
2022-03-28 18:33:57,404: time cost, forward:0.13471412801790253, backward:0.03669536682159434, data cost:0.2867431127059138 
2022-03-28 18:33:57,414: ============================================================
2022-03-28 18:33:57,414: Epoch 35/45 Batch 3000/7662 eta: 10:20:42.931615	Training Loss 0.4361 (0.4316)	Training Prec@1 90.039 (92.072)	Training Prec@5 93.555 (95.102)	
2022-03-28 18:33:57,414: ============================================================
2022-03-28 18:34:42,561: time cost, forward:0.13470961732916695, backward:0.03670765361927447, data cost:0.28648119074331246 
2022-03-28 18:34:42,561: ============================================================
2022-03-28 18:34:42,561: Epoch 35/45 Batch 3100/7662 eta: 10:10:51.853012	Training Loss 0.4349 (0.4317)	Training Prec@1 91.406 (92.077)	Training Prec@5 94.531 (95.106)	
2022-03-28 18:34:42,561: ============================================================
2022-03-28 18:35:27,562: time cost, forward:0.13448121824201922, backward:0.036634051155097484, data cost:0.2865289025248569 
2022-03-28 18:35:27,563: ============================================================
2022-03-28 18:35:27,564: Epoch 35/45 Batch 3200/7662 eta: 10:08:09.437436	Training Loss 0.4393 (0.4316)	Training Prec@1 92.773 (92.075)	Training Prec@5 94.336 (95.103)	
2022-03-28 18:35:27,564: ============================================================
2022-03-28 18:36:11,478: time cost, forward:0.13449542476900347, backward:0.03655317047213959, data cost:0.2860259018944552 
2022-03-28 18:36:11,490: ============================================================
2022-03-28 18:36:11,490: Epoch 35/45 Batch 3300/7662 eta: 9:52:52.701765	Training Loss 0.4316 (0.4316)	Training Prec@1 92.773 (92.073)	Training Prec@5 96.289 (95.101)	
2022-03-28 18:36:11,490: ============================================================
2022-03-28 18:36:56,000: time cost, forward:0.13371021630449062, backward:0.03640259213572427, data cost:0.28655742504695614 
2022-03-28 18:36:56,001: ============================================================
2022-03-28 18:36:56,001: Epoch 35/45 Batch 3400/7662 eta: 10:00:01.996171	Training Loss 0.4290 (0.4316)	Training Prec@1 91.211 (92.078)	Training Prec@5 94.336 (95.104)	
2022-03-28 18:36:56,002: ============================================================
2022-03-28 18:37:41,595: time cost, forward:0.13357835396523815, backward:0.03638421007141654, data cost:0.28670386274598875 
2022-03-28 18:37:41,595: ============================================================
2022-03-28 18:37:41,596: Epoch 35/45 Batch 3500/7662 eta: 10:13:52.545473	Training Loss 0.4311 (0.4317)	Training Prec@1 89.648 (92.078)	Training Prec@5 92.578 (95.099)	
2022-03-28 18:37:41,596: ============================================================
2022-03-28 18:38:28,233: time cost, forward:0.13410947348681315, backward:0.036416741668200886, data cost:0.2863923789859315 
2022-03-28 18:38:28,245: ============================================================
2022-03-28 18:38:28,245: Epoch 35/45 Batch 3600/7662 eta: 10:27:17.893021	Training Loss 0.4389 (0.4317)	Training Prec@1 90.820 (92.070)	Training Prec@5 94.141 (95.090)	
2022-03-28 18:38:28,245: ============================================================
2022-03-28 18:39:12,387: time cost, forward:0.13370264391861725, backward:0.0363865749485978, data cost:0.2863885453985523 
2022-03-28 18:39:12,387: ============================================================
2022-03-28 18:39:12,388: Epoch 35/45 Batch 3700/7662 eta: 9:52:51.669449	Training Loss 0.4420 (0.4318)	Training Prec@1 89.844 (92.074)	Training Prec@5 93.555 (95.092)	
2022-03-28 18:39:12,388: ============================================================
2022-03-28 18:39:59,807: time cost, forward:0.13501258918127595, backward:0.03646483280747211, data cost:0.2854721094941302 
2022-03-28 18:39:59,818: ============================================================
2022-03-28 18:39:59,818: Epoch 35/45 Batch 3800/7662 eta: 10:36:13.362695	Training Loss 0.4412 (0.4318)	Training Prec@1 91.992 (92.070)	Training Prec@5 94.531 (95.088)	
2022-03-28 18:39:59,818: ============================================================
2022-03-28 18:40:48,092: time cost, forward:0.13654314624740393, backward:0.03661431426052926, data cost:0.284419528457806 
2022-03-28 18:40:48,103: ============================================================
2022-03-28 18:40:48,103: Epoch 35/45 Batch 3900/7662 eta: 10:46:53.144348	Training Loss 0.4324 (0.4317)	Training Prec@1 92.188 (92.070)	Training Prec@5 94.531 (95.086)	
2022-03-28 18:40:48,103: ============================================================
2022-03-28 18:41:35,168: time cost, forward:0.13697869099805163, backward:0.03659550348917643, data cost:0.28430554806575026 
2022-03-28 18:41:35,169: ============================================================
2022-03-28 18:41:35,169: Epoch 35/45 Batch 4000/7662 eta: 10:29:46.036007	Training Loss 0.4415 (0.4317)	Training Prec@1 91.211 (92.067)	Training Prec@5 94.922 (95.084)	
2022-03-28 18:41:35,169: ============================================================
2022-03-28 18:42:26,175: time cost, forward:0.138690191409913, backward:0.03674867194464917, data cost:0.2836899186204369 
2022-03-28 18:42:26,186: ============================================================
2022-03-28 18:42:26,186: Epoch 35/45 Batch 4100/7662 eta: 11:21:46.776024	Training Loss 0.4409 (0.4317)	Training Prec@1 91.602 (92.072)	Training Prec@5 94.141 (95.088)	
2022-03-28 18:42:26,186: ============================================================
2022-03-28 18:43:14,301: time cost, forward:0.13964683040320008, backward:0.03682310185451739, data cost:0.2831620961094107 
2022-03-28 18:43:14,302: ============================================================
2022-03-28 18:43:14,302: Epoch 35/45 Batch 4200/7662 eta: 10:42:12.849262	Training Loss 0.4305 (0.4317)	Training Prec@1 92.773 (92.067)	Training Prec@5 96.875 (95.086)	
2022-03-28 18:43:14,303: ============================================================
2022-03-28 18:44:04,604: time cost, forward:0.1412117342029402, backward:0.037019744089077336, data cost:0.28234677604254693 
2022-03-28 18:44:04,604: ============================================================
2022-03-28 18:44:04,605: Epoch 35/45 Batch 4300/7662 eta: 11:10:33.491489	Training Loss 0.4385 (0.4317)	Training Prec@1 91.797 (92.071)	Training Prec@5 95.117 (95.088)	
2022-03-28 18:44:04,605: ============================================================
2022-03-28 18:44:53,444: time cost, forward:0.1417370252810871, backward:0.037022689928599826, data cost:0.28243349508470666 
2022-03-28 18:44:53,445: ============================================================
2022-03-28 18:44:53,445: Epoch 35/45 Batch 4400/7662 eta: 10:50:14.827420	Training Loss 0.4272 (0.4318)	Training Prec@1 92.383 (92.073)	Training Prec@5 95.898 (95.088)	
2022-03-28 18:44:53,445: ============================================================
2022-03-28 18:45:42,259: time cost, forward:0.14204750550696785, backward:0.03699342132436087, data cost:0.2827295772126845 
2022-03-28 18:45:42,259: ============================================================
2022-03-28 18:45:42,260: Epoch 35/45 Batch 4500/7662 eta: 10:49:06.056659	Training Loss 0.4298 (0.4318)	Training Prec@1 92.578 (92.070)	Training Prec@5 95.312 (95.087)	
2022-03-28 18:45:42,260: ============================================================
2022-03-28 18:46:32,241: time cost, forward:0.14316602099535386, backward:0.037072706077378274, data cost:0.28227807050374415 
2022-03-28 18:46:32,242: ============================================================
2022-03-28 18:46:32,242: Epoch 35/45 Batch 4600/7662 eta: 11:03:47.137772	Training Loss 0.4260 (0.4317)	Training Prec@1 92.383 (92.071)	Training Prec@5 95.703 (95.088)	
2022-03-28 18:46:32,242: ============================================================
2022-03-28 18:47:17,934: time cost, forward:0.14332627894955002, backward:0.03710000954883914, data cost:0.2820040320660059 
2022-03-28 18:47:17,935: ============================================================
2022-03-28 18:47:17,936: Epoch 35/45 Batch 4700/7662 eta: 10:06:04.790171	Training Loss 0.4226 (0.4318)	Training Prec@1 92.773 (92.067)	Training Prec@5 95.898 (95.085)	
2022-03-28 18:47:17,937: ============================================================
2022-03-28 18:48:05,094: time cost, forward:0.1434609503466826, backward:0.03713672408016306, data cost:0.28200965882539997 
2022-03-28 18:48:05,095: ============================================================
2022-03-28 18:48:05,095: Epoch 35/45 Batch 4800/7662 eta: 10:24:43.386390	Training Loss 0.4272 (0.4318)	Training Prec@1 92.188 (92.067)	Training Prec@5 95.898 (95.087)	
2022-03-28 18:48:05,095: ============================================================
2022-03-28 18:48:55,123: time cost, forward:0.14432798781962414, backward:0.037269884918533704, data cost:0.28176046030987134 
2022-03-28 18:48:55,124: ============================================================
2022-03-28 18:48:55,124: Epoch 35/45 Batch 4900/7662 eta: 11:01:54.717561	Training Loss 0.4244 (0.4318)	Training Prec@1 93.945 (92.068)	Training Prec@5 95.898 (95.088)	
2022-03-28 18:48:55,124: ============================================================
2022-03-28 18:49:39,899: time cost, forward:0.1435972878302925, backward:0.037193005550954554, data cost:0.2822437694154279 
2022-03-28 18:49:39,899: ============================================================
2022-03-28 18:49:39,899: Epoch 35/45 Batch 5000/7662 eta: 9:51:39.070314	Training Loss 0.4221 (0.4318)	Training Prec@1 93.750 (92.071)	Training Prec@5 95.703 (95.089)	
2022-03-28 18:49:39,900: ============================================================
2022-03-28 18:50:28,253: time cost, forward:0.1442596830557692, backward:0.03726704828083153, data cost:0.28189824421888804 
2022-03-28 18:50:28,253: ============================================================
2022-03-28 18:50:28,254: Epoch 35/45 Batch 5100/7662 eta: 10:38:08.332149	Training Loss 0.4227 (0.4318)	Training Prec@1 92.773 (92.076)	Training Prec@5 95.703 (95.090)	
2022-03-28 18:50:28,254: ============================================================
2022-03-28 18:51:14,625: time cost, forward:0.14415205439871884, backward:0.03724630389403416, data cost:0.2820075639143979 
2022-03-28 18:51:14,626: ============================================================
2022-03-28 18:51:14,626: Epoch 35/45 Batch 5200/7662 eta: 10:11:12.847845	Training Loss 0.4384 (0.4318)	Training Prec@1 93.164 (92.075)	Training Prec@5 94.727 (95.088)	
2022-03-28 18:51:14,626: ============================================================
2022-03-28 18:51:59,960: time cost, forward:0.14386412453529948, backward:0.03721721974560215, data cost:0.2821185192897154 
2022-03-28 18:51:59,960: ============================================================
2022-03-28 18:51:59,961: Epoch 35/45 Batch 5300/7662 eta: 9:56:46.595648	Training Loss 0.4274 (0.4318)	Training Prec@1 93.359 (92.074)	Training Prec@5 94.727 (95.086)	
2022-03-28 18:51:59,961: ============================================================
2022-03-28 18:52:47,805: time cost, forward:0.14393803291264276, backward:0.037230223367602365, data cost:0.28231645019038076 
2022-03-28 18:52:47,806: ============================================================
2022-03-28 18:52:47,806: Epoch 35/45 Batch 5400/7662 eta: 10:29:01.561969	Training Loss 0.4326 (0.4318)	Training Prec@1 92.773 (92.078)	Training Prec@5 94.727 (95.088)	
2022-03-28 18:52:47,806: ============================================================
2022-03-28 18:53:36,337: time cost, forward:0.1448889338421722, backward:0.03728937396007704, data cost:0.28165894192898355 
2022-03-28 18:53:36,338: ============================================================
2022-03-28 18:53:36,338: Epoch 35/45 Batch 5500/7662 eta: 10:37:15.104232	Training Loss 0.4265 (0.4318)	Training Prec@1 92.383 (92.077)	Training Prec@5 95.508 (95.087)	
2022-03-28 18:53:36,338: ============================================================
2022-03-28 18:54:24,763: time cost, forward:0.14485671256648405, backward:0.03723899963434774, data cost:0.28209753010097965 
2022-03-28 18:54:24,764: ============================================================
2022-03-28 18:54:24,764: Epoch 35/45 Batch 5600/7662 eta: 10:35:03.238654	Training Loss 0.4301 (0.4318)	Training Prec@1 91.406 (92.078)	Training Prec@5 95.312 (95.089)	
2022-03-28 18:54:24,764: ============================================================
2022-03-28 18:55:14,518: time cost, forward:0.14562167726413977, backward:0.037295614466120806, data cost:0.28182950813031066 
2022-03-28 18:55:14,519: ============================================================
2022-03-28 18:55:14,519: Epoch 35/45 Batch 5700/7662 eta: 10:51:38.898109	Training Loss 0.4182 (0.4318)	Training Prec@1 93.359 (92.080)	Training Prec@5 96.289 (95.088)	
2022-03-28 18:55:14,519: ============================================================
2022-03-28 18:56:01,156: time cost, forward:0.14561992288560863, backward:0.03728662151409688, data cost:0.2818488973156093 
2022-03-28 18:56:01,156: ============================================================
2022-03-28 18:56:01,157: Epoch 35/45 Batch 5800/7662 eta: 10:10:02.505981	Training Loss 0.4289 (0.4318)	Training Prec@1 92.383 (92.081)	Training Prec@5 94.727 (95.089)	
2022-03-28 18:56:01,157: ============================================================
2022-03-28 18:56:51,706: time cost, forward:0.14648970698356467, backward:0.03730407050553571, data cost:0.28161852200933785 
2022-03-28 18:56:51,706: ============================================================
2022-03-28 18:56:51,707: Epoch 35/45 Batch 5900/7662 eta: 11:00:22.674300	Training Loss 0.4433 (0.4318)	Training Prec@1 89.844 (92.080)	Training Prec@5 94.727 (95.089)	
2022-03-28 18:56:51,707: ============================================================
2022-03-28 18:57:36,896: time cost, forward:0.1465196777212757, backward:0.037262811683022715, data cost:0.281418750834159 
2022-03-28 18:57:36,908: ============================================================
2022-03-28 18:57:36,908: Epoch 35/45 Batch 6000/7662 eta: 9:49:44.790078	Training Loss 0.4165 (0.4318)	Training Prec@1 94.141 (92.083)	Training Prec@5 95.703 (95.089)	
2022-03-28 18:57:36,908: ============================================================
2022-03-28 18:58:28,316: time cost, forward:0.1474302639080122, backward:0.03736534183544103, data cost:0.2811842875160884 
2022-03-28 18:58:28,317: ============================================================
2022-03-28 18:58:28,317: Epoch 35/45 Batch 6100/7662 eta: 11:09:53.317803	Training Loss 0.4157 (0.4318)	Training Prec@1 92.383 (92.086)	Training Prec@5 95.508 (95.089)	
2022-03-28 18:58:28,317: ============================================================
2022-03-28 18:59:16,808: time cost, forward:0.14779618467548467, backward:0.03737806000811839, data cost:0.2810884182405849 
2022-03-28 18:59:16,819: ============================================================
2022-03-28 18:59:16,820: Epoch 35/45 Batch 6200/7662 eta: 10:31:12.277117	Training Loss 0.4429 (0.4318)	Training Prec@1 92.383 (92.088)	Training Prec@5 95.312 (95.091)	
2022-03-28 18:59:16,820: ============================================================
2022-03-28 19:00:03,638: time cost, forward:0.14787981044226062, backward:0.03733199099582951, data cost:0.28105064452044604 
2022-03-28 19:00:03,639: ============================================================
2022-03-28 19:00:03,639: Epoch 35/45 Batch 6300/7662 eta: 10:08:31.138548	Training Loss 0.4205 (0.4318)	Training Prec@1 92.383 (92.090)	Training Prec@5 95.703 (95.092)	
2022-03-28 19:00:03,639: ============================================================
2022-03-28 19:00:53,859: time cost, forward:0.14879751503514133, backward:0.03739555237125206, data cost:0.2805843211688331 
2022-03-28 19:00:53,860: ============================================================
2022-03-28 19:00:53,860: Epoch 35/45 Batch 6400/7662 eta: 10:51:53.664847	Training Loss 0.4289 (0.4318)	Training Prec@1 91.211 (92.092)	Training Prec@5 94.922 (95.093)	
2022-03-28 19:00:53,860: ============================================================
2022-03-28 19:01:39,884: time cost, forward:0.14864955797326035, backward:0.03735054814094653, data cost:0.280682880743813 
2022-03-28 19:01:39,885: ============================================================
2022-03-28 19:01:39,885: Epoch 35/45 Batch 6500/7662 eta: 9:56:39.665325	Training Loss 0.4440 (0.4318)	Training Prec@1 91.016 (92.089)	Training Prec@5 94.141 (95.092)	
2022-03-28 19:01:39,885: ============================================================
2022-03-28 19:02:27,836: time cost, forward:0.1486935954795568, backward:0.03732161341986705, data cost:0.28084827502725124 
2022-03-28 19:02:27,837: ============================================================
2022-03-28 19:02:27,837: Epoch 35/45 Batch 6600/7662 eta: 10:20:50.449478	Training Loss 0.4337 (0.4318)	Training Prec@1 93.359 (92.089)	Training Prec@5 96.875 (95.092)	
2022-03-28 19:02:27,837: ============================================================
2022-03-28 19:03:16,377: time cost, forward:0.14915336141089464, backward:0.03732669469153175, data cost:0.28064649221451893 
2022-03-28 19:03:16,378: ============================================================
2022-03-28 19:03:16,378: Epoch 35/45 Batch 6700/7662 eta: 10:27:39.661610	Training Loss 0.4436 (0.4318)	Training Prec@1 91.406 (92.090)	Training Prec@5 94.141 (95.093)	
2022-03-28 19:03:16,378: ============================================================
2022-03-28 19:04:02,253: time cost, forward:0.1490712516850313, backward:0.037290189981215104, data cost:0.2806354639617498 
2022-03-28 19:04:02,253: ============================================================
2022-03-28 19:04:02,253: Epoch 35/45 Batch 6800/7662 eta: 9:52:25.495466	Training Loss 0.4363 (0.4318)	Training Prec@1 90.625 (92.087)	Training Prec@5 92.773 (95.089)	
2022-03-28 19:04:02,253: ============================================================
2022-03-28 19:04:50,652: time cost, forward:0.14917034048121086, backward:0.0372764484003534, data cost:0.28078491090950647 
2022-03-28 19:04:50,653: ============================================================
2022-03-28 19:04:50,653: Epoch 35/45 Batch 6900/7662 eta: 10:24:13.121160	Training Loss 0.4370 (0.4318)	Training Prec@1 93.164 (92.086)	Training Prec@5 95.703 (95.089)	
2022-03-28 19:04:50,653: ============================================================
2022-03-28 19:05:37,243: time cost, forward:0.1491006152734431, backward:0.03725931739480108, data cost:0.28083876542217956 
2022-03-28 19:05:37,243: ============================================================
2022-03-28 19:05:37,243: Epoch 35/45 Batch 7000/7662 eta: 10:00:06.521833	Training Loss 0.4294 (0.4318)	Training Prec@1 91.992 (92.087)	Training Prec@5 94.922 (95.089)	
2022-03-28 19:05:37,244: ============================================================
2022-03-28 19:06:24,201: time cost, forward:0.1489365086754975, backward:0.037232347669223946, data cost:0.28105157428198185 
2022-03-28 19:06:24,201: ============================================================
2022-03-28 19:06:24,201: Epoch 35/45 Batch 7100/7662 eta: 10:04:03.456003	Training Loss 0.4307 (0.4318)	Training Prec@1 93.555 (92.089)	Training Prec@5 96.094 (95.090)	
2022-03-28 19:06:24,201: ============================================================
2022-03-28 19:07:13,429: time cost, forward:0.14959721688446626, backward:0.03726556152415816, data cost:0.28069452167070646 
2022-03-28 19:07:13,429: ============================================================
2022-03-28 19:07:13,429: Epoch 35/45 Batch 7200/7662 eta: 10:32:26.350816	Training Loss 0.4227 (0.4318)	Training Prec@1 93.555 (92.089)	Training Prec@5 95.508 (95.089)	
2022-03-28 19:07:13,429: ============================================================
2022-03-28 19:08:00,908: time cost, forward:0.14998765487608182, backward:0.03727105500388103, data cost:0.2803774833483212 
2022-03-28 19:08:00,915: ============================================================
2022-03-28 19:08:00,916: Epoch 35/45 Batch 7300/7662 eta: 10:09:16.403835	Training Loss 0.4211 (0.4318)	Training Prec@1 93.359 (92.092)	Training Prec@5 95.312 (95.091)	
2022-03-28 19:08:00,916: ============================================================
2022-03-28 19:08:48,322: time cost, forward:0.15034071125360998, backward:0.03728412473374145, data cost:0.28008564296067123 
2022-03-28 19:08:48,322: ============================================================
2022-03-28 19:08:48,333: Epoch 35/45 Batch 7400/7662 eta: 10:07:35.530399	Training Loss 0.4329 (0.4318)	Training Prec@1 89.844 (92.091)	Training Prec@5 93.359 (95.090)	
2022-03-28 19:08:48,333: ============================================================
2022-03-28 19:09:33,816: time cost, forward:0.15019898672774087, backward:0.037258565576891375, data cost:0.28005246875095025 
2022-03-28 19:09:33,817: ============================================================
2022-03-28 19:09:33,817: Epoch 35/45 Batch 7500/7662 eta: 9:42:04.285780	Training Loss 0.4457 (0.4319)	Training Prec@1 92.773 (92.084)	Training Prec@5 94.727 (95.085)	
2022-03-28 19:09:33,817: ============================================================
2022-03-28 19:10:19,401: time cost, forward:0.1500456746682821, backward:0.03722812950901333, data cost:0.2800652826125723 
2022-03-28 19:10:19,401: ============================================================
2022-03-28 19:10:19,401: Epoch 35/45 Batch 7600/7662 eta: 9:42:35.522336	Training Loss 0.4203 (0.4318)	Training Prec@1 92.188 (92.087)	Training Prec@5 96.289 (95.086)	
2022-03-28 19:10:19,402: ============================================================
2022-03-28 19:10:49,637: Epoch: 35/45 eta: 9:42:06.804134	Training Loss 0.4296 (0.4318)	Training Prec@1 93.359 (92.089)	Training Prec@5 96.289 (95.088)
2022-03-28 19:10:49,638: ============================================================
2022-03-28 19:10:49,640: Save Checkpoint...
2022-03-28 19:10:49,642: ============================================================
2022-03-28 19:10:51,707: Save done!
2022-03-28 19:10:51,707: ============================================================
2022-03-28 19:11:51,610: time cost, forward:0.11586036344971319, backward:0.033165464497575854, data cost:0.4535389885757909 
2022-03-28 19:11:51,611: ============================================================
2022-03-28 19:11:51,611: Epoch 36/45 Batch 100/7662 eta: 12:43:58.178596	Training Loss 0.4410 (0.4312)	Training Prec@1 90.430 (92.245)	Training Prec@5 93.555 (95.263)	
2022-03-28 19:11:51,611: ============================================================
2022-03-28 19:12:34,809: time cost, forward:0.11204250134415362, backward:0.03263765962878663, data cost:0.37185713873436704 
2022-03-28 19:12:34,809: ============================================================
2022-03-28 19:12:34,809: Epoch 36/45 Batch 200/7662 eta: 9:10:12.517912	Training Loss 0.4283 (0.4315)	Training Prec@1 90.430 (92.172)	Training Prec@5 94.922 (95.172)	
2022-03-28 19:12:34,809: ============================================================
2022-03-28 19:13:16,836: time cost, forward:0.11110060430290707, backward:0.03280069915745968, data cost:0.34015193511809794 
2022-03-28 19:13:16,836: ============================================================
2022-03-28 19:13:16,836: Epoch 36/45 Batch 300/7662 eta: 8:54:35.527986	Training Loss 0.4442 (0.4320)	Training Prec@1 92.969 (92.110)	Training Prec@5 95.898 (95.122)	
2022-03-28 19:13:16,836: ============================================================
2022-03-28 19:14:01,728: time cost, forward:0.11266965734630001, backward:0.03262897720910553, data cost:0.3297822128859976 
2022-03-28 19:14:01,729: ============================================================
2022-03-28 19:14:01,729: Epoch 36/45 Batch 400/7662 eta: 9:30:17.617586	Training Loss 0.4333 (0.4319)	Training Prec@1 92.383 (92.111)	Training Prec@5 95.312 (95.120)	
2022-03-28 19:14:01,729: ============================================================
2022-03-28 19:14:47,034: time cost, forward:0.11712247097420549, backward:0.033225587470259124, data cost:0.3202229367946097 
2022-03-28 19:14:47,035: ============================================================
2022-03-28 19:14:47,035: Epoch 36/45 Batch 500/7662 eta: 9:34:47.516552	Training Loss 0.4269 (0.4318)	Training Prec@1 91.602 (92.103)	Training Prec@5 94.531 (95.102)	
2022-03-28 19:14:47,035: ============================================================
2022-03-28 19:15:31,772: time cost, forward:0.12031098478823551, backward:0.03356862506006715, data cost:0.31270863496401474 
2022-03-28 19:15:31,773: ============================================================
2022-03-28 19:15:31,773: Epoch 36/45 Batch 600/7662 eta: 9:26:50.274191	Training Loss 0.4416 (0.4318)	Training Prec@1 90.625 (92.094)	Training Prec@5 93.945 (95.110)	
2022-03-28 19:15:31,773: ============================================================
2022-03-28 19:16:17,481: time cost, forward:0.12113084097276941, backward:0.03381219271767634, data cost:0.3103092705913538 
2022-03-28 19:16:17,483: ============================================================
2022-03-28 19:16:17,483: Epoch 36/45 Batch 700/7662 eta: 9:38:23.253454	Training Loss 0.4204 (0.4317)	Training Prec@1 92.383 (92.098)	Training Prec@5 94.336 (95.111)	
2022-03-28 19:16:17,483: ============================================================
2022-03-28 19:17:04,728: time cost, forward:0.12681317150369006, backward:0.034467876181286175, data cost:0.30472823168070656 
2022-03-28 19:17:04,728: ============================================================
2022-03-28 19:17:04,728: Epoch 36/45 Batch 800/7662 eta: 9:57:02.142765	Training Loss 0.4284 (0.4318)	Training Prec@1 92.969 (92.078)	Training Prec@5 95.703 (95.094)	
2022-03-28 19:17:04,729: ============================================================
2022-03-28 19:17:50,359: time cost, forward:0.1292647570205875, backward:0.0352008348047004, data cost:0.3004405962611995 
2022-03-28 19:17:50,359: ============================================================
2022-03-28 19:17:50,360: Epoch 36/45 Batch 900/7662 eta: 9:35:52.355515	Training Loss 0.4458 (0.4318)	Training Prec@1 91.016 (92.073)	Training Prec@5 93.945 (95.086)	
2022-03-28 19:17:50,360: ============================================================
2022-03-28 19:18:38,345: time cost, forward:0.13392240530974395, backward:0.03567895349916873, data cost:0.2967620767033971 
2022-03-28 19:18:38,345: ============================================================
2022-03-28 19:18:38,345: Epoch 36/45 Batch 1000/7662 eta: 10:04:47.240333	Training Loss 0.4284 (0.4318)	Training Prec@1 91.406 (92.088)	Training Prec@5 94.531 (95.091)	
2022-03-28 19:18:38,345: ============================================================
2022-03-28 19:19:24,878: time cost, forward:0.13837691455455778, backward:0.03608987502774073, data cost:0.2917216390344205 
2022-03-28 19:19:24,879: ============================================================
2022-03-28 19:19:24,879: Epoch 36/45 Batch 1100/7662 eta: 9:45:42.556219	Training Loss 0.4189 (0.4318)	Training Prec@1 94.727 (92.102)	Training Prec@5 97.461 (95.094)	
2022-03-28 19:19:24,879: ============================================================
2022-03-28 19:20:09,374: time cost, forward:0.13868967208194177, backward:0.03623633507990261, data cost:0.28939657990787304 
2022-03-28 19:20:09,374: ============================================================
2022-03-28 19:20:09,374: Epoch 36/45 Batch 1200/7662 eta: 9:19:19.016395	Training Loss 0.4295 (0.4318)	Training Prec@1 93.359 (92.097)	Training Prec@5 95.117 (95.092)	
2022-03-28 19:20:09,375: ============================================================
2022-03-28 19:20:55,213: time cost, forward:0.1396012043751048, backward:0.0361595434624934, data cost:0.2880876093666952 
2022-03-28 19:20:55,214: ============================================================
2022-03-28 19:20:55,214: Epoch 36/45 Batch 1300/7662 eta: 9:35:27.021887	Training Loss 0.4400 (0.4318)	Training Prec@1 91.602 (92.094)	Training Prec@5 93.945 (95.087)	
2022-03-28 19:20:55,214: ============================================================
2022-03-28 19:21:42,612: time cost, forward:0.14082770674802986, backward:0.036178104701938586, data cost:0.2875320930153749 
2022-03-28 19:21:42,623: ============================================================
2022-03-28 19:21:42,623: Epoch 36/45 Batch 1400/7662 eta: 9:54:21.651162	Training Loss 0.4193 (0.4317)	Training Prec@1 94.531 (92.097)	Training Prec@5 97.070 (95.090)	
2022-03-28 19:21:42,624: ============================================================
2022-03-28 19:22:25,933: time cost, forward:0.1398399499673061, backward:0.0360144134200836, data cost:0.28647818034135797 
2022-03-28 19:22:25,934: ============================================================
2022-03-28 19:22:25,934: Epoch 36/45 Batch 1500/7662 eta: 9:02:15.416036	Training Loss 0.4234 (0.4316)	Training Prec@1 90.039 (92.098)	Training Prec@5 93.945 (95.094)	
2022-03-28 19:22:25,934: ============================================================
2022-03-28 19:23:14,029: time cost, forward:0.1425873709887993, backward:0.03612135409414209, data cost:0.2847954727397105 
2022-03-28 19:23:14,029: ============================================================
2022-03-28 19:23:14,030: Epoch 36/45 Batch 1600/7662 eta: 10:01:21.807863	Training Loss 0.4252 (0.4315)	Training Prec@1 93.945 (92.108)	Training Prec@5 97.070 (95.097)	
2022-03-28 19:23:14,030: ============================================================
2022-03-28 19:23:59,445: time cost, forward:0.14300814328858824, backward:0.036074677097439556, data cost:0.2837796282810348 
2022-03-28 19:23:59,446: ============================================================
2022-03-28 19:23:59,446: Epoch 36/45 Batch 1700/7662 eta: 9:27:06.378855	Training Loss 0.4296 (0.4315)	Training Prec@1 91.602 (92.100)	Training Prec@5 94.141 (95.092)	
2022-03-28 19:23:59,446: ============================================================
2022-03-28 19:24:44,900: time cost, forward:0.1436431710093203, backward:0.03600487780610742, data cost:0.2827780747691945 
2022-03-28 19:24:44,901: ============================================================
2022-03-28 19:24:44,901: Epoch 36/45 Batch 1800/7662 eta: 9:26:49.741891	Training Loss 0.4291 (0.4316)	Training Prec@1 91.406 (92.097)	Training Prec@5 95.117 (95.088)	
2022-03-28 19:24:44,901: ============================================================
2022-03-28 19:25:28,871: time cost, forward:0.1435755405255278, backward:0.03607093239533644, data cost:0.2815166796553191 
2022-03-28 19:25:28,871: ============================================================
2022-03-28 19:25:28,871: Epoch 36/45 Batch 1900/7662 eta: 9:07:35.151693	Training Loss 0.4209 (0.4315)	Training Prec@1 92.773 (92.097)	Training Prec@5 96.484 (95.088)	
2022-03-28 19:25:28,871: ============================================================
2022-03-28 19:26:13,818: time cost, forward:0.1425105945774172, backward:0.03598249059966232, data cost:0.28209947239702615 
2022-03-28 19:26:13,819: ============================================================
2022-03-28 19:26:13,819: Epoch 36/45 Batch 2000/7662 eta: 9:19:00.460952	Training Loss 0.4363 (0.4316)	Training Prec@1 91.992 (92.082)	Training Prec@5 95.117 (95.084)	
2022-03-28 19:26:13,819: ============================================================
2022-03-28 19:26:59,495: time cost, forward:0.1424566318217546, backward:0.03593889561762635, data cost:0.2819947146869603 
2022-03-28 19:26:59,495: ============================================================
2022-03-28 19:26:59,495: Epoch 36/45 Batch 2100/7662 eta: 9:27:18.461322	Training Loss 0.4375 (0.4317)	Training Prec@1 92.578 (92.083)	Training Prec@5 95.312 (95.084)	
2022-03-28 19:26:59,496: ============================================================
2022-03-28 19:27:45,956: time cost, forward:0.14304532187263658, backward:0.0360464127077846, data cost:0.28148262356562526 
2022-03-28 19:27:45,956: ============================================================
2022-03-28 19:27:45,956: Epoch 36/45 Batch 2200/7662 eta: 9:36:16.767045	Training Loss 0.4270 (0.4317)	Training Prec@1 92.969 (92.084)	Training Prec@5 96.289 (95.087)	
2022-03-28 19:27:45,957: ============================================================
2022-03-28 19:28:33,848: time cost, forward:0.14531210050628515, backward:0.036322641600832414, data cost:0.2796859013199443 
2022-03-28 19:28:33,859: ============================================================
2022-03-28 19:28:33,859: Epoch 36/45 Batch 2300/7662 eta: 9:53:21.806560	Training Loss 0.4388 (0.4317)	Training Prec@1 93.359 (92.092)	Training Prec@5 97.070 (95.089)	
2022-03-28 19:28:33,859: ============================================================
2022-03-28 19:29:18,040: time cost, forward:0.14487628133757108, backward:0.03622430351785244, data cost:0.27935600857180126 
2022-03-28 19:29:18,040: ============================================================
2022-03-28 19:29:18,040: Epoch 36/45 Batch 2400/7662 eta: 9:06:31.704052	Training Loss 0.4280 (0.4317)	Training Prec@1 91.602 (92.095)	Training Prec@5 94.922 (95.091)	
2022-03-28 19:29:18,041: ============================================================
2022-03-28 19:30:02,301: time cost, forward:0.14358649458013187, backward:0.03605876917264709, data cost:0.2800743484458908 
2022-03-28 19:30:02,301: ============================================================
2022-03-28 19:30:02,302: Epoch 36/45 Batch 2500/7662 eta: 9:06:46.901626	Training Loss 0.4385 (0.4317)	Training Prec@1 92.969 (92.091)	Training Prec@5 95.508 (95.085)	
2022-03-28 19:30:02,302: ============================================================
2022-03-28 19:30:46,728: time cost, forward:0.14223221046459497, backward:0.03589856675057376, data cost:0.28097652756007374 
2022-03-28 19:30:46,728: ============================================================
2022-03-28 19:30:46,728: Epoch 36/45 Batch 2600/7662 eta: 9:08:05.048337	Training Loss 0.4348 (0.4317)	Training Prec@1 91.406 (92.085)	Training Prec@5 94.727 (95.078)	
2022-03-28 19:30:46,728: ============================================================
2022-03-28 19:31:31,470: time cost, forward:0.1413568178341538, backward:0.035756664304390534, data cost:0.28154954401463567 
2022-03-28 19:31:31,471: ============================================================
2022-03-28 19:31:31,471: Epoch 36/45 Batch 2700/7662 eta: 9:11:14.205518	Training Loss 0.4332 (0.4317)	Training Prec@1 91.797 (92.082)	Training Prec@5 95.508 (95.078)	
2022-03-28 19:31:31,471: ============================================================
2022-03-28 19:32:15,500: time cost, forward:0.14014488673712705, backward:0.035763212253724565, data cost:0.282075173175944 
2022-03-28 19:32:15,500: ============================================================
2022-03-28 19:32:15,500: Epoch 36/45 Batch 2800/7662 eta: 9:01:42.908959	Training Loss 0.4232 (0.4317)	Training Prec@1 90.625 (92.087)	Training Prec@5 94.141 (95.080)	
2022-03-28 19:32:15,500: ============================================================
2022-03-28 19:32:59,329: time cost, forward:0.1389747006435072, backward:0.03571389682871097, data cost:0.282611539833461 
2022-03-28 19:32:59,330: ============================================================
2022-03-28 19:32:59,330: Epoch 36/45 Batch 2900/7662 eta: 8:58:31.625287	Training Loss 0.4198 (0.4317)	Training Prec@1 93.359 (92.091)	Training Prec@5 96.094 (95.081)	
2022-03-28 19:32:59,330: ============================================================
2022-03-28 19:33:44,535: time cost, forward:0.13855137742332874, backward:0.035674137765465276, data cost:0.28288171648303445 
2022-03-28 19:33:44,535: ============================================================
2022-03-28 19:33:44,536: Epoch 36/45 Batch 3000/7662 eta: 9:14:40.937661	Training Loss 0.4263 (0.4317)	Training Prec@1 92.188 (92.097)	Training Prec@5 95.508 (95.086)	
2022-03-28 19:33:44,536: ============================================================
2022-03-28 19:34:27,813: time cost, forward:0.13747979464474014, backward:0.03555184966250749, data cost:0.283284257865406 
2022-03-28 19:34:27,814: ============================================================
2022-03-28 19:34:27,814: Epoch 36/45 Batch 3100/7662 eta: 8:50:18.506460	Training Loss 0.4312 (0.4316)	Training Prec@1 91.406 (92.103)	Training Prec@5 94.531 (95.091)	
2022-03-28 19:34:27,814: ============================================================
2022-03-28 19:35:12,209: time cost, forward:0.13727233312248477, backward:0.03550175123939144, data cost:0.2831457963247678 
2022-03-28 19:35:12,209: ============================================================
2022-03-28 19:35:12,210: Epoch 36/45 Batch 3200/7662 eta: 9:03:15.900618	Training Loss 0.4190 (0.4317)	Training Prec@1 93.750 (92.103)	Training Prec@5 95.898 (95.088)	
2022-03-28 19:35:12,210: ============================================================
2022-03-28 19:35:56,506: time cost, forward:0.1368317666217102, backward:0.035492974362542465, data cost:0.28318464550042594 
2022-03-28 19:35:56,507: ============================================================
2022-03-28 19:35:56,507: Epoch 36/45 Batch 3300/7662 eta: 9:01:19.402763	Training Loss 0.4217 (0.4317)	Training Prec@1 93.164 (92.106)	Training Prec@5 95.508 (95.088)	
2022-03-28 19:35:56,507: ============================================================
2022-03-28 19:36:40,003: time cost, forward:0.13646900215440724, backward:0.03545717906867732, data cost:0.28296522387128326 
2022-03-28 19:36:40,003: ============================================================
2022-03-28 19:36:40,003: Epoch 36/45 Batch 3400/7662 eta: 8:50:48.299070	Training Loss 0.4279 (0.4317)	Training Prec@1 91.602 (92.101)	Training Prec@5 95.312 (95.089)	
2022-03-28 19:36:40,004: ============================================================
2022-03-28 19:37:23,465: time cost, forward:0.13559454991225345, backward:0.035349459469607984, data cost:0.2833306497763007 
2022-03-28 19:37:23,465: ============================================================
2022-03-28 19:37:23,466: Epoch 36/45 Batch 3500/7662 eta: 8:49:40.049949	Training Loss 0.4388 (0.4317)	Training Prec@1 92.969 (92.097)	Training Prec@5 96.875 (95.090)	
2022-03-28 19:37:23,466: ============================================================
2022-03-28 19:38:08,400: time cost, forward:0.13476499473760445, backward:0.035297441827022026, data cost:0.2840774656170174 
2022-03-28 19:38:08,400: ============================================================
2022-03-28 19:38:08,400: Epoch 36/45 Batch 3600/7662 eta: 9:06:51.701314	Training Loss 0.4342 (0.4317)	Training Prec@1 92.578 (92.096)	Training Prec@5 95.703 (95.092)	
2022-03-28 19:38:08,400: ============================================================
2022-03-28 19:38:52,866: time cost, forward:0.13396450370413188, backward:0.03524460512807351, data cost:0.2846764508825149 
2022-03-28 19:38:52,866: ============================================================
2022-03-28 19:38:52,866: Epoch 36/45 Batch 3700/7662 eta: 9:00:25.122331	Training Loss 0.4374 (0.4317)	Training Prec@1 91.211 (92.096)	Training Prec@5 95.117 (95.092)	
2022-03-28 19:38:52,867: ============================================================
2022-03-28 19:39:36,225: time cost, forward:0.133209968372595, backward:0.03518705406952105, data cost:0.28493200386471107 
2022-03-28 19:39:36,226: ============================================================
2022-03-28 19:39:36,226: Epoch 36/45 Batch 3800/7662 eta: 8:46:14.839675	Training Loss 0.4294 (0.4316)	Training Prec@1 93.164 (92.101)	Training Prec@5 95.898 (95.096)	
2022-03-28 19:39:36,226: ============================================================
2022-03-28 19:40:18,927: time cost, forward:0.13252948491687683, backward:0.035115935796956335, data cost:0.28500460679484013 
2022-03-28 19:40:18,928: ============================================================
2022-03-28 19:40:18,928: Epoch 36/45 Batch 3900/7662 eta: 8:37:33.335046	Training Loss 0.4351 (0.4316)	Training Prec@1 93.164 (92.099)	Training Prec@5 97.070 (95.099)	
2022-03-28 19:40:18,928: ============================================================
2022-03-28 19:41:02,705: time cost, forward:0.13187005043506742, backward:0.03508363034791367, data cost:0.2853281408525998 
2022-03-28 19:41:02,705: ============================================================
2022-03-28 19:41:02,706: Epoch 36/45 Batch 4000/7662 eta: 8:49:51.914982	Training Loss 0.4191 (0.4316)	Training Prec@1 95.703 (92.098)	Training Prec@5 96.875 (95.097)	
2022-03-28 19:41:02,706: ============================================================
2022-03-28 19:41:45,067: time cost, forward:0.131237010602167, backward:0.03502876196119314, data cost:0.28530667176564223 
2022-03-28 19:41:45,068: ============================================================
2022-03-28 19:41:45,068: Epoch 36/45 Batch 4100/7662 eta: 8:32:01.475689	Training Loss 0.4392 (0.4316)	Training Prec@1 90.625 (92.096)	Training Prec@5 94.141 (95.097)	
2022-03-28 19:41:45,068: ============================================================
2022-03-28 19:42:29,246: time cost, forward:0.13062443992131437, backward:0.03488982987137004, data cost:0.28580820852871536 
2022-03-28 19:42:29,247: ============================================================
2022-03-28 19:42:29,247: Epoch 36/45 Batch 4200/7662 eta: 8:53:14.923591	Training Loss 0.4337 (0.4316)	Training Prec@1 92.188 (92.095)	Training Prec@5 96.094 (95.095)	
2022-03-28 19:42:29,247: ============================================================
2022-03-28 19:43:12,466: time cost, forward:0.1300510724052825, backward:0.03483278481287578, data cost:0.2859816474120377 
2022-03-28 19:43:12,466: ============================================================
2022-03-28 19:43:12,466: Epoch 36/45 Batch 4300/7662 eta: 8:40:56.713086	Training Loss 0.4242 (0.4316)	Training Prec@1 93.750 (92.096)	Training Prec@5 96.094 (95.096)	
2022-03-28 19:43:12,467: ============================================================
2022-03-28 19:43:55,749: time cost, forward:0.12948893774474418, backward:0.03479579200146279, data cost:0.28616831968740647 
2022-03-28 19:43:55,750: ============================================================
2022-03-28 19:43:55,750: Epoch 36/45 Batch 4400/7662 eta: 8:40:59.847789	Training Loss 0.4280 (0.4316)	Training Prec@1 91.992 (92.102)	Training Prec@5 95.508 (95.101)	
2022-03-28 19:43:55,750: ============================================================
2022-03-28 19:44:40,136: time cost, forward:0.1289503354447554, backward:0.03476564871997244, data cost:0.28658850348295173 
2022-03-28 19:44:40,136: ============================================================
2022-03-28 19:44:40,136: Epoch 36/45 Batch 4500/7662 eta: 8:53:31.734470	Training Loss 0.4238 (0.4316)	Training Prec@1 92.969 (92.104)	Training Prec@5 96.680 (95.100)	
2022-03-28 19:44:40,136: ============================================================
2022-03-28 19:45:24,030: time cost, forward:0.12845788621000426, backward:0.03472740589522569, data cost:0.2868283925613234 
2022-03-28 19:45:24,030: ============================================================
2022-03-28 19:45:24,030: Epoch 36/45 Batch 4600/7662 eta: 8:46:52.969897	Training Loss 0.4336 (0.4316)	Training Prec@1 91.211 (92.101)	Training Prec@5 94.727 (95.098)	
2022-03-28 19:45:24,031: ============================================================
2022-03-28 19:46:07,965: time cost, forward:0.12829253693341347, backward:0.03469909310569203, data cost:0.28680858086413286 
2022-03-28 19:46:07,965: ============================================================
2022-03-28 19:46:07,965: Epoch 36/45 Batch 4700/7662 eta: 8:46:38.596593	Training Loss 0.4295 (0.4316)	Training Prec@1 93.750 (92.102)	Training Prec@5 96.094 (95.097)	
2022-03-28 19:46:07,966: ============================================================
2022-03-28 19:46:54,259: time cost, forward:0.12865067025129187, backward:0.03477621898225855, data cost:0.2866559667024893 
2022-03-28 19:46:54,260: ============================================================
2022-03-28 19:46:54,260: Epoch 36/45 Batch 4800/7662 eta: 9:14:09.200837	Training Loss 0.4365 (0.4316)	Training Prec@1 91.602 (92.098)	Training Prec@5 94.727 (95.095)	
2022-03-28 19:46:54,260: ============================================================
2022-03-28 19:47:40,404: time cost, forward:0.12880264785635787, backward:0.03478699149976436, data cost:0.28669494184870503 
2022-03-28 19:47:40,405: ============================================================
2022-03-28 19:47:40,405: Epoch 36/45 Batch 4900/7662 eta: 9:11:35.587380	Training Loss 0.4184 (0.4316)	Training Prec@1 94.531 (92.098)	Training Prec@5 95.898 (95.094)	
2022-03-28 19:47:40,405: ============================================================
2022-03-28 19:48:27,413: time cost, forward:0.12984562740489994, backward:0.03491605365483803, data cost:0.2859260583787137 
2022-03-28 19:48:27,414: ============================================================
2022-03-28 19:48:27,414: Epoch 36/45 Batch 5000/7662 eta: 9:21:08.424708	Training Loss 0.4196 (0.4316)	Training Prec@1 92.578 (92.099)	Training Prec@5 95.508 (95.094)	
2022-03-28 19:48:27,414: ============================================================
2022-03-28 19:49:11,738: time cost, forward:0.13039894991469023, backward:0.034973412874891, data cost:0.2851015744337406 
2022-03-28 19:49:11,739: ============================================================
2022-03-28 19:49:11,739: Epoch 36/45 Batch 5100/7662 eta: 8:48:21.735463	Training Loss 0.4398 (0.4316)	Training Prec@1 93.164 (92.098)	Training Prec@5 95.703 (95.092)	
2022-03-28 19:49:11,740: ============================================================
2022-03-28 19:49:56,608: time cost, forward:0.13081897531800327, backward:0.03500763987962914, data cost:0.2846389587257064 
2022-03-28 19:49:56,609: ============================================================
2022-03-28 19:49:56,609: Epoch 36/45 Batch 5200/7662 eta: 8:54:06.568392	Training Loss 0.4370 (0.4316)	Training Prec@1 92.969 (92.097)	Training Prec@5 95.117 (95.090)	
2022-03-28 19:49:56,609: ============================================================
2022-03-28 19:50:40,058: time cost, forward:0.1303774711207908, backward:0.03499538274413348, data cost:0.28477571896504267 
2022-03-28 19:50:40,058: ============================================================
2022-03-28 19:50:40,059: Epoch 36/45 Batch 5300/7662 eta: 8:36:28.863666	Training Loss 0.4386 (0.4316)	Training Prec@1 89.062 (92.097)	Training Prec@5 94.336 (95.091)	
2022-03-28 19:50:40,059: ============================================================
