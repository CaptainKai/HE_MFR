2022-03-30 07:12:27,065: [('name', 'amsoft-36'), ('backbone_model_name', 'SimpleResnet_36'), ('classify_model_name', 'Sphereface2'), ('resume_net_model', None), ('resume_net_classifier', None), ('no_cuda', False), ('gpu_num', 1), ('log_interval', 100), ('log_path', './logs/SR_36_ddp_sphereface2_1.log'), ('log_pic_path', './logs/pic/SR_36_ddp_sphereface2_1/'), ('save_path', 'snapshot/SR_36_ddp_sphereface2_1/'), ('lmdb_path', '/home/ubuntu/data4/lk/data/lmdb_default'), ('batch_size', 512), ('datanum', 3923399), ('num_class', 86876), ('lmdb_workers', 4), ('num_workers', 4), ('start_epoch', 1), ('max_epoch', 31), ('lr', 0.1), ('base', 'epoch'), ('step_size', [10, 20, 30, 40]), ('momentum', 0.9), ('gama', 0.1), ('weight_decay', 0.0005), ('rank', 0), ('dist_url', 'env://'), ('world_size', 2), ('gpu', 0), ('dist_backend', 'nccl'), ('distributed', True), ('master_port', 22000), ('multiprocessing_distributed', False), ('SEED', 1337), ('local_rank', 0)]
2022-03-30 07:12:27,065: SimpleResidualBackbone(
  (conv1): ConvPrelu(
    (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (prelu): PReLU(num_parameters=64)
  )
  (layer1): Sequential(
    (0): SimpleResidualUnit(
      (conv1): ConvPrelu(
        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=64)
      )
      (conv2): ConvPrelu(
        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=64)
      )
    )
    (1): SimpleResidualUnit(
      (conv1): ConvPrelu(
        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=64)
      )
      (conv2): ConvPrelu(
        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=64)
      )
    )
  )
  (conv2): ConvPrelu(
    (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (prelu): PReLU(num_parameters=128)
  )
  (layer2): Sequential(
    (0): SimpleResidualUnit(
      (conv1): ConvPrelu(
        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=128)
      )
      (conv2): ConvPrelu(
        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=128)
      )
    )
    (1): SimpleResidualUnit(
      (conv1): ConvPrelu(
        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=128)
      )
      (conv2): ConvPrelu(
        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=128)
      )
    )
    (2): SimpleResidualUnit(
      (conv1): ConvPrelu(
        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=128)
      )
      (conv2): ConvPrelu(
        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=128)
      )
    )
    (3): SimpleResidualUnit(
      (conv1): ConvPrelu(
        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=128)
      )
      (conv2): ConvPrelu(
        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=128)
      )
    )
  )
  (conv3): ConvPrelu(
    (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (prelu): PReLU(num_parameters=256)
  )
  (layer3): Sequential(
    (0): SimpleResidualUnit(
      (conv1): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
      (conv2): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
    )
    (1): SimpleResidualUnit(
      (conv1): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
      (conv2): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
    )
    (2): SimpleResidualUnit(
      (conv1): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
      (conv2): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
    )
    (3): SimpleResidualUnit(
      (conv1): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
      (conv2): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
    )
    (4): SimpleResidualUnit(
      (conv1): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
      (conv2): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
    )
    (5): SimpleResidualUnit(
      (conv1): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
      (conv2): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
    )
    (6): SimpleResidualUnit(
      (conv1): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
      (conv2): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
    )
    (7): SimpleResidualUnit(
      (conv1): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
      (conv2): ConvPrelu(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=256)
      )
    )
  )
  (conv4): ConvPrelu(
    (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (prelu): PReLU(num_parameters=512)
  )
  (layer4): Sequential(
    (0): SimpleResidualUnit(
      (conv1): ConvPrelu(
        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=512)
      )
      (conv2): ConvPrelu(
        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=512)
      )
    )
    (1): SimpleResidualUnit(
      (conv1): ConvPrelu(
        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=512)
      )
      (conv2): ConvPrelu(
        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (prelu): PReLU(num_parameters=512)
      )
    )
  )
  (fc5): Linear(in_features=25088, out_features=512, bias=True)
)
2022-03-30 07:12:27,569: data balance
2022-03-30 07:13:02,000: time cost, forward:0.12314639910302981, backward:0.041144638350515655, data cost:0.17956358495384755 
2022-03-30 07:13:02,000: ============================================================
2022-03-30 07:13:02,001: Epoch 1/31 Batch 100/7662 eta: 22:35:30.853137	Training Loss 0.8340 (0.8396)	Training Prec@1 0.000 (0.004)	Training Prec@5 0.000 (0.012)	
2022-03-30 07:13:02,001: ============================================================
2022-03-30 07:13:34,915: time cost, forward:0.12111088378944589, backward:0.03701386260027861, data cost:0.17815238387141397 
2022-03-30 07:13:34,916: ============================================================
2022-03-30 07:13:34,916: Epoch 1/31 Batch 200/7662 eta: 21:41:55.630917	Training Loss 0.8356 (0.8370)	Training Prec@1 0.000 (0.003)	Training Prec@5 0.000 (0.012)	
2022-03-30 07:13:34,916: ============================================================
2022-03-30 07:14:08,046: time cost, forward:0.12094416267497085, backward:0.03553625253530649, data cost:0.1780150175891991 
2022-03-30 07:14:08,047: ============================================================
2022-03-30 07:14:08,047: Epoch 1/31 Batch 300/7662 eta: 21:49:54.082446	Training Loss 0.8344 (0.8363)	Training Prec@1 0.000 (0.005)	Training Prec@5 0.000 (0.014)	
2022-03-30 07:14:08,047: ============================================================
2022-03-30 07:14:41,311: time cost, forward:0.12108461360883593, backward:0.03528330618875068, data cost:0.17758504131384062 
2022-03-30 07:14:41,312: ============================================================
2022-03-30 07:14:41,312: Epoch 1/31 Batch 400/7662 eta: 21:54:38.918710	Training Loss 0.8368 (0.8362)	Training Prec@1 0.000 (0.004)	Training Prec@5 0.000 (0.013)	
2022-03-30 07:14:41,312: ============================================================
2022-03-30 07:15:14,694: time cost, forward:0.121138049032024, backward:0.034971703030542284, data cost:0.17774336944839997 
2022-03-30 07:15:14,694: ============================================================
2022-03-30 07:15:14,695: Epoch 1/31 Batch 500/7662 eta: 21:58:44.247460	Training Loss 0.8355 (0.8363)	Training Prec@1 0.000 (0.005)	Training Prec@5 0.000 (0.013)	
2022-03-30 07:15:14,695: ============================================================
2022-03-30 07:15:48,691: time cost, forward:0.12115536468454911, backward:0.034790779792008696, data cost:0.17877622040762925 
2022-03-30 07:15:48,692: ============================================================
2022-03-30 07:15:48,692: Epoch 1/31 Batch 600/7662 eta: 22:22:27.919384	Training Loss 0.8347 (0.8363)	Training Prec@1 0.000 (0.004)	Training Prec@5 0.000 (0.014)	
2022-03-30 07:15:48,692: ============================================================
2022-03-30 07:16:22,596: time cost, forward:0.12117243836366055, backward:0.03459051306837788, data cost:0.17960123408676387 
2022-03-30 07:16:22,596: ============================================================
2022-03-30 07:16:22,597: Epoch 1/31 Batch 700/7662 eta: 22:18:14.002430	Training Loss 0.8348 (0.8361)	Training Prec@1 0.000 (0.004)	Training Prec@5 0.000 (0.017)	
2022-03-30 07:16:22,597: ============================================================
2022-03-30 07:16:57,267: time cost, forward:0.12086104720047627, backward:0.03431339914419773, data cost:0.18155294395656849 
2022-03-30 07:16:57,268: ============================================================
2022-03-30 07:16:57,268: Epoch 1/31 Batch 800/7662 eta: 22:47:55.049707	Training Loss 0.8327 (0.8358)	Training Prec@1 0.000 (0.005)	Training Prec@5 0.000 (0.019)	
2022-03-30 07:16:57,268: ============================================================
2022-03-30 07:17:32,548: time cost, forward:0.1203672782465136, backward:0.03408447043914286, data cost:0.1840269748573176 
2022-03-30 07:17:32,548: ============================================================
2022-03-30 07:17:32,549: Epoch 1/31 Batch 900/7662 eta: 23:11:21.933068	Training Loss 0.8305 (0.8354)	Training Prec@1 0.000 (0.005)	Training Prec@5 0.000 (0.022)	
2022-03-30 07:17:32,549: ============================================================
2022-03-30 07:18:07,885: time cost, forward:0.12004514499469562, backward:0.0341123584751133, data cost:0.1857871610719759 
2022-03-30 07:18:07,885: ============================================================
2022-03-30 07:18:07,886: Epoch 1/31 Batch 1000/7662 eta: 23:13:00.188085	Training Loss 0.8268 (0.8348)	Training Prec@1 0.000 (0.007)	Training Prec@5 0.000 (0.030)	
2022-03-30 07:18:07,886: ============================================================
2022-03-30 07:18:43,651: time cost, forward:0.11938663934771857, backward:0.03432074821027872, data cost:0.18779534616721988 
2022-03-30 07:18:43,651: ============================================================
2022-03-30 07:18:43,651: Epoch 1/31 Batch 1100/7662 eta: 23:29:18.199669	Training Loss 0.8272 (0.8341)	Training Prec@1 0.000 (0.012)	Training Prec@5 0.000 (0.044)	
2022-03-30 07:18:43,651: ============================================================
2022-03-30 07:19:19,855: time cost, forward:0.11888160896460348, backward:0.03418099174308618, data cost:0.1901255337967288 
2022-03-30 07:19:19,855: ============================================================
2022-03-30 07:19:19,855: Epoch 1/31 Batch 1200/7662 eta: 23:45:58.201428	Training Loss 0.8228 (0.8332)	Training Prec@1 0.000 (0.017)	Training Prec@5 0.000 (0.061)	
2022-03-30 07:19:19,855: ============================================================
2022-03-30 07:19:56,805: time cost, forward:0.11872552907677959, backward:0.03364599164033322, data cost:0.19280636246338362 
2022-03-30 07:19:56,806: ============================================================
2022-03-30 07:19:56,806: Epoch 1/31 Batch 1300/7662 eta: 1 day, 0:14:45.882080	Training Loss 0.8180 (0.8322)	Training Prec@1 0.000 (0.027)	Training Prec@5 0.195 (0.085)	
2022-03-30 07:19:56,806: ============================================================
2022-03-30 07:20:34,187: time cost, forward:0.11840823311904569, backward:0.033483345564814276, data cost:0.19531322701476658 
2022-03-30 07:20:34,187: ============================================================
2022-03-30 07:20:34,188: Epoch 1/31 Batch 1400/7662 eta: 1 day, 0:31:07.202544	Training Loss 0.8150 (0.8311)	Training Prec@1 0.000 (0.037)	Training Prec@5 0.000 (0.115)	
2022-03-30 07:20:34,188: ============================================================
2022-03-30 07:21:10,275: time cost, forward:0.11818411160978658, backward:0.033514262995296834, data cost:0.19639859444463625 
2022-03-30 07:21:10,275: ============================================================
2022-03-30 07:21:10,275: Epoch 1/31 Batch 1500/7662 eta: 23:39:35.084525	Training Loss 0.8096 (0.8299)	Training Prec@1 0.000 (0.047)	Training Prec@5 0.586 (0.148)	
2022-03-30 07:21:10,276: ============================================================
2022-03-30 07:21:48,524: time cost, forward:0.11791510608809079, backward:0.0335691708784837, data cost:0.19874694244499277 
2022-03-30 07:21:48,525: ============================================================
2022-03-30 07:21:48,525: Epoch 1/31 Batch 1600/7662 eta: 1 day, 1:03:59.061417	Training Loss 0.8095 (0.8285)	Training Prec@1 0.391 (0.067)	Training Prec@5 0.781 (0.203)	
2022-03-30 07:21:48,525: ============================================================
2022-03-30 07:22:25,180: time cost, forward:0.1177678012791769, backward:0.03355118469184395, data cost:0.19984265816359045 
2022-03-30 07:22:25,181: ============================================================
2022-03-30 07:22:25,181: Epoch 1/31 Batch 1700/7662 eta: 1 day, 0:00:43.247156	Training Loss 0.8012 (0.8271)	Training Prec@1 1.562 (0.097)	Training Prec@5 2.148 (0.279)	
2022-03-30 07:22:25,181: ============================================================
2022-03-30 07:23:04,200: time cost, forward:0.11759566160756525, backward:0.033507391503945265, data cost:0.2022079904587021 
2022-03-30 07:23:04,201: ============================================================
2022-03-30 07:23:04,201: Epoch 1/31 Batch 1800/7662 eta: 1 day, 1:32:59.445577	Training Loss 0.7968 (0.8256)	Training Prec@1 0.586 (0.138)	Training Prec@5 2.539 (0.380)	
2022-03-30 07:23:04,201: ============================================================
2022-03-30 07:23:40,948: time cost, forward:0.11727828148354223, backward:0.03355258536878669, data cost:0.20320783331872538 
2022-03-30 07:23:40,949: ============================================================
2022-03-30 07:23:40,949: Epoch 1/31 Batch 1900/7662 eta: 1 day, 0:03:06.433094	Training Loss 0.7942 (0.8240)	Training Prec@1 1.562 (0.194)	Training Prec@5 3.125 (0.514)	
2022-03-30 07:23:40,949: ============================================================
2022-03-30 07:24:20,467: time cost, forward:0.1168530329637017, backward:0.03367600481530438, data cost:0.20552396798145778 
2022-03-30 07:24:20,468: ============================================================
2022-03-30 07:24:20,468: Epoch 1/31 Batch 2000/7662 eta: 1 day, 1:51:16.699491	Training Loss 0.7894 (0.8222)	Training Prec@1 1.953 (0.268)	Training Prec@5 3.906 (0.681)	
2022-03-30 07:24:20,468: ============================================================
2022-03-30 07:24:59,622: time cost, forward:0.11652762132238467, backward:0.03389896353748425, data cost:0.20730130555460485 
2022-03-30 07:24:59,623: ============================================================
2022-03-30 07:24:59,623: Epoch 1/31 Batch 2100/7662 eta: 1 day, 1:36:19.897473	Training Loss 0.7833 (0.8204)	Training Prec@1 2.344 (0.362)	Training Prec@5 5.469 (0.888)	
2022-03-30 07:24:59,623: ============================================================
2022-03-30 07:25:38,823: time cost, forward:0.11623437895781347, backward:0.033917182247548715, data cost:0.20915606272334453 
2022-03-30 07:25:38,823: ============================================================
2022-03-30 07:25:38,823: Epoch 1/31 Batch 2200/7662 eta: 1 day, 1:37:27.941882	Training Loss 0.7751 (0.8185)	Training Prec@1 2.930 (0.485)	Training Prec@5 7.227 (1.150)	
2022-03-30 07:25:38,824: ============================================================
2022-03-30 07:26:19,323: time cost, forward:0.11601927758507648, backward:0.033997332443097925, data cost:0.21122346001119394 
2022-03-30 07:26:19,323: ============================================================
2022-03-30 07:26:19,324: Epoch 1/31 Batch 2300/7662 eta: 1 day, 2:27:45.543813	Training Loss 0.7671 (0.8165)	Training Prec@1 4.297 (0.644)	Training Prec@5 9.570 (1.466)	
2022-03-30 07:26:19,324: ============================================================
2022-03-30 07:27:00,721: time cost, forward:0.11583944776248017, backward:0.033963794159660644, data cost:0.21362678266257334 
2022-03-30 07:27:00,721: ============================================================
2022-03-30 07:27:00,721: Epoch 1/31 Batch 2400/7662 eta: 1 day, 3:02:15.727358	Training Loss 0.7598 (0.8144)	Training Prec@1 7.617 (0.842)	Training Prec@5 13.281 (1.845)	
2022-03-30 07:27:00,722: ============================================================
2022-03-30 07:27:40,995: time cost, forward:0.1157103710624875, backward:0.03391587385991995, data cost:0.21535583666297328 
2022-03-30 07:27:40,996: ============================================================
2022-03-30 07:27:40,996: Epoch 1/31 Batch 2500/7662 eta: 1 day, 2:17:34.992920	Training Loss 0.7517 (0.8122)	Training Prec@1 8.594 (1.078)	Training Prec@5 14.258 (2.279)	
2022-03-30 07:27:40,996: ============================================================
2022-03-30 07:28:19,739: time cost, forward:0.11554575039818453, backward:0.03389458795747834, data cost:0.2164191588753689 
2022-03-30 07:28:19,739: ============================================================
2022-03-30 07:28:19,740: Epoch 1/31 Batch 2600/7662 eta: 1 day, 1:16:57.008263	Training Loss 0.7470 (0.8100)	Training Prec@1 8.008 (1.354)	Training Prec@5 16.016 (2.762)	
2022-03-30 07:28:19,740: ============================================================
2022-03-30 07:28:59,603: time cost, forward:0.1155400944003444, backward:0.03386835938870973, data cost:0.21764565379851922 
2022-03-30 07:28:59,603: ============================================================
2022-03-30 07:28:59,603: Epoch 1/31 Batch 2700/7662 eta: 1 day, 2:00:09.610809	Training Loss 0.7411 (0.8077)	Training Prec@1 12.500 (1.670)	Training Prec@5 18.359 (3.294)	
2022-03-30 07:28:59,604: ============================================================
2022-03-30 07:29:41,033: time cost, forward:0.11543420418197234, backward:0.03381410459400543, data cost:0.219452647576804 
2022-03-30 07:29:41,033: ============================================================
2022-03-30 07:29:41,033: Epoch 1/31 Batch 2800/7662 eta: 1 day, 3:00:45.986410	Training Loss 0.7389 (0.8053)	Training Prec@1 14.844 (2.031)	Training Prec@5 22.656 (3.884)	
2022-03-30 07:29:41,034: ============================================================
2022-03-30 07:30:21,572: time cost, forward:0.11530351449638122, backward:0.033772510674625646, data cost:0.22088822778976142 
2022-03-30 07:30:21,572: ============================================================
2022-03-30 07:30:21,572: Epoch 1/31 Batch 2900/7662 eta: 1 day, 2:25:13.566887	Training Loss 0.7359 (0.8029)	Training Prec@1 16.016 (2.433)	Training Prec@5 23.633 (4.519)	
2022-03-30 07:30:21,573: ============================================================
2022-03-30 07:31:02,198: time cost, forward:0.11513668722055402, backward:0.03374853782870047, data cost:0.22227418418724007 
2022-03-30 07:31:02,199: ============================================================
2022-03-30 07:31:02,199: Epoch 1/31 Batch 3000/7662 eta: 1 day, 2:27:58.479071	Training Loss 0.7234 (0.8004)	Training Prec@1 14.648 (2.867)	Training Prec@5 25.781 (5.181)	
2022-03-30 07:31:02,199: ============================================================
2022-03-30 07:31:43,679: time cost, forward:0.11550627958009073, backward:0.03379443846582866, data cost:0.2232666924215971 
2022-03-30 07:31:43,679: ============================================================
2022-03-30 07:31:43,680: Epoch 1/31 Batch 3100/7662 eta: 1 day, 3:00:40.613828	Training Loss 0.7184 (0.7979)	Training Prec@1 18.164 (3.341)	Training Prec@5 27.539 (5.887)	
2022-03-30 07:31:43,680: ============================================================
2022-03-30 07:32:25,004: time cost, forward:0.11626700640991726, backward:0.03387811639302222, data cost:0.22368397746990903 
2022-03-30 07:32:25,004: ============================================================
2022-03-30 07:32:25,005: Epoch 1/31 Batch 3200/7662 eta: 1 day, 2:53:53.682091	Training Loss 0.7111 (0.7952)	Training Prec@1 20.312 (3.848)	Training Prec@5 31.445 (6.626)	
2022-03-30 07:32:25,005: ============================================================
2022-03-30 07:33:06,510: time cost, forward:0.11629223079166834, backward:0.03390853337787722, data cost:0.22486406956777027 
2022-03-30 07:33:06,510: ============================================================
2022-03-30 07:33:06,510: Epoch 1/31 Batch 3300/7662 eta: 1 day, 3:00:15.852973	Training Loss 0.7057 (0.7926)	Training Prec@1 22.070 (4.390)	Training Prec@5 33.984 (7.397)	
2022-03-30 07:33:06,510: ============================================================
2022-03-30 07:33:50,846: time cost, forward:0.1165756002248263, backward:0.0338997175217236, data cost:0.22659700048008397 
2022-03-30 07:33:50,846: ============================================================
2022-03-30 07:33:50,846: Epoch 1/31 Batch 3400/7662 eta: 1 day, 4:50:00.793295	Training Loss 0.6963 (0.7900)	Training Prec@1 25.195 (4.949)	Training Prec@5 38.086 (8.180)	
2022-03-30 07:33:50,846: ============================================================
2022-03-30 07:34:33,352: time cost, forward:0.11742965771014706, backward:0.033974768673225485, data cost:0.22703125811399955 
2022-03-30 07:34:33,352: ============================================================
2022-03-30 07:34:33,353: Epoch 1/31 Batch 3500/7662 eta: 1 day, 3:37:54.781739	Training Loss 0.6931 (0.7873)	Training Prec@1 24.414 (5.543)	Training Prec@5 38.086 (8.998)	
2022-03-30 07:34:33,353: ============================================================
2022-03-30 07:35:17,333: time cost, forward:0.11832346761183594, backward:0.034066847477664086, data cost:0.22770905554311147 
2022-03-30 07:35:17,333: ============================================================
2022-03-30 07:35:17,333: Epoch 1/31 Batch 3600/7662 eta: 1 day, 4:34:40.338218	Training Loss 0.6903 (0.7846)	Training Prec@1 26.562 (6.149)	Training Prec@5 34.961 (9.820)	
2022-03-30 07:35:17,333: ============================================================
2022-03-30 07:36:03,272: time cost, forward:0.11937123222846088, backward:0.034146235098094994, data cost:0.2287181701232176 
2022-03-30 07:36:03,272: ============================================================
2022-03-30 07:36:03,273: Epoch 1/31 Batch 3700/7662 eta: 1 day, 5:50:16.926646	Training Loss 0.6843 (0.7819)	Training Prec@1 28.125 (6.784)	Training Prec@5 40.820 (10.663)	
2022-03-30 07:36:03,273: ============================================================
2022-03-30 07:36:47,729: time cost, forward:0.1205484607276806, backward:0.03420447198677515, data cost:0.22912295061087853 
2022-03-30 07:36:47,730: ============================================================
2022-03-30 07:36:47,730: Epoch 1/31 Batch 3800/7662 eta: 1 day, 4:51:46.460389	Training Loss 0.8815 (0.7836)	Training Prec@1 0.000 (6.772)	Training Prec@5 0.000 (10.620)	
2022-03-30 07:36:47,730: ============================================================
2022-03-30 07:37:30,980: time cost, forward:0.12065676072524982, backward:0.03426402915774568, data cost:0.23016861341769465 
2022-03-30 07:37:30,981: ============================================================
2022-03-30 07:37:30,981: Epoch 1/31 Batch 3900/7662 eta: 1 day, 4:04:04.453446	Training Loss 0.8668 (0.7860)	Training Prec@1 0.000 (6.599)	Training Prec@5 0.000 (10.348)	
2022-03-30 07:37:30,981: ============================================================
2022-03-30 07:38:15,874: time cost, forward:0.12138016070685229, backward:0.03439191586674974, data cost:0.23092226017472386 
2022-03-30 07:38:15,875: ============================================================
2022-03-30 07:38:15,875: Epoch 1/31 Batch 4000/7662 eta: 1 day, 5:07:18.585190	Training Loss 0.8499 (0.7876)	Training Prec@1 0.000 (6.434)	Training Prec@5 0.195 (10.089)	
2022-03-30 07:38:15,875: ============================================================
2022-03-30 07:39:02,195: time cost, forward:0.1226115364015146, backward:0.034559981419883666, data cost:0.23136965726520645 
2022-03-30 07:39:02,195: ============================================================
2022-03-30 07:39:02,196: Epoch 1/31 Batch 4100/7662 eta: 1 day, 6:02:02.965380	Training Loss 0.8472 (0.7891)	Training Prec@1 0.000 (6.277)	Training Prec@5 0.000 (9.844)	
2022-03-30 07:39:02,196: ============================================================
2022-03-30 07:39:47,141: time cost, forward:0.12350377578853454, backward:0.03466656418691564, data cost:0.23182498980034077 
2022-03-30 07:39:47,142: ============================================================
2022-03-30 07:39:47,142: Epoch 1/31 Batch 4200/7662 eta: 1 day, 5:07:50.147962	Training Loss 0.8366 (0.7903)	Training Prec@1 0.195 (6.128)	Training Prec@5 0.195 (9.611)	
2022-03-30 07:39:47,142: ============================================================
2022-03-30 07:40:35,129: time cost, forward:0.12496712435731003, backward:0.03481274023808056, data cost:0.23229715707108542 
2022-03-30 07:40:35,129: ============================================================
2022-03-30 07:40:35,129: Epoch 1/31 Batch 4300/7662 eta: 1 day, 7:05:17.548937	Training Loss 0.8599 (0.7914)	Training Prec@1 0.000 (5.986)	Training Prec@5 0.000 (9.391)	
2022-03-30 07:40:35,130: ============================================================
2022-03-30 07:41:21,217: time cost, forward:0.12615312655206756, backward:0.0348944760474543, data cost:0.2325796699979192 
2022-03-30 07:41:21,217: ============================================================
2022-03-30 07:41:21,217: Epoch 1/31 Batch 4400/7662 eta: 1 day, 5:50:41.532177	Training Loss 0.8292 (0.7926)	Training Prec@1 0.000 (5.851)	Training Prec@5 0.195 (9.181)	
2022-03-30 07:41:21,218: ============================================================
2022-03-30 07:42:05,551: time cost, forward:0.12653186215801115, backward:0.03495602179538093, data cost:0.23323470313010097 
2022-03-30 07:42:05,552: ============================================================
2022-03-30 07:42:05,552: Epoch 1/31 Batch 4500/7662 eta: 1 day, 4:41:50.199902	Training Loss 0.8187 (0.7934)	Training Prec@1 0.000 (5.723)	Training Prec@5 0.195 (8.985)	
2022-03-30 07:42:05,552: ============================================================
2022-03-30 07:42:51,480: time cost, forward:0.12746218407405513, backward:0.03502069579230414, data cost:0.2336415866064647 
2022-03-30 07:42:51,480: ============================================================
2022-03-30 07:42:51,481: Epoch 1/31 Batch 4600/7662 eta: 1 day, 5:42:58.534154	Training Loss 0.8669 (0.7944)	Training Prec@1 0.000 (5.601)	Training Prec@5 0.000 (8.798)	
2022-03-30 07:42:51,481: ============================================================
2022-03-30 07:43:37,907: time cost, forward:0.12804097808607337, backward:0.03510092060270145, data cost:0.23442383450785959 
2022-03-30 07:43:37,908: ============================================================
2022-03-30 07:43:37,908: Epoch 1/31 Batch 4700/7662 eta: 1 day, 6:01:33.241117	Training Loss 0.8588 (0.7959)	Training Prec@1 0.000 (5.482)	Training Prec@5 0.000 (8.612)	
2022-03-30 07:43:37,908: ============================================================
2022-03-30 07:44:25,819: time cost, forward:0.12918095316432818, backward:0.03523879494362012, data cost:0.2348334614399399 
2022-03-30 07:44:25,819: ============================================================
2022-03-30 07:44:25,820: Epoch 1/31 Batch 4800/7662 eta: 1 day, 6:58:21.729827	Training Loss 0.8591 (0.7973)	Training Prec@1 0.000 (5.368)	Training Prec@5 0.000 (8.432)	
2022-03-30 07:44:25,820: ============================================================
2022-03-30 07:45:13,032: time cost, forward:0.13004023071502419, backward:0.03533464359930521, data cost:0.23535307681567816 
2022-03-30 07:45:13,033: ============================================================
2022-03-30 07:45:13,033: Epoch 1/31 Batch 4900/7662 eta: 1 day, 6:30:28.455748	Training Loss 0.8635 (0.7986)	Training Prec@1 0.000 (5.258)	Training Prec@5 0.000 (8.260)	
2022-03-30 07:45:13,033: ============================================================
2022-03-30 07:46:01,181: time cost, forward:0.1313948521592136, backward:0.035403644211126774, data cost:0.23552664720337255 
2022-03-30 07:46:01,181: ============================================================
2022-03-30 07:46:01,182: Epoch 1/31 Batch 5000/7662 eta: 1 day, 7:05:56.520629	Training Loss 0.8629 (0.7999)	Training Prec@1 0.000 (5.153)	Training Prec@5 0.000 (8.095)	
2022-03-30 07:46:01,182: ============================================================
2022-03-30 07:46:46,166: time cost, forward:0.13175212016033644, backward:0.03541610965216293, data cost:0.23607950968984576 
2022-03-30 07:46:46,166: ============================================================
2022-03-30 07:46:46,167: Epoch 1/31 Batch 5100/7662 eta: 1 day, 5:02:36.147949	Training Loss 0.8652 (0.8012)	Training Prec@1 0.000 (5.052)	Training Prec@5 0.000 (7.936)	
2022-03-30 07:46:46,167: ============================================================
2022-03-30 07:47:34,462: time cost, forward:0.13272149165242103, backward:0.03549916832739171, data cost:0.23655708924374413 
2022-03-30 07:47:34,463: ============================================================
2022-03-30 07:47:34,463: Epoch 1/31 Batch 5200/7662 eta: 1 day, 7:10:02.687286	Training Loss 0.8636 (0.8024)	Training Prec@1 0.000 (4.955)	Training Prec@5 0.000 (7.784)	
2022-03-30 07:47:34,463: ============================================================
2022-03-30 07:48:18,298: time cost, forward:0.13326223221876235, backward:0.03553438866041723, data cost:0.23659826305862372 
2022-03-30 07:48:18,299: ============================================================
2022-03-30 07:48:18,299: Epoch 1/31 Batch 5300/7662 eta: 1 day, 4:16:38.072582	Training Loss 0.8641 (0.8035)	Training Prec@1 0.000 (4.862)	Training Prec@5 0.000 (7.637)	
2022-03-30 07:48:18,299: ============================================================
2022-03-30 07:49:06,876: time cost, forward:0.1342405289008587, backward:0.035597627793093216, data cost:0.23704197486697975 
2022-03-30 07:49:06,877: ============================================================
2022-03-30 07:49:06,877: Epoch 1/31 Batch 5400/7662 eta: 1 day, 7:19:20.328130	Training Loss 0.8637 (0.8047)	Training Prec@1 0.000 (4.772)	Training Prec@5 0.000 (7.496)	
2022-03-30 07:49:06,877: ============================================================
2022-03-30 07:49:53,983: time cost, forward:0.13496788824053327, backward:0.03563923310271001, data cost:0.2374281024776777 
2022-03-30 07:49:53,983: ============================================================
2022-03-30 07:49:53,983: Epoch 1/31 Batch 5500/7662 eta: 1 day, 6:21:37.895479	Training Loss 0.8617 (0.8058)	Training Prec@1 0.000 (4.685)	Training Prec@5 0.000 (7.360)	
2022-03-30 07:49:53,984: ============================================================
2022-03-30 07:50:41,346: time cost, forward:0.1356910154806288, backward:0.03570035747767219, data cost:0.23779073939363451 
2022-03-30 07:50:41,347: ============================================================
2022-03-30 07:50:41,347: Epoch 1/31 Batch 5600/7662 eta: 1 day, 6:30:47.652153	Training Loss 0.8576 (0.8067)	Training Prec@1 0.000 (4.601)	Training Prec@5 0.000 (7.229)	
2022-03-30 07:50:41,348: ============================================================
2022-03-30 07:51:27,884: time cost, forward:0.13629886551391954, backward:0.0358161271549271, data cost:0.23806943716051537 
2022-03-30 07:51:27,884: ============================================================
2022-03-30 07:51:27,885: Epoch 1/31 Batch 5700/7662 eta: 1 day, 5:58:03.956310	Training Loss 0.8571 (0.8076)	Training Prec@1 0.000 (4.521)	Training Prec@5 0.000 (7.103)	
2022-03-30 07:51:27,885: ============================================================
2022-03-30 07:52:15,570: time cost, forward:0.13677447478223165, backward:0.03590738319039119, data cost:0.2386103900758783 
2022-03-30 07:52:15,570: ============================================================
2022-03-30 07:52:15,571: Epoch 1/31 Batch 5800/7662 eta: 1 day, 6:41:39.491909	Training Loss 0.8581 (0.8085)	Training Prec@1 0.000 (4.443)	Training Prec@5 0.000 (6.981)	
2022-03-30 07:52:15,571: ============================================================
2022-03-30 07:52:58,241: time cost, forward:0.13659309148586368, backward:0.03592373205657975, data cost:0.23906090518623554 
2022-03-30 07:52:58,242: ============================================================
2022-03-30 07:52:58,242: Epoch 1/31 Batch 5900/7662 eta: 1 day, 3:27:16.925535	Training Loss 0.8584 (0.8093)	Training Prec@1 0.000 (4.368)	Training Prec@5 0.000 (6.863)	
2022-03-30 07:52:58,242: ============================================================
2022-03-30 07:53:48,027: time cost, forward:0.1374281823863624, backward:0.035989585330554255, data cost:0.2395791022215988 
2022-03-30 07:53:48,027: ============================================================
2022-03-30 07:53:48,027: Epoch 1/31 Batch 6000/7662 eta: 1 day, 8:01:04.375445	Training Loss 0.8505 (0.8100)	Training Prec@1 0.000 (4.295)	Training Prec@5 0.000 (6.749)	
2022-03-30 07:53:48,028: ============================================================
2022-03-30 07:54:32,883: time cost, forward:0.1378437360987778, backward:0.03601257850702013, data cost:0.23971064248267188 
2022-03-30 07:54:32,883: ============================================================
2022-03-30 07:54:32,883: Epoch 1/31 Batch 6100/7662 eta: 1 day, 4:50:07.043374	Training Loss 0.8499 (0.8107)	Training Prec@1 0.000 (4.225)	Training Prec@5 0.000 (6.640)	
2022-03-30 07:54:32,884: ============================================================
2022-03-30 07:55:21,266: time cost, forward:0.13847816388517412, backward:0.036068002530347344, data cost:0.24014236800496397 
2022-03-30 07:55:21,266: ============================================================
2022-03-30 07:55:21,266: Epoch 1/31 Batch 6200/7662 eta: 1 day, 7:05:21.337210	Training Loss 0.8485 (0.8113)	Training Prec@1 0.000 (4.157)	Training Prec@5 0.000 (6.533)	
2022-03-30 07:55:21,267: ============================================================
2022-03-30 07:56:08,104: time cost, forward:0.1390854202200258, backward:0.036101852869075295, data cost:0.24035900565324692 
2022-03-30 07:56:08,104: ============================================================
2022-03-30 07:56:08,104: Epoch 1/31 Batch 6300/7662 eta: 1 day, 6:04:59.851802	Training Loss 0.8462 (0.8119)	Training Prec@1 0.000 (4.091)	Training Prec@5 0.195 (6.431)	
2022-03-30 07:56:08,104: ============================================================
2022-03-30 07:56:56,286: time cost, forward:0.13973045155375727, backward:0.03615461675277593, data cost:0.2406874689986546 
2022-03-30 07:56:56,286: ============================================================
2022-03-30 07:56:56,287: Epoch 1/31 Batch 6400/7662 eta: 1 day, 6:56:01.362699	Training Loss 0.8467 (0.8125)	Training Prec@1 0.195 (4.027)	Training Prec@5 0.195 (6.332)	
2022-03-30 07:56:56,288: ============================================================
2022-03-30 07:57:44,458: time cost, forward:0.1403498527801556, backward:0.03621190664382655, data cost:0.24099452104141464 
2022-03-30 07:57:44,458: ============================================================
2022-03-30 07:57:44,458: Epoch 1/31 Batch 6500/7662 eta: 1 day, 6:54:46.433590	Training Loss 0.8434 (0.8130)	Training Prec@1 0.000 (3.966)	Training Prec@5 0.000 (6.237)	
2022-03-30 07:57:44,458: ============================================================
2022-03-30 07:58:27,483: time cost, forward:0.14047641612522746, backward:0.036281032478435414, data cost:0.24099191403204717 
2022-03-30 07:58:27,484: ============================================================
2022-03-30 07:58:27,485: Epoch 1/31 Batch 6600/7662 eta: 1 day, 3:35:57.801024	Training Loss 0.8411 (0.8134)	Training Prec@1 0.000 (3.906)	Training Prec@5 0.000 (6.145)	
2022-03-30 07:58:27,485: ============================================================
2022-03-30 07:59:13,595: time cost, forward:0.14065853203031656, backward:0.036313418577564066, data cost:0.24142602401485122 
2022-03-30 07:59:13,595: ============================================================
2022-03-30 07:59:13,595: Epoch 1/31 Batch 6700/7662 eta: 1 day, 5:33:53.942869	Training Loss 0.8400 (0.8138)	Training Prec@1 0.000 (3.848)	Training Prec@5 0.195 (6.056)	
2022-03-30 07:59:13,595: ============================================================
2022-03-30 07:59:58,518: time cost, forward:0.14042577506479295, backward:0.03631100400017717, data cost:0.24210479761295625 
2022-03-30 07:59:58,519: ============================================================
2022-03-30 07:59:58,519: Epoch 1/31 Batch 6800/7662 eta: 1 day, 4:47:29.767891	Training Loss 0.8362 (0.8142)	Training Prec@1 0.000 (3.793)	Training Prec@5 0.195 (5.972)	
2022-03-30 07:59:58,519: ============================================================
2022-03-30 08:00:45,873: time cost, forward:0.14077088811774585, backward:0.0363734550520309, data cost:0.2424559498165013 
2022-03-30 08:00:45,873: ============================================================
2022-03-30 08:00:45,873: Epoch 1/31 Batch 6900/7662 eta: 1 day, 6:20:10.033805	Training Loss 0.8289 (0.8144)	Training Prec@1 0.000 (3.739)	Training Prec@5 0.586 (5.890)	
2022-03-30 08:00:45,874: ============================================================
2022-03-30 08:01:30,548: time cost, forward:0.14088118612842912, backward:0.03642529939852335, data cost:0.24268233484839657 
2022-03-30 08:01:30,548: ============================================================
2022-03-30 08:01:30,549: Epoch 1/31 Batch 7000/7662 eta: 1 day, 4:36:26.769001	Training Loss 0.8303 (0.8147)	Training Prec@1 0.000 (3.687)	Training Prec@5 0.195 (5.812)	
2022-03-30 08:01:30,549: ============================================================
2022-03-30 08:02:16,117: time cost, forward:0.14102324036951383, backward:0.036514435440474016, data cost:0.2429581520036503 
2022-03-30 08:02:16,118: ============================================================
2022-03-30 08:02:16,118: Epoch 1/31 Batch 7100/7662 eta: 1 day, 5:10:02.243583	Training Loss 0.8286 (0.8149)	Training Prec@1 0.000 (3.637)	Training Prec@5 0.195 (5.736)	
2022-03-30 08:02:16,118: ============================================================
2022-03-30 08:03:02,291: time cost, forward:0.14121811405488693, backward:0.03658393317253197, data cost:0.24327183654033768 
2022-03-30 08:03:02,291: ============================================================
2022-03-30 08:03:02,292: Epoch 1/31 Batch 7200/7662 eta: 1 day, 5:32:27.982973	Training Loss 0.8235 (0.8152)	Training Prec@1 0.391 (3.588)	Training Prec@5 0.781 (5.662)	
2022-03-30 08:03:02,292: ============================================================
2022-03-30 08:03:47,545: time cost, forward:0.14108956047959126, backward:0.03660626659035307, data cost:0.24379298520065332 
2022-03-30 08:03:47,545: ============================================================
2022-03-30 08:03:47,545: Epoch 1/31 Batch 7300/7662 eta: 1 day, 4:56:24.493855	Training Loss 0.8255 (0.8154)	Training Prec@1 0.586 (3.541)	Training Prec@5 1.172 (5.592)	
2022-03-30 08:03:47,546: ============================================================
2022-03-30 08:04:34,840: time cost, forward:0.14146543747831927, backward:0.03670497081749889, data cost:0.24399969732137866 
2022-03-30 08:04:34,841: ============================================================
2022-03-30 08:04:34,841: Epoch 1/31 Batch 7400/7662 eta: 1 day, 6:13:58.137937	Training Loss 0.8268 (0.8155)	Training Prec@1 0.391 (3.496)	Training Prec@5 0.586 (5.527)	
2022-03-30 08:04:34,841: ============================================================
2022-03-30 08:05:22,566: time cost, forward:0.14201976490236948, backward:0.03675916983709604, data cost:0.24412625521751288 
2022-03-30 08:05:22,567: ============================================================
2022-03-30 08:05:22,567: Epoch 1/31 Batch 7500/7662 eta: 1 day, 6:29:41.035995	Training Loss 0.8151 (0.8156)	Training Prec@1 0.586 (3.453)	Training Prec@5 1.172 (5.464)	
2022-03-30 08:05:22,567: ============================================================
2022-03-30 08:06:11,036: time cost, forward:0.14260336467664106, backward:0.03678446001780003, data cost:0.24432853363520032 
2022-03-30 08:06:11,036: ============================================================
2022-03-30 08:06:11,037: Epoch 1/31 Batch 7600/7662 eta: 1 day, 6:57:22.741876	Training Loss 0.8219 (0.8156)	Training Prec@1 0.195 (3.412)	Training Prec@5 0.781 (5.405)	
2022-03-30 08:06:11,037: ============================================================
2022-03-30 08:06:39,456: Epoch: 1/31 eta: 1 day, 6:56:52.206032	Training Loss 0.8074 (0.8156)	Training Prec@1 0.781 (3.387)	Training Prec@5 2.539 (5.371)
2022-03-30 08:06:39,456: ============================================================
2022-03-30 08:07:26,409: time cost, forward:0.15430324968665537, backward:0.043851262391215624, data cost:0.27208650232565523 
2022-03-30 08:07:26,410: ============================================================
2022-03-30 08:07:26,410: Epoch 2/31 Batch 100/7662 eta: 1 day, 5:51:42.835476	Training Loss 0.8052 (0.8065)	Training Prec@1 0.195 (0.543)	Training Prec@5 0.781 (1.663)	
2022-03-30 08:07:26,410: ============================================================
2022-03-30 08:08:14,819: time cost, forward:0.16834327084335252, backward:0.04462824275146177, data cost:0.26376612342182715 
2022-03-30 08:08:14,819: ============================================================
2022-03-30 08:08:14,819: Epoch 2/31 Batch 200/7662 eta: 1 day, 6:52:56.942646	Training Loss 0.7938 (0.8024)	Training Prec@1 0.977 (0.694)	Training Prec@5 2.539 (2.038)	
2022-03-30 08:08:14,819: ============================================================
2022-03-30 08:09:02,570: time cost, forward:0.1737499683597016, backward:0.0441378773654185, data cost:0.2589471140832805 
2022-03-30 08:09:02,570: ============================================================
2022-03-30 08:09:02,570: Epoch 2/31 Batch 300/7662 eta: 1 day, 6:26:57.157919	Training Loss 0.7931 (0.8001)	Training Prec@1 0.977 (0.839)	Training Prec@5 3.125 (2.403)	
2022-03-30 08:09:02,570: ============================================================
2022-03-30 08:09:50,908: time cost, forward:0.17403992734158547, backward:0.04354024650459003, data cost:0.2604754461082898 
2022-03-30 08:09:50,909: ============================================================
2022-03-30 08:09:50,910: Epoch 2/31 Batch 400/7662 eta: 1 day, 6:48:40.144064	Training Loss 0.7750 (0.7974)	Training Prec@1 3.320 (0.987)	Training Prec@5 8.008 (2.814)	
2022-03-30 08:09:50,910: ============================================================
2022-03-30 08:10:35,406: time cost, forward:0.169233299209503, backward:0.042565838846271645, data cost:0.2594809943067287 
2022-03-30 08:10:35,407: ============================================================
2022-03-30 08:10:35,407: Epoch 2/31 Batch 500/7662 eta: 1 day, 4:21:00.125752	Training Loss 0.7720 (0.7935)	Training Prec@1 2.539 (1.325)	Training Prec@5 6.836 (3.561)	
2022-03-30 08:10:35,407: ============================================================
2022-03-30 08:11:23,794: time cost, forward:0.172138479993817, backward:0.04196891760786308, data cost:0.2593421557908862 
2022-03-30 08:11:23,795: ============================================================
2022-03-30 08:11:23,795: Epoch 2/31 Batch 600/7662 eta: 1 day, 6:48:54.278131	Training Loss 0.7592 (0.7888)	Training Prec@1 6.250 (1.865)	Training Prec@5 13.086 (4.626)	
2022-03-30 08:11:23,795: ============================================================
2022-03-30 08:12:10,331: time cost, forward:0.1715224672625846, backward:0.041486802189817415, data cost:0.25911252590719724 
2022-03-30 08:12:10,331: ============================================================
2022-03-30 08:12:10,331: Epoch 2/31 Batch 700/7662 eta: 1 day, 5:37:23.121888	Training Loss 0.7459 (0.7838)	Training Prec@1 7.422 (2.611)	Training Prec@5 15.820 (5.943)	
2022-03-30 08:12:10,331: ============================================================
2022-03-30 08:12:58,143: time cost, forward:0.17234864193148847, backward:0.041703207173544414, data cost:0.2587818574248924 
2022-03-30 08:12:58,144: ============================================================
2022-03-30 08:12:58,144: Epoch 2/31 Batch 800/7662 eta: 1 day, 6:25:20.249369	Training Loss 0.7382 (0.7790)	Training Prec@1 11.914 (3.497)	Training Prec@5 20.508 (7.360)	
2022-03-30 08:12:58,144: ============================================================
2022-03-30 08:13:44,976: time cost, forward:0.17160185611287798, backward:0.0412787648541511, data cost:0.2593115362627753 
2022-03-30 08:13:44,976: ============================================================
2022-03-30 08:13:44,976: Epoch 2/31 Batch 900/7662 eta: 1 day, 5:47:08.171745	Training Loss 0.7570 (0.7785)	Training Prec@1 7.422 (3.732)	Training Prec@5 13.867 (7.739)	
2022-03-30 08:13:44,977: ============================================================
2022-03-30 08:14:32,927: time cost, forward:0.1710221261472196, backward:0.040827054280537865, data cost:0.2609993046349114 
2022-03-30 08:14:32,928: ============================================================
2022-03-30 08:14:32,928: Epoch 2/31 Batch 1000/7662 eta: 1 day, 6:29:02.361394	Training Loss 0.7431 (0.7766)	Training Prec@1 11.719 (4.129)	Training Prec@5 18.164 (8.390)	
2022-03-30 08:14:32,928: ============================================================
2022-03-30 08:15:21,185: time cost, forward:0.17215479711058793, backward:0.04074469840559122, data cost:0.26067753614784916 
2022-03-30 08:15:21,185: ============================================================
2022-03-30 08:15:21,185: Epoch 2/31 Batch 1100/7662 eta: 1 day, 6:39:54.057991	Training Loss 0.7320 (0.7728)	Training Prec@1 13.867 (4.936)	Training Prec@5 22.852 (9.616)	
2022-03-30 08:15:21,186: ============================================================
2022-03-30 08:16:07,403: time cost, forward:0.1723054248755728, backward:0.04078834110543169, data cost:0.25946091452273257 
2022-03-30 08:16:07,403: ============================================================
2022-03-30 08:16:07,403: Epoch 2/31 Batch 1200/7662 eta: 1 day, 5:21:22.462872	Training Loss 0.7186 (0.7688)	Training Prec@1 16.992 (5.854)	Training Prec@5 27.539 (10.927)	
2022-03-30 08:16:07,404: ============================================================
2022-03-30 08:16:53,673: time cost, forward:0.17136997769115703, backward:0.04069010141356529, data cost:0.2597748115486691 
2022-03-30 08:16:53,674: ============================================================
2022-03-30 08:16:53,674: Epoch 2/31 Batch 1300/7662 eta: 1 day, 5:22:36.342445	Training Loss 0.7165 (0.7650)	Training Prec@1 16.602 (6.777)	Training Prec@5 30.469 (12.222)	
2022-03-30 08:16:53,674: ============================================================
2022-03-30 08:17:38,532: time cost, forward:0.17011302739403092, backward:0.04051628276396854, data cost:0.2594377110053847 
2022-03-30 08:17:38,533: ============================================================
2022-03-30 08:17:38,533: Epoch 2/31 Batch 1400/7662 eta: 1 day, 4:28:05.538883	Training Loss 0.7137 (0.7614)	Training Prec@1 20.312 (7.702)	Training Prec@5 30.664 (13.475)	
2022-03-30 08:17:38,533: ============================================================
2022-03-30 08:18:27,210: time cost, forward:0.17112230841043077, backward:0.04035719209229493, data cost:0.25968226295379576 
2022-03-30 08:18:27,210: ============================================================
2022-03-30 08:18:27,211: Epoch 2/31 Batch 1500/7662 eta: 1 day, 6:52:40.511821	Training Loss 0.7180 (0.7579)	Training Prec@1 17.969 (8.598)	Training Prec@5 28.516 (14.676)	
2022-03-30 08:18:27,211: ============================================================
2022-03-30 08:19:15,598: time cost, forward:0.17150606640880744, backward:0.04026647088227979, data cost:0.26012344118801783 
2022-03-30 08:19:15,599: ============================================================
2022-03-30 08:19:15,599: Epoch 2/31 Batch 1600/7662 eta: 1 day, 6:40:52.353112	Training Loss 0.6986 (0.7545)	Training Prec@1 23.633 (9.494)	Training Prec@5 34.570 (15.845)	
2022-03-30 08:19:15,599: ============================================================
2022-03-30 08:20:01,137: time cost, forward:0.17114087579669918, backward:0.04010971143990843, data cost:0.2596628345974478 
2022-03-30 08:20:01,138: ============================================================
2022-03-30 08:20:01,138: Epoch 2/31 Batch 1700/7662 eta: 1 day, 4:51:41.435500	Training Loss 0.6842 (0.7512)	Training Prec@1 25.977 (10.381)	Training Prec@5 39.648 (16.988)	
2022-03-30 08:20:01,138: ============================================================
2022-03-30 08:20:47,283: time cost, forward:0.17061988495004515, backward:0.04000485466877045, data cost:0.25971369931537486 
2022-03-30 08:20:47,283: ============================================================
2022-03-30 08:20:47,284: Epoch 2/31 Batch 1800/7662 eta: 1 day, 5:14:01.078875	Training Loss 0.6875 (0.7481)	Training Prec@1 27.148 (11.261)	Training Prec@5 39.453 (18.099)	
2022-03-30 08:20:47,284: ============================================================
2022-03-30 08:21:34,745: time cost, forward:0.17101878778378293, backward:0.03995998399140899, data cost:0.2595623689554063 
2022-03-30 08:21:34,745: ============================================================
2022-03-30 08:21:34,745: Epoch 2/31 Batch 1900/7662 eta: 1 day, 6:03:13.922907	Training Loss 0.6886 (0.7450)	Training Prec@1 27.930 (12.133)	Training Prec@5 36.719 (19.177)	
2022-03-30 08:21:34,746: ============================================================
2022-03-30 08:22:18,116: time cost, forward:0.1703791658898602, backward:0.039553753908662094, data cost:0.2587350937889599 
2022-03-30 08:22:18,117: ============================================================
2022-03-30 08:22:18,117: Epoch 2/31 Batch 2000/7662 eta: 1 day, 3:27:07.118280	Training Loss nan (nan)	Training Prec@1 0.000 (12.293)	Training Prec@5 0.000 (19.295)	
2022-03-30 08:22:18,117: ============================================================
2022-03-30 08:23:04,431: time cost, forward:0.17167534142122093, backward:0.039493584189885224, data cost:0.25719727374872403 
2022-03-30 08:23:04,431: ============================================================
2022-03-30 08:23:04,432: Epoch 2/31 Batch 2100/7662 eta: 1 day, 5:18:06.478249	Training Loss nan (nan)	Training Prec@1 0.000 (11.707)	Training Prec@5 0.000 (18.376)	
2022-03-30 08:23:04,432: ============================================================
2022-03-30 08:23:43,880: time cost, forward:0.170856187646093, backward:0.03931072431566933, data cost:0.25481886872382203 
2022-03-30 08:23:43,881: ============================================================
2022-03-30 08:23:43,881: Epoch 2/31 Batch 2200/7662 eta: 1 day, 0:56:50.521722	Training Loss nan (nan)	Training Prec@1 0.000 (11.175)	Training Prec@5 0.000 (17.541)	
2022-03-30 08:23:43,881: ============================================================
2022-03-30 08:24:23,477: time cost, forward:0.1699539256541819, backward:0.03917764549205593, data cost:0.252825959654879 
2022-03-30 08:24:23,478: ============================================================
2022-03-30 08:24:23,478: Epoch 2/31 Batch 2300/7662 eta: 1 day, 1:01:47.376649	Training Loss nan (nan)	Training Prec@1 0.000 (10.689)	Training Prec@5 0.000 (16.778)	
2022-03-30 08:24:23,478: ============================================================
2022-03-30 08:25:02,406: time cost, forward:0.16932540965507606, backward:0.03904612842526422, data cost:0.2505521017991289 
2022-03-30 08:25:02,406: ============================================================
2022-03-30 08:25:02,406: Epoch 2/31 Batch 2400/7662 eta: 1 day, 0:35:46.937233	Training Loss nan (nan)	Training Prec@1 0.000 (10.243)	Training Prec@5 0.000 (16.079)	
2022-03-30 08:25:02,406: ============================================================
2022-03-30 08:25:40,499: time cost, forward:0.16819695130783638, backward:0.038908396305299464, data cost:0.24865568900594906 
2022-03-30 08:25:40,499: ============================================================
2022-03-30 08:25:40,499: Epoch 2/31 Batch 2500/7662 eta: 1 day, 0:03:29.046772	Training Loss nan (nan)	Training Prec@1 0.000 (9.833)	Training Prec@5 0.000 (15.435)	
2022-03-30 08:25:40,500: ============================================================
2022-03-30 08:26:20,299: time cost, forward:0.1677253906797473, backward:0.03882472503180685, data cost:0.24699517266940227 
2022-03-30 08:26:20,299: ============================================================
2022-03-30 08:26:20,300: Epoch 2/31 Batch 2600/7662 eta: 1 day, 1:07:30.477700	Training Loss nan (nan)	Training Prec@1 0.000 (9.455)	Training Prec@5 0.000 (14.842)	
2022-03-30 08:26:20,300: ============================================================
2022-03-30 08:27:01,379: time cost, forward:0.16758836380151873, backward:0.038760237094869254, data cost:0.24556438867230468 
2022-03-30 08:27:01,379: ============================================================
2022-03-30 08:27:01,380: Epoch 2/31 Batch 2700/7662 eta: 1 day, 1:55:17.145581	Training Loss nan (nan)	Training Prec@1 0.000 (9.105)	Training Prec@5 0.000 (14.292)	
2022-03-30 08:27:01,380: ============================================================
2022-03-30 08:27:41,027: time cost, forward:0.16725047454956643, backward:0.03864172400215261, data cost:0.24403303143977267 
2022-03-30 08:27:41,027: ============================================================
2022-03-30 08:27:41,028: Epoch 2/31 Batch 2800/7662 eta: 1 day, 1:00:25.347210	Training Loss nan (nan)	Training Prec@1 0.000 (8.780)	Training Prec@5 0.000 (13.781)	
2022-03-30 08:27:41,028: ============================================================
2022-03-30 08:28:20,647: time cost, forward:0.1668561090473966, backward:0.038536485864113595, data cost:0.24265748034184784 
2022-03-30 08:28:20,648: ============================================================
2022-03-30 08:28:20,648: Epoch 2/31 Batch 2900/7662 eta: 1 day, 0:58:42.252048	Training Loss nan (nan)	Training Prec@1 0.000 (8.477)	Training Prec@5 0.000 (13.306)	
2022-03-30 08:28:20,648: ============================================================
2022-03-30 08:28:59,767: time cost, forward:0.1662998720978689, backward:0.03844157001422858, data cost:0.24140271746186107 
2022-03-30 08:28:59,768: ============================================================
2022-03-30 08:28:59,768: Epoch 2/31 Batch 3000/7662 eta: 1 day, 0:39:09.170144	Training Loss nan (nan)	Training Prec@1 0.000 (8.194)	Training Prec@5 0.000 (12.862)	
2022-03-30 08:28:59,768: ============================================================
2022-03-30 08:29:40,223: time cost, forward:0.16605212881550477, backward:0.038451289545916555, data cost:0.24028171874431303 
2022-03-30 08:29:40,224: ============================================================
2022-03-30 08:29:40,224: Epoch 2/31 Batch 3100/7662 eta: 1 day, 1:28:58.298988	Training Loss nan (nan)	Training Prec@1 0.000 (7.930)	Training Prec@5 0.000 (12.448)	
2022-03-30 08:29:40,224: ============================================================
2022-03-30 08:30:19,962: time cost, forward:0.1657240951981683, backward:0.03842252021508129, data cost:0.23913819270418674 
2022-03-30 08:30:19,962: ============================================================
2022-03-30 08:30:19,962: Epoch 2/31 Batch 3200/7662 eta: 1 day, 1:01:10.998145	Training Loss nan (nan)	Training Prec@1 0.000 (7.682)	Training Prec@5 0.000 (12.058)	
2022-03-30 08:30:19,962: ============================================================
2022-03-30 08:31:00,653: time cost, forward:0.1656193785249699, backward:0.03838256540787007, data cost:0.2381550410762994 
2022-03-30 08:31:00,654: ============================================================
2022-03-30 08:31:00,654: Epoch 2/31 Batch 3300/7662 eta: 1 day, 1:36:30.619782	Training Loss nan (nan)	Training Prec@1 0.000 (7.449)	Training Prec@5 0.000 (11.693)	
2022-03-30 08:31:00,654: ============================================================
2022-03-30 08:31:42,177: time cost, forward:0.16574518236562621, backward:0.03831780970394699, data cost:0.23728729669189902 
2022-03-30 08:31:42,178: ============================================================
2022-03-30 08:31:42,178: Epoch 2/31 Batch 3400/7662 eta: 1 day, 2:07:15.952720	Training Loss nan (nan)	Training Prec@1 0.000 (7.230)	Training Prec@5 0.000 (11.349)	
2022-03-30 08:31:42,178: ============================================================
2022-03-30 08:32:21,690: time cost, forward:0.1654882084883837, backward:0.03823139627308939, data cost:0.23628978287706923 
2022-03-30 08:32:21,691: ============================================================
2022-03-30 08:32:21,691: Epoch 2/31 Batch 3500/7662 eta: 1 day, 0:50:42.870883	Training Loss nan (nan)	Training Prec@1 0.000 (7.023)	Training Prec@5 0.000 (11.025)	
2022-03-30 08:32:21,692: ============================================================
2022-03-30 08:33:00,380: time cost, forward:0.16502147636402975, backward:0.03817888636428735, data cost:0.2353116497459528 
2022-03-30 08:33:00,380: ============================================================
2022-03-30 08:33:00,381: Epoch 2/31 Batch 3600/7662 eta: 1 day, 0:18:59.271384	Training Loss nan (nan)	Training Prec@1 0.000 (6.828)	Training Prec@5 0.000 (10.719)	
2022-03-30 08:33:00,381: ============================================================
2022-03-30 08:33:38,764: time cost, forward:0.16447565619382834, backward:0.03810746413883437, data cost:0.23442305342897785 
2022-03-30 08:33:38,776: ============================================================
2022-03-30 08:33:38,776: Epoch 2/31 Batch 3700/7662 eta: 1 day, 0:07:14.685758	Training Loss nan (nan)	Training Prec@1 0.000 (6.644)	Training Prec@5 0.000 (10.429)	
2022-03-30 08:33:38,776: ============================================================
2022-03-30 08:34:17,693: time cost, forward:0.16396656937585374, backward:0.03804334548624857, data cost:0.2337334171098356 
2022-03-30 08:34:17,693: ============================================================
2022-03-30 08:34:17,693: Epoch 2/31 Batch 3800/7662 eta: 1 day, 0:26:17.659991	Training Loss nan (nan)	Training Prec@1 0.000 (6.469)	Training Prec@5 0.000 (10.155)	
2022-03-30 08:34:17,694: ============================================================
2022-03-30 08:35:00,994: time cost, forward:0.16422315761657152, backward:0.038039309332143896, data cost:0.23340686592391652 
2022-03-30 08:35:00,994: ============================================================
2022-03-30 08:35:00,994: Epoch 2/31 Batch 3900/7662 eta: 1 day, 3:10:43.177317	Training Loss nan (nan)	Training Prec@1 0.000 (6.303)	Training Prec@5 0.000 (9.894)	
2022-03-30 08:35:00,995: ============================================================
2022-03-30 08:35:38,757: time cost, forward:0.1637114430403942, backward:0.03796514769618766, data cost:0.23253017736989637 
2022-03-30 08:35:38,757: ============================================================
2022-03-30 08:35:38,758: Epoch 2/31 Batch 4000/7662 eta: 23:41:32.236246	Training Loss nan (nan)	Training Prec@1 0.000 (6.145)	Training Prec@5 0.000 (9.647)	
2022-03-30 08:35:38,758: ============================================================
2022-03-30 08:36:17,517: time cost, forward:0.16331855236945952, backward:0.037917035415306936, data cost:0.23182586374210712 
2022-03-30 08:36:17,518: ============================================================
2022-03-30 08:36:17,518: Epoch 2/31 Batch 4100/7662 eta: 1 day, 0:18:26.378654	Training Loss nan (nan)	Training Prec@1 0.000 (5.995)	Training Prec@5 0.000 (9.411)	
2022-03-30 08:36:17,518: ============================================================
2022-03-30 08:36:56,665: time cost, forward:0.16295905980816283, backward:0.0378838174256236, data cost:0.2312142466840133 
2022-03-30 08:36:56,665: ============================================================
2022-03-30 08:36:56,665: Epoch 2/31 Batch 4200/7662 eta: 1 day, 0:32:20.093973	Training Loss nan (nan)	Training Prec@1 0.000 (5.852)	Training Prec@5 0.000 (9.187)	
2022-03-30 08:36:56,666: ============================================================
2022-03-30 08:37:37,062: time cost, forward:0.16286715736886073, backward:0.03786965363744969, data cost:0.23065060209357258 
2022-03-30 08:37:37,062: ============================================================
2022-03-30 08:37:37,062: Epoch 2/31 Batch 4300/7662 eta: 1 day, 1:18:39.696106	Training Loss nan (nan)	Training Prec@1 0.000 (5.716)	Training Prec@5 0.000 (8.974)	
2022-03-30 08:37:37,062: ============================================================
2022-03-30 08:38:17,528: time cost, forward:0.16274660640099992, backward:0.03789965703720991, data cost:0.2301267585745723 
2022-03-30 08:38:17,528: ============================================================
2022-03-30 08:38:17,529: Epoch 2/31 Batch 4400/7662 eta: 1 day, 1:20:35.771433	Training Loss nan (nan)	Training Prec@1 0.000 (5.586)	Training Prec@5 0.000 (8.770)	
2022-03-30 08:38:17,529: ============================================================
2022-03-30 08:38:57,026: time cost, forward:0.16266155947735902, backward:0.037925335480706326, data cost:0.22937743561510035 
2022-03-30 08:38:57,026: ============================================================
2022-03-30 08:38:57,026: Epoch 2/31 Batch 4500/7662 eta: 1 day, 0:43:32.313316	Training Loss nan (nan)	Training Prec@1 0.000 (5.462)	Training Prec@5 0.000 (8.575)	
2022-03-30 08:38:57,027: ============================================================
2022-03-30 08:39:39,560: time cost, forward:0.16273599796954588, backward:0.03794153256633018, data cost:0.22917349858500694 
2022-03-30 08:39:39,560: ============================================================
2022-03-30 08:39:39,561: Epoch 2/31 Batch 4600/7662 eta: 1 day, 2:36:53.436236	Training Loss nan (nan)	Training Prec@1 0.000 (5.343)	Training Prec@5 0.000 (8.389)	
2022-03-30 08:39:39,561: ============================================================
2022-03-30 08:40:21,188: time cost, forward:0.16270280138029142, backward:0.03792846910647571, data cost:0.2289129412663848 
2022-03-30 08:40:21,188: ============================================================
2022-03-30 08:40:21,188: Epoch 2/31 Batch 4700/7662 eta: 1 day, 2:02:08.816490	Training Loss nan (nan)	Training Prec@1 0.000 (5.230)	Training Prec@5 0.000 (8.210)	
2022-03-30 08:40:21,188: ============================================================
2022-03-30 08:40:58,504: time cost, forward:0.1622598250723551, backward:0.03790840066256784, data cost:0.22818998819491496 
2022-03-30 08:40:58,505: ============================================================
2022-03-30 08:40:58,505: Epoch 2/31 Batch 4800/7662 eta: 23:19:45.117072	Training Loss nan (nan)	Training Prec@1 0.000 (5.121)	Training Prec@5 0.000 (8.039)	
2022-03-30 08:40:58,505: ============================================================
2022-03-30 08:41:38,271: time cost, forward:0.16212936152387916, backward:0.03785446006975799, data cost:0.22774093047432278 
2022-03-30 08:41:38,271: ============================================================
2022-03-30 08:41:38,272: Epoch 2/31 Batch 4900/7662 eta: 1 day, 0:50:59.918971	Training Loss nan (nan)	Training Prec@1 0.000 (5.016)	Training Prec@5 0.000 (7.875)	
2022-03-30 08:41:38,272: ============================================================
2022-03-30 08:42:18,829: time cost, forward:0.16216484821660682, backward:0.03781090886336752, data cost:0.2272876855682721 
2022-03-30 08:42:18,829: ============================================================
2022-03-30 08:42:18,830: Epoch 2/31 Batch 5000/7662 eta: 1 day, 1:19:59.381374	Training Loss nan (nan)	Training Prec@1 0.000 (4.916)	Training Prec@5 0.000 (7.718)	
2022-03-30 08:42:18,830: ============================================================
2022-03-30 08:42:57,956: time cost, forward:0.16193849625693135, backward:0.03777351466364243, data cost:0.22683644374227216 
2022-03-30 08:42:57,956: ============================================================
2022-03-30 08:42:57,957: Epoch 2/31 Batch 5100/7662 eta: 1 day, 0:25:41.706703	Training Loss nan (nan)	Training Prec@1 0.000 (4.819)	Training Prec@5 0.000 (7.566)	
2022-03-30 08:42:57,957: ============================================================
2022-03-30 08:43:40,872: time cost, forward:0.16216222316766157, backward:0.03774123596305869, data cost:0.22665659664548068 
2022-03-30 08:43:40,884: ============================================================
2022-03-30 08:43:40,884: Epoch 2/31 Batch 5200/7662 eta: 1 day, 2:47:22.251732	Training Loss nan (nan)	Training Prec@1 0.000 (4.727)	Training Prec@5 0.000 (7.421)	
2022-03-30 08:43:40,885: ============================================================
2022-03-30 08:44:19,431: time cost, forward:0.16190961370559925, backward:0.03772526409158528, data cost:0.22617203997179885 
2022-03-30 08:44:19,432: ============================================================
2022-03-30 08:44:19,432: Epoch 2/31 Batch 5300/7662 eta: 1 day, 0:02:42.769562	Training Loss nan (nan)	Training Prec@1 0.000 (4.638)	Training Prec@5 0.000 (7.281)	
2022-03-30 08:44:19,432: ============================================================
2022-03-30 08:45:00,255: time cost, forward:0.16193646981906484, backward:0.03768015318522742, data cost:0.2258326109525649 
2022-03-30 08:45:00,255: ============================================================
2022-03-30 08:45:00,255: Epoch 2/31 Batch 5400/7662 eta: 1 day, 1:27:12.480272	Training Loss nan (nan)	Training Prec@1 0.000 (4.552)	Training Prec@5 0.000 (7.146)	
2022-03-30 08:45:00,255: ============================================================
2022-03-30 08:45:39,829: time cost, forward:0.16186169113413076, backward:0.03764683382145642, data cost:0.22541096046157785 
2022-03-30 08:45:39,830: ============================================================
2022-03-30 08:45:39,830: Epoch 2/31 Batch 5500/7662 eta: 1 day, 0:39:50.013732	Training Loss nan (nan)	Training Prec@1 0.000 (4.469)	Training Prec@5 0.195 (7.016)	
2022-03-30 08:45:39,830: ============================================================
2022-03-30 08:46:20,389: time cost, forward:0.16183025442206192, backward:0.03760847531805977, data cost:0.22512404172202055 
2022-03-30 08:46:20,390: ============================================================
2022-03-30 08:46:20,390: Epoch 2/31 Batch 5600/7662 eta: 1 day, 1:16:00.351298	Training Loss nan (nan)	Training Prec@1 0.000 (4.389)	Training Prec@5 0.000 (6.891)	
2022-03-30 08:46:20,390: ============================================================
2022-03-30 08:47:01,480: time cost, forward:0.16180176290634912, backward:0.03759345549787423, data cost:0.22492202697542388 
2022-03-30 08:47:01,480: ============================================================
2022-03-30 08:47:01,480: Epoch 2/31 Batch 5700/7662 eta: 1 day, 1:35:08.216999	Training Loss nan (nan)	Training Prec@1 0.000 (4.312)	Training Prec@5 0.000 (6.770)	
2022-03-30 08:47:01,480: ============================================================
2022-03-30 08:47:41,733: time cost, forward:0.16172727927563005, backward:0.0375664592673191, data cost:0.2246334084068255 
2022-03-30 08:47:41,734: ============================================================
2022-03-30 08:47:41,734: Epoch 2/31 Batch 5800/7662 eta: 1 day, 1:03:12.979019	Training Loss nan (nan)	Training Prec@1 0.000 (4.238)	Training Prec@5 0.000 (6.654)	
2022-03-30 08:47:41,734: ============================================================
2022-03-30 08:48:25,281: time cost, forward:0.16203711113296823, backward:0.037628055568629755, data cost:0.22444320699080994 
2022-03-30 08:48:25,281: ============================================================
2022-03-30 08:48:25,282: Epoch 2/31 Batch 5900/7662 eta: 1 day, 3:05:30.030833	Training Loss nan (nan)	Training Prec@1 0.000 (4.166)	Training Prec@5 0.000 (6.541)	
2022-03-30 08:48:25,282: ============================================================
2022-03-30 08:49:04,148: time cost, forward:0.16185558702691752, backward:0.037657803069513864, data cost:0.2239776145698508 
2022-03-30 08:49:04,149: ============================================================
2022-03-30 08:49:04,149: Epoch 2/31 Batch 6000/7662 eta: 1 day, 0:10:09.308290	Training Loss nan (nan)	Training Prec@1 0.000 (4.096)	Training Prec@5 0.000 (6.432)	
2022-03-30 08:49:04,150: ============================================================
2022-03-30 08:49:47,197: time cost, forward:0.16207934528281404, backward:0.037745221635712464, data cost:0.2237802113562573 
2022-03-30 08:49:47,198: ============================================================
2022-03-30 08:49:47,198: Epoch 2/31 Batch 6100/7662 eta: 1 day, 2:45:26.470162	Training Loss nan (nan)	Training Prec@1 0.000 (4.029)	Training Prec@5 0.000 (6.326)	
2022-03-30 08:49:47,198: ============================================================
2022-03-30 08:50:25,654: time cost, forward:0.16183495806155426, backward:0.03770100026192983, data cost:0.22343263596868415 
2022-03-30 08:50:25,654: ============================================================
2022-03-30 08:50:25,654: Epoch 2/31 Batch 6200/7662 eta: 23:53:31.423106	Training Loss nan (nan)	Training Prec@1 0.000 (3.964)	Training Prec@5 0.000 (6.224)	
2022-03-30 08:50:25,654: ============================================================
2022-03-30 08:51:05,730: time cost, forward:0.1617853336058679, backward:0.03766913924222523, data cost:0.22315511394633358 
2022-03-30 08:51:05,731: ============================================================
2022-03-30 08:51:05,731: Epoch 2/31 Batch 6300/7662 eta: 1 day, 0:53:15.687869	Training Loss nan (nan)	Training Prec@1 0.000 (3.901)	Training Prec@5 0.000 (6.126)	
2022-03-30 08:51:05,731: ============================================================
2022-03-30 08:51:45,184: time cost, forward:0.16164863290293438, backward:0.03766488827733551, data cost:0.2228377320315991 
2022-03-30 08:51:45,185: ============================================================
2022-03-30 08:51:45,185: Epoch 2/31 Batch 6400/7662 eta: 1 day, 0:29:23.713152	Training Loss nan (nan)	Training Prec@1 0.000 (3.840)	Training Prec@5 0.000 (6.030)	
2022-03-30 08:51:45,185: ============================================================
2022-03-30 08:52:26,428: time cost, forward:0.16163794806158016, backward:0.03765508310118718, data cost:0.22271299332834277 
2022-03-30 08:52:26,428: ============================================================
2022-03-30 08:52:26,428: Epoch 2/31 Batch 6500/7662 eta: 1 day, 1:35:22.377563	Training Loss nan (nan)	Training Prec@1 0.000 (3.781)	Training Prec@5 0.000 (5.937)	
2022-03-30 08:52:26,429: ============================================================
2022-03-30 08:53:06,841: time cost, forward:0.16156136185712103, backward:0.037641242482947986, data cost:0.2225181144157094 
2022-03-30 08:53:06,841: ============================================================
2022-03-30 08:53:06,842: Epoch 2/31 Batch 6600/7662 eta: 1 day, 1:03:46.899860	Training Loss nan (nan)	Training Prec@1 0.000 (3.724)	Training Prec@5 0.000 (5.847)	
2022-03-30 08:53:06,842: ============================================================
2022-03-30 08:53:45,062: time cost, forward:0.16129060936856757, backward:0.03763179939920323, data cost:0.22220366409348452 
2022-03-30 08:53:45,063: ============================================================
2022-03-30 08:53:45,063: Epoch 2/31 Batch 6700/7662 eta: 23:41:35.086118	Training Loss nan (nan)	Training Prec@1 0.000 (3.668)	Training Prec@5 0.000 (5.760)	
2022-03-30 08:53:45,063: ============================================================
2022-03-30 08:54:27,658: time cost, forward:0.1614318400695931, backward:0.037640114148972156, data cost:0.22211596102797998 
2022-03-30 08:54:27,658: ============================================================
2022-03-30 08:54:27,658: Epoch 2/31 Batch 6800/7662 eta: 1 day, 2:23:33.684456	Training Loss nan (nan)	Training Prec@1 0.000 (3.614)	Training Prec@5 0.000 (5.676)	
2022-03-30 08:54:27,658: ============================================================
2022-03-30 08:55:08,124: time cost, forward:0.16137230992403595, backward:0.037638704229704036, data cost:0.22192345803881888 
2022-03-30 08:55:08,124: ============================================================
2022-03-30 08:55:08,124: Epoch 2/31 Batch 6900/7662 eta: 1 day, 1:03:43.783303	Training Loss nan (nan)	Training Prec@1 0.000 (3.562)	Training Prec@5 0.000 (5.593)	
2022-03-30 08:55:08,125: ============================================================
2022-03-30 08:55:49,426: time cost, forward:0.16139913184384921, backward:0.037656905327818874, data cost:0.22175802641518544 
2022-03-30 08:55:49,427: ============================================================
2022-03-30 08:55:49,427: Epoch 2/31 Batch 7000/7662 eta: 1 day, 1:34:07.216086	Training Loss nan (nan)	Training Prec@1 0.000 (3.511)	Training Prec@5 0.000 (5.514)	
2022-03-30 08:55:49,427: ============================================================
2022-03-30 08:56:27,022: time cost, forward:0.1611357618845891, backward:0.03766550868644666, data cost:0.22134757223289137 
2022-03-30 08:56:27,022: ============================================================
2022-03-30 08:56:27,023: Epoch 2/31 Batch 7100/7662 eta: 23:15:48.841474	Training Loss nan (nan)	Training Prec@1 0.000 (3.462)	Training Prec@5 0.000 (5.436)	
2022-03-30 08:56:27,023: ============================================================
2022-03-30 08:57:08,431: time cost, forward:0.1612663569425871, backward:0.037629065670591544, data cost:0.2211772824578988 
2022-03-30 08:57:08,431: ============================================================
2022-03-30 08:57:08,432: Epoch 2/31 Batch 7200/7662 eta: 1 day, 1:36:41.419916	Training Loss nan (nan)	Training Prec@1 0.000 (3.414)	Training Prec@5 0.000 (5.360)	
2022-03-30 08:57:08,432: ============================================================
2022-03-30 08:57:51,156: time cost, forward:0.16139951972606034, backward:0.03759630429677758, data cost:0.2211663530597721 
2022-03-30 08:57:51,156: ============================================================
2022-03-30 08:57:51,156: Epoch 2/31 Batch 7300/7662 eta: 1 day, 2:24:48.485549	Training Loss nan (nan)	Training Prec@1 0.000 (3.367)	Training Prec@5 0.000 (5.287)	
2022-03-30 08:57:51,157: ============================================================
2022-03-30 08:58:29,106: time cost, forward:0.1611921698648489, backward:0.03758020912703509, data cost:0.22083413103075022 
2022-03-30 08:58:29,106: ============================================================
2022-03-30 08:58:29,106: Epoch 2/31 Batch 7400/7662 eta: 23:27:03.635616	Training Loss nan (nan)	Training Prec@1 0.000 (3.321)	Training Prec@5 0.000 (5.216)	
2022-03-30 08:58:29,106: ============================================================
2022-03-30 08:59:10,482: time cost, forward:0.16116724877154642, backward:0.03756282192529845, data cost:0.22079302349286423 
2022-03-30 08:59:10,482: ============================================================
2022-03-30 08:59:10,483: Epoch 2/31 Batch 7500/7662 eta: 1 day, 1:33:24.702703	Training Loss nan (nan)	Training Prec@1 0.000 (3.277)	Training Prec@5 0.000 (5.146)	
2022-03-30 08:59:10,483: ============================================================
2022-03-30 08:59:50,228: time cost, forward:0.16109189549187702, backward:0.03751588972136604, data cost:0.22061583973542845 
2022-03-30 08:59:50,228: ============================================================
2022-03-30 08:59:50,229: Epoch 2/31 Batch 7600/7662 eta: 1 day, 0:32:20.223343	Training Loss nan (nan)	Training Prec@1 0.000 (3.234)	Training Prec@5 0.000 (5.078)	
2022-03-30 08:59:50,229: ============================================================
2022-03-30 09:00:15,485: Epoch: 2/31 eta: 1 day, 0:31:55.183259	Training Loss nan (nan)	Training Prec@1 0.000 (3.207)	Training Prec@5 0.000 (5.037)
2022-03-30 09:00:15,485: ============================================================
2022-03-30 09:00:52,189: time cost, forward:0.12976103358798557, backward:0.035102839421744296, data cost:0.20272160298896558 
2022-03-30 09:00:52,189: ============================================================
2022-03-30 09:00:52,189: Epoch 3/31 Batch 100/7662 eta: 22:36:06.686136	Training Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.000)	
2022-03-30 09:00:52,190: ============================================================
2022-03-30 09:01:32,685: time cost, forward:0.14135638313676843, backward:0.037414391436169495, data cost:0.20736299567486174 
2022-03-30 09:01:32,685: ============================================================
2022-03-30 09:01:32,685: Epoch 3/31 Batch 200/7662 eta: 1 day, 0:58:20.390701	Training Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.000)	
2022-03-30 09:01:32,686: ============================================================
2022-03-30 09:02:12,805: time cost, forward:0.14713887705850762, backward:0.036730904244260246, data cost:0.2071599705163452 
2022-03-30 09:02:12,805: ============================================================
2022-03-30 09:02:12,805: Epoch 3/31 Batch 300/7662 eta: 1 day, 0:43:45.793898	Training Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.001)	
2022-03-30 09:02:12,805: ============================================================
2022-03-30 09:02:52,713: time cost, forward:0.15082744189671107, backward:0.036713519490751106, data cost:0.20541058865406162 
2022-03-30 09:02:52,713: ============================================================
2022-03-30 09:02:52,714: Epoch 3/31 Batch 400/7662 eta: 1 day, 0:35:16.167841	Training Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.001)	
2022-03-30 09:02:52,714: ============================================================
2022-03-30 09:03:32,591: time cost, forward:0.15190938909450372, backward:0.03708570562527032, data cost:0.20497152179419875 
2022-03-30 09:03:32,592: ============================================================
2022-03-30 09:03:32,592: Epoch 3/31 Batch 500/7662 eta: 1 day, 0:33:30.248367	Training Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2022-03-30 09:03:32,592: ============================================================
2022-03-30 09:04:12,474: time cost, forward:0.15330744068292226, backward:0.036555310919607224, data cost:0.20490027389462684 
2022-03-30 09:04:12,474: ============================================================
2022-03-30 09:04:12,475: Epoch 3/31 Batch 600/7662 eta: 1 day, 0:32:59.066225	Training Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2022-03-30 09:04:12,475: ============================================================
2022-03-30 09:04:55,957: time cost, forward:0.15692462498196888, backward:0.03728740307394527, data cost:0.2062352048139886 
2022-03-30 09:04:55,957: ============================================================
2022-03-30 09:04:55,958: Epoch 3/31 Batch 700/7662 eta: 1 day, 2:45:14.421877	Training Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.004)	
2022-03-30 09:04:55,958: ============================================================
2022-03-30 09:05:37,101: time cost, forward:0.15787606871918833, backward:0.03772708441647182, data cost:0.20616241390624543 
2022-03-30 09:05:37,102: ============================================================
2022-03-30 09:05:37,102: Epoch 3/31 Batch 800/7662 eta: 1 day, 1:18:12.928312	Training Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2022-03-30 09:05:37,102: ============================================================
2022-03-30 09:06:16,717: time cost, forward:0.157408007790435, backward:0.03767174687878839, data cost:0.2060073291897376 
2022-03-30 09:06:16,718: ============================================================
2022-03-30 09:06:16,718: Epoch 3/31 Batch 900/7662 eta: 1 day, 0:21:10.865881	Training Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.004)	
2022-03-30 09:06:16,718: ============================================================
2022-03-30 09:06:56,596: time cost, forward:0.1572577795824847, backward:0.037574994552123535, data cost:0.20597157607207428 
2022-03-30 09:06:56,597: ============================================================
2022-03-30 09:06:56,597: Epoch 3/31 Batch 1000/7662 eta: 1 day, 0:30:11.161718	Training Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.004)	
2022-03-30 09:06:56,597: ============================================================
2022-03-30 09:07:38,636: time cost, forward:0.15840154653033742, backward:0.037682775153801376, data cost:0.20640284392484434 
2022-03-30 09:07:38,649: ============================================================
2022-03-30 09:07:38,650: Epoch 3/31 Batch 1100/7662 eta: 1 day, 1:49:38.160416	Training Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.004)	
2022-03-30 09:07:38,650: ============================================================
2022-03-30 09:08:16,458: time cost, forward:0.15737626391515025, backward:0.03776669382949587, data cost:0.20534590445924938 
2022-03-30 09:08:16,458: ============================================================
2022-03-30 09:08:16,459: Epoch 3/31 Batch 1200/7662 eta: 23:12:37.664745	Training Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2022-03-30 09:08:16,459: ============================================================
2022-03-30 09:08:59,501: time cost, forward:0.15891308946000143, backward:0.03782121780929976, data cost:0.20588212567535705 
2022-03-30 09:08:59,512: ============================================================
2022-03-30 09:08:59,513: Epoch 3/31 Batch 1300/7662 eta: 1 day, 2:25:05.391776	Training Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2022-03-30 09:08:59,513: ============================================================
2022-03-30 09:09:40,811: time cost, forward:0.15963797061421175, backward:0.03771388607420523, data cost:0.2061050258592847 
2022-03-30 09:09:40,811: ============================================================
2022-03-30 09:09:40,811: Epoch 3/31 Batch 1400/7662 eta: 1 day, 1:19:47.089447	Training Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.004)	
2022-03-30 09:09:40,811: ============================================================
2022-03-30 09:10:21,038: time cost, forward:0.159611049216934, backward:0.037720629658358665, data cost:0.20599549718504354 
2022-03-30 09:10:21,039: ============================================================
2022-03-30 09:10:21,039: Epoch 3/31 Batch 1500/7662 eta: 1 day, 0:39:41.725395	Training Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.004)	
2022-03-30 09:10:21,039: ============================================================
2022-03-30 09:11:03,144: time cost, forward:0.16022044304686087, backward:0.037641490974450124, data cost:0.20653915942050727 
2022-03-30 09:11:03,144: ============================================================
2022-03-30 09:11:03,145: Epoch 3/31 Batch 1600/7662 eta: 1 day, 1:48:05.102934	Training Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.004)	
2022-03-30 09:11:03,145: ============================================================
2022-03-30 09:11:42,086: time cost, forward:0.15934887095154418, backward:0.037547883459790585, data cost:0.2065854858411909 
2022-03-30 09:11:42,087: ============================================================
2022-03-30 09:11:42,087: Epoch 3/31 Batch 1700/7662 eta: 23:51:07.507901	Training Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.004)	
2022-03-30 09:11:42,087: ============================================================
2022-03-30 09:12:23,856: time cost, forward:0.15975623729826677, backward:0.03753105516629858, data cost:0.2069444511916122 
2022-03-30 09:12:23,856: ============================================================
2022-03-30 09:12:23,857: Epoch 3/31 Batch 1800/7662 eta: 1 day, 1:34:19.814146	Training Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2022-03-30 09:12:23,857: ============================================================
2022-03-30 09:13:02,366: time cost, forward:0.15922875426956828, backward:0.03744065579519327, data cost:0.20652961467302994 
2022-03-30 09:13:02,367: ============================================================
2022-03-30 09:13:02,367: Epoch 3/31 Batch 1900/7662 eta: 23:33:58.414369	Training Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.004)	
2022-03-30 09:13:02,367: ============================================================
2022-03-30 09:13:42,561: time cost, forward:0.1593137801200405, backward:0.03751085411613735, data cost:0.20627734015857416 
2022-03-30 09:13:42,561: ============================================================
2022-03-30 09:13:42,561: Epoch 3/31 Batch 2000/7662 eta: 1 day, 0:35:06.823872	Training Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.004)	
2022-03-30 09:13:42,561: ============================================================
2022-03-30 09:14:25,284: time cost, forward:0.15986518918928388, backward:0.03757024186176593, data cost:0.20674095714927118 
2022-03-30 09:14:25,284: ============================================================
2022-03-30 09:14:25,285: Epoch 3/31 Batch 2100/7662 eta: 1 day, 2:07:14.194128	Training Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2022-03-30 09:14:25,285: ============================================================
2022-03-30 09:15:08,870: time cost, forward:0.16086214582938507, backward:0.03755297548069418, data cost:0.20721059563269448 
2022-03-30 09:15:08,870: ============================================================
2022-03-30 09:15:08,870: Epoch 3/31 Batch 2200/7662 eta: 1 day, 2:38:07.772725	Training Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2022-03-30 09:15:08,870: ============================================================
2022-03-30 09:15:46,124: time cost, forward:0.1600120404638794, backward:0.037423451292146644, data cost:0.20673205905398268 
2022-03-30 09:15:46,125: ============================================================
2022-03-30 09:15:46,125: Epoch 3/31 Batch 2300/7662 eta: 22:45:22.238160	Training Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2022-03-30 09:15:46,125: ============================================================
2022-03-30 09:16:26,338: time cost, forward:0.1600874496927456, backward:0.03752299505951704, data cost:0.2064517580901349 
2022-03-30 09:16:26,338: ============================================================
2022-03-30 09:16:26,338: Epoch 3/31 Batch 2400/7662 eta: 1 day, 0:33:08.662347	Training Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2022-03-30 09:16:26,338: ============================================================
2022-03-30 09:17:11,597: time cost, forward:0.16127370309238198, backward:0.03771815246560661, data cost:0.2069759670378161 
2022-03-30 09:17:11,597: ============================================================
2022-03-30 09:17:11,598: Epoch 3/31 Batch 2500/7662 eta: 1 day, 3:37:14.483014	Training Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2022-03-30 09:17:11,598: ============================================================
2022-03-30 09:17:50,879: time cost, forward:0.16094643457800942, backward:0.037612192352811574, data cost:0.20687973173639781 
2022-03-30 09:17:50,880: ============================================================
2022-03-30 09:17:50,880: Epoch 3/31 Batch 2600/7662 eta: 23:57:43.520689	Training Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2022-03-30 09:17:50,880: ============================================================
2022-03-30 09:18:30,957: time cost, forward:0.16085642282500803, backward:0.03753595398107517, data cost:0.20680831140304062 
2022-03-30 09:18:30,969: ============================================================
2022-03-30 09:18:30,969: Epoch 3/31 Batch 2700/7662 eta: 1 day, 0:26:34.676718	Training Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2022-03-30 09:18:30,969: ============================================================
2022-03-30 09:19:15,655: time cost, forward:0.1614490693362537, backward:0.03745757949995373, data cost:0.20781812688289858 
2022-03-30 09:19:15,655: ============================================================
2022-03-30 09:19:15,655: Epoch 3/31 Batch 2800/7662 eta: 1 day, 3:14:01.729586	Training Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2022-03-30 09:19:15,655: ============================================================
2022-03-30 09:19:56,871: time cost, forward:0.16152114479983418, backward:0.03744338158288056, data cost:0.2078458359834448 
2022-03-30 09:19:56,871: ============================================================
2022-03-30 09:19:56,872: Epoch 3/31 Batch 2900/7662 eta: 1 day, 1:06:26.993365	Training Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2022-03-30 09:19:56,872: ============================================================
2022-03-30 09:20:36,569: time cost, forward:0.16130616109185952, backward:0.037347548482576265, data cost:0.20789302424932965 
2022-03-30 09:20:36,569: ============================================================
2022-03-30 09:20:36,570: Epoch 3/31 Batch 3000/7662 eta: 1 day, 0:10:17.808628	Training Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2022-03-30 09:20:36,570: ============================================================
2022-03-30 09:21:18,529: time cost, forward:0.16163923110297204, backward:0.03733312233527578, data cost:0.20797692125325973 
2022-03-30 09:21:18,530: ============================================================
2022-03-30 09:21:18,530: Epoch 3/31 Batch 3100/7662 eta: 1 day, 1:32:14.490542	Training Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.004)	
2022-03-30 09:21:18,530: ============================================================
2022-03-30 09:21:58,174: time cost, forward:0.1613761248831527, backward:0.03726536678351176, data cost:0.20796797267345013 
2022-03-30 09:21:58,175: ============================================================
2022-03-30 09:21:58,175: Epoch 3/31 Batch 3200/7662 eta: 1 day, 0:07:02.253758	Training Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2022-03-30 09:21:58,175: ============================================================
2022-03-30 09:22:42,322: time cost, forward:0.16204690376748893, backward:0.03730679649625919, data cost:0.20829053855946875 
2022-03-30 09:22:42,323: ============================================================
2022-03-30 09:22:42,323: Epoch 3/31 Batch 3300/7662 eta: 1 day, 2:50:39.038898	Training Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2022-03-30 09:22:42,323: ============================================================
2022-03-30 09:23:22,473: time cost, forward:0.16190986403229307, backward:0.0372821501192888, data cost:0.20825782296376286 
2022-03-30 09:23:22,474: ============================================================
2022-03-30 09:23:22,474: Epoch 3/31 Batch 3400/7662 eta: 1 day, 0:24:10.570796	Training Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2022-03-30 09:23:22,474: ============================================================
2022-03-30 09:24:03,821: time cost, forward:0.16190263304447372, backward:0.03724347212955794, data cost:0.20845556715686991 
2022-03-30 09:24:03,821: ============================================================
2022-03-30 09:24:03,821: Epoch 3/31 Batch 3500/7662 eta: 1 day, 1:07:05.613452	Training Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2022-03-30 09:24:03,821: ============================================================
2022-03-30 09:24:45,471: time cost, forward:0.1620176276355096, backward:0.037192974729185535, data cost:0.2086292584427995 
2022-03-30 09:24:45,471: ============================================================
2022-03-30 09:24:45,471: Epoch 3/31 Batch 3600/7662 eta: 1 day, 1:17:27.196162	Training Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.004)	
2022-03-30 09:24:45,472: ============================================================
2022-03-30 09:25:26,308: time cost, forward:0.16208785751375773, backward:0.03715260153237018, data cost:0.20858877122192712 
2022-03-30 09:25:26,309: ============================================================
2022-03-30 09:25:26,309: Epoch 3/31 Batch 3700/7662 eta: 1 day, 0:47:09.540794	Training Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.004)	
2022-03-30 09:25:26,309: ============================================================
2022-03-30 09:26:07,878: time cost, forward:0.16216907334283767, backward:0.03711198988510578, data cost:0.2087423054598984 
2022-03-30 09:26:07,878: ============================================================
2022-03-30 09:26:07,879: Epoch 3/31 Batch 3800/7662 eta: 1 day, 1:13:08.015828	Training Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.004)	
2022-03-30 09:26:07,879: ============================================================
2022-03-30 09:26:51,095: time cost, forward:0.16262679064570404, backward:0.03714899791757643, data cost:0.20881074274096864 
2022-03-30 09:26:51,096: ============================================================
2022-03-30 09:26:51,096: Epoch 3/31 Batch 3900/7662 eta: 1 day, 2:12:22.495881	Training Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.004)	
2022-03-30 09:26:51,096: ============================================================
2022-03-30 09:27:30,891: time cost, forward:0.1624278244181674, backward:0.03714953013794754, data cost:0.20876075709334133 
2022-03-30 09:27:30,892: ============================================================
2022-03-30 09:27:30,892: Epoch 3/31 Batch 4000/7662 eta: 1 day, 0:07:14.491736	Training Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.195 (0.004)	
2022-03-30 09:27:30,892: ============================================================
2022-03-30 09:28:13,717: time cost, forward:0.16273230330250152, backward:0.037119889730475364, data cost:0.2089596443799799 
2022-03-30 09:28:13,727: ============================================================
2022-03-30 09:28:13,727: Epoch 3/31 Batch 4100/7662 eta: 1 day, 1:57:03.961986	Training Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2022-03-30 09:28:13,728: ============================================================
2022-03-30 09:28:54,990: time cost, forward:0.16272768777845928, backward:0.03710335185057097, data cost:0.20906185110173472 
2022-03-30 09:28:54,990: ============================================================
2022-03-30 09:28:54,991: Epoch 3/31 Batch 4200/7662 eta: 1 day, 0:59:13.112426	Training Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.004)	
2022-03-30 09:28:54,991: ============================================================
2022-03-30 09:29:36,801: time cost, forward:0.16275556078065853, backward:0.03711706551043592, data cost:0.2091956169667924 
2022-03-30 09:29:36,802: ============================================================
2022-03-30 09:29:36,802: Epoch 3/31 Batch 4300/7662 eta: 1 day, 1:18:26.681572	Training Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2022-03-30 09:29:36,802: ============================================================
2022-03-30 09:30:17,570: time cost, forward:0.16273779927613818, backward:0.037072167013905, data cost:0.20924451362547428 
2022-03-30 09:30:17,570: ============================================================
2022-03-30 09:30:17,571: Epoch 3/31 Batch 4400/7662 eta: 1 day, 0:39:53.496115	Training Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2022-03-30 09:30:17,571: ============================================================
2022-03-30 09:31:00,167: time cost, forward:0.16297533046301008, backward:0.03706287060559763, data cost:0.20934533850302509 
2022-03-30 09:31:00,177: ============================================================
2022-03-30 09:31:00,177: Epoch 3/31 Batch 4500/7662 eta: 1 day, 1:45:54.758329	Training Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2022-03-30 09:31:00,178: ============================================================
2022-03-30 09:31:42,981: time cost, forward:0.1632655165096448, backward:0.03708064693501525, data cost:0.20945029778178814 
2022-03-30 09:31:42,982: ============================================================
2022-03-30 09:31:42,982: Epoch 3/31 Batch 4600/7662 eta: 1 day, 1:52:21.919826	Training Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2022-03-30 09:31:42,982: ============================================================
2022-03-30 09:32:24,606: time cost, forward:0.16344473356387798, backward:0.03704775462989783, data cost:0.20944123791745176 
2022-03-30 09:32:24,606: ============================================================
2022-03-30 09:32:24,606: Epoch 3/31 Batch 4700/7662 eta: 1 day, 1:08:52.405669	Training Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.003)	
2022-03-30 09:32:24,606: ============================================================
2022-03-30 09:33:04,621: time cost, forward:0.16325983551050827, backward:0.036999708862447765, data cost:0.20946966347333715 
2022-03-30 09:33:04,622: ============================================================
2022-03-30 09:33:04,622: Epoch 3/31 Batch 4800/7662 eta: 1 day, 0:09:53.983249	Training Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.004)	
2022-03-30 09:33:04,622: ============================================================
2022-03-30 09:33:48,731: time cost, forward:0.16365909323348735, backward:0.03701137187262413, data cost:0.20967798633073198 
2022-03-30 09:33:48,732: ============================================================
2022-03-30 09:33:48,732: Epoch 3/31 Batch 4900/7662 eta: 1 day, 2:37:30.280932	Training Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.004)	
2022-03-30 09:33:48,732: ============================================================
2022-03-30 09:34:29,791: time cost, forward:0.163653363464212, backward:0.03707842799181174, data cost:0.20961002035841128 
2022-03-30 09:34:29,791: ============================================================
2022-03-30 09:34:29,791: Epoch 3/31 Batch 5000/7662 eta: 1 day, 0:46:20.485208	Training Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.004)	
2022-03-30 09:34:29,791: ============================================================
2022-03-30 09:35:13,217: time cost, forward:0.16394073903782738, backward:0.03713183338489222, data cost:0.20972022062938667 
2022-03-30 09:35:13,218: ============================================================
2022-03-30 09:35:13,219: Epoch 3/31 Batch 5100/7662 eta: 1 day, 2:11:20.535221	Training Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.004)	
2022-03-30 09:35:13,219: ============================================================
2022-03-30 09:35:56,124: time cost, forward:0.16413512943478406, backward:0.03713408023675008, data cost:0.20985655047201518 
2022-03-30 09:35:56,124: ============================================================
2022-03-30 09:35:56,125: Epoch 3/31 Batch 5200/7662 eta: 1 day, 1:51:45.488282	Training Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.004)	
2022-03-30 09:35:56,125: ============================================================
2022-03-30 09:36:37,860: time cost, forward:0.16417251057974863, backward:0.03706937327567621, data cost:0.2099872306734464 
2022-03-30 09:36:37,860: ============================================================
2022-03-30 09:36:37,861: Epoch 3/31 Batch 5300/7662 eta: 1 day, 1:08:44.938330	Training Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.004)	
2022-03-30 09:36:37,861: ============================================================
2022-03-30 09:37:19,876: time cost, forward:0.16426553534542904, backward:0.037062792231317226, data cost:0.21005485450411135 
2022-03-30 09:37:19,877: ============================================================
2022-03-30 09:37:19,877: Epoch 3/31 Batch 5400/7662 eta: 1 day, 1:18:11.021535	Training Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.004)	
2022-03-30 09:37:19,877: ============================================================
2022-03-30 09:38:02,901: time cost, forward:0.16446802780441336, backward:0.037116798363418704, data cost:0.2101302360660229 
2022-03-30 09:38:02,902: ============================================================
2022-03-30 09:38:02,902: Epoch 3/31 Batch 5500/7662 eta: 1 day, 1:53:55.603630	Training Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.195 (0.004)	
2022-03-30 09:38:02,903: ============================================================
2022-03-30 09:38:46,109: time cost, forward:0.16471538737025043, backward:0.03713728589955388, data cost:0.21019522720925574 
2022-03-30 09:38:46,109: ============================================================
2022-03-30 09:38:46,110: Epoch 3/31 Batch 5600/7662 eta: 1 day, 1:59:46.521259	Training Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.004)	
2022-03-30 09:38:46,110: ============================================================
2022-03-30 09:39:27,863: time cost, forward:0.16481821841159439, backward:0.03713328513289192, data cost:0.21018048001289535 
2022-03-30 09:39:27,865: ============================================================
2022-03-30 09:39:27,865: Epoch 3/31 Batch 5700/7662 eta: 1 day, 1:06:40.218951	Training Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.004)	
2022-03-30 09:39:27,865: ============================================================
2022-03-30 09:40:11,658: time cost, forward:0.1650808169977853, backward:0.03713418825224857, data cost:0.21035341871958557 
2022-03-30 09:40:11,668: ============================================================
2022-03-30 09:40:11,669: Epoch 3/31 Batch 5800/7662 eta: 1 day, 2:19:50.563872	Training Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.004)	
2022-03-30 09:40:11,669: ============================================================
2022-03-30 09:40:53,672: time cost, forward:0.16516950320825674, backward:0.03713439306701153, data cost:0.21038282081259976 
2022-03-30 09:40:53,672: ============================================================
2022-03-30 09:40:53,673: Epoch 3/31 Batch 5900/7662 eta: 1 day, 1:14:14.132835	Training Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.004)	
2022-03-30 09:40:53,673: ============================================================
2022-03-30 09:41:36,226: time cost, forward:0.16535345739792895, backward:0.037152808514172325, data cost:0.21040344965579133 
2022-03-30 09:41:36,227: ============================================================
2022-03-30 09:41:36,227: Epoch 3/31 Batch 6000/7662 eta: 1 day, 1:33:21.934095	Training Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.004)	
2022-03-30 09:41:36,227: ============================================================
2022-03-30 09:42:18,275: time cost, forward:0.16543822899121186, backward:0.03716194678533466, data cost:0.21042873945797558 
2022-03-30 09:42:18,275: ============================================================
2022-03-30 09:42:18,276: Epoch 3/31 Batch 6100/7662 eta: 1 day, 1:14:26.857512	Training Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.004)	
2022-03-30 09:42:18,276: ============================================================
2022-03-30 09:43:02,214: time cost, forward:0.16573397535184714, backward:0.037220390251825504, data cost:0.21046946702185784 
2022-03-30 09:43:02,215: ============================================================
2022-03-30 09:43:02,215: Epoch 3/31 Batch 6200/7662 eta: 1 day, 2:21:48.634131	Training Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.004)	
2022-03-30 09:43:02,215: ============================================================
2022-03-30 09:43:43,087: time cost, forward:0.16568919582430533, backward:0.037269316394852536, data cost:0.2104011137233043 
2022-03-30 09:43:43,087: ============================================================
2022-03-30 09:43:43,088: Epoch 3/31 Batch 6300/7662 eta: 1 day, 0:30:43.536388	Training Loss nan (nan)	Training Prec@1 0.000 (0.000)	Training Prec@5 0.000 (0.004)	
2022-03-30 09:43:43,088: ============================================================
